{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 優化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bitsandbytes\n",
    "! pip install accelerate\n",
    "\n",
    "! pip install langchain langchain-chroma langchain-openai\n",
    "! pip install langchain_huggingface\n",
    "! pip install langchain_community\n",
    "! pip install langchain_cohere\n",
    "\n",
    "\n",
    "! pip install transformers\n",
    "! pip install sentence_transformers\n",
    "\n",
    "! pip install einops transformers_stream_generator\n",
    "! pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import LLMChain\n",
    "\n",
    "import re\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接跟LLM QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tlyu0419\\.cache\\huggingface\\hub\\models--shenzhi-wang--Llama3-8B-Chinese-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 4/4 [11:31<00:00, 172.92s/it]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.09s/it]"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
    "\n",
    "\n",
    "# 量化參數\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True)\n",
    "\n",
    "# llm 初始化\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=MODEL_NAME,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs=dict(\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config),\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.0001,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "問題: 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "Let's think step by step.\n",
      "\n",
      "答案: 是的，模型的可解釋性非常重要。主要有以下幾個原因：\n",
      "\n",
      "1. 理解和預測能力：模型的主要目的之一是幫助我們了解數據背後的趨勢、關係或行為。如果一個模型無法提供足夠多的信息來支持我們對數據的理解，它就失去了它的價值。因此，具有良好可解釋性的模型有助於提高我們對數據的洞察力並使我們能夠更好地做出決策。\n",
      "2. 信任和可信度：在很多情況下，模型被用於支持重大決定，如醫療診斷、金融投資等。在這些場景中，人們需要相信模型的結果。如果模型不透明且不可解釋，人們可能會懷疑其準確性和可靠性，從而降低對其結果的信心。\n",
      "3. 合規遵循：一些行業（例如金融業）要求公司使用符合監管規定的模型。這些模型通常需要能解釋其算法以及如何得出結論。因此，可解釋性對於遵守法律和法規至關重要。\n",
      "4. 避免偏見和歧視：某些模型可能無意間嵌入了人類的偏見和歧視。只有當模型可以被理解時，才能發現這種偏差，並加以糾正。\n",
      "5. 改進模型性能：通過分析模型的內部結構，研究人員可以找到改善模型性能的方法。這包括消除錯誤、減少過擬合、增加通用性等。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"問題: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "答案: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# 使用 LLM Chain 將 Prompt 與 LLM 串接起來\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 將問題透過參數化的方式帶入\n",
    "question = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "print(llm_chain.invoke({\"question\": question})[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader('https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf')\n",
    "ml_book = loader.load()\n",
    "\n",
    "for i, p in enumerate(ml_book):\n",
    "  ml_book[i].page_content = re.sub('\\n', '', ml_book[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 0, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='序言机器学习对于改进产品、过程和研究有着很⼤的潜⼒。但是计算机通常无法解释他们的预测，这是采⽤机器学习的障碍。这本书是关于使机器学习模型及其决策可解释的。在探索了可解释性的概念之后，你将学习简单的、可解释的模型，例如决策树、决策规则和线性回归。后⾯⼏章重点介绍了解释⿊盒模型的模型⽆关的⼀般⽅法，如特征重要性和累积局部效应，以及⽤Shapley 值和LIME 解释单个实例预测。所有的解释⽅法进⾏了深⼊说明和批判性讨论。它们如何在⿊盒下⼯作的？它们的优缺点是什么？如何解释它们的输出？本书将使你能够选择并正确应⽤最适合你的机器学习项⽬的解释⽅法。这本书的重点是表格式数据(也称为关系数据或结构化数据) 的机器学习模型，较少涉及到计算机视觉和⾃然语⾔处理任务。建议机器学习从业者、数据科学家、统计学家和任何对使机器学习模型可解释的⼈阅读本书。你可以在leanpub.com 上购买PDF 和电⼦书版本(epub，mobi)。你可以在lulu.com 上购买印刷版。关于我：我叫克⾥斯托夫·莫纳(Christoph Molnar)，我是统计学家和机器学习热爱者。我的⽬标是使机器学习可解释。邮件：christoph.molnar.ai@gmail.com⽹站：https://christophm.github.io/在Twitter 上关注我！@ChristophMolnar@YvonneDoinel 的封⾯本书采⽤Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License 协议授权。关于译者：我叫朱明超(Mingchao Zhu)，我是机器学习热爱者。i', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='序言ii邮件：deityrayleigh@gmail.com你可以在Github ⽹站上关注这本书的更新：https://github.com/MingchaoZhu/InterpretableMLBook', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 2, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录序言i第一章前言11.1故事时间. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21.1.1闪电永不两次. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21.1.2信任倒下. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41.1.3费⽶的回形针. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51.2什么是机器学习？. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .71.3术语. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8第二章可解释性112.1可解释性的重要性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .112.2可解释性⽅法的分类. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .152.3可解释性范围. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .172.3.1算法透明度. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .172.3.2全局、整体的模型可解释性. . . . . . . . . . . . . . . . . . . . . . . . . . . .172.3.3模块层⾯上的全局模型可解释性. . . . . . . . . . . . . . . . . . . . . . . . . .172.3.4单个预测的局部可解释性. . . . . . . . . . . . . . . . . . . . . . . . . . . . .182.3.5⼀组预测的局部可解释性. . . . . . . . . . . . . . . . . . . . . . . . . . . . .182.4可解释性评估. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .182.5解释的性质. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .192.6⼈性化的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .212.6.1什么是解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .212.6.2什么是好的解释？. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22第三章数据集253.1⾃⾏车租赁(回归) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .253.2YouTube 垃圾评论(⽂本分类). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .263.3宫颈癌的危险因素(分类). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .271', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 3, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录2第四章可解释的模型284.1线性回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .294.1.1解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304.1.2⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .324.1.3可视化解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .334.1.4解释单个实例预测. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .354.1.5分类特征的编码. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .364.1.6线性模型是否有很好的解释？. . . . . . . . . . . . . . . . . . . . . . . . . . .384.1.7稀疏线性模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .384.1.8优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .414.1.9缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .424.2逻辑回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .424.2.1线性回归⽤于分类有什么问题？. . . . . . . . . . . . . . . . . . . . . . . . . .424.2.2理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .434.2.3解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .454.2.4⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .464.2.5优缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .474.2.6软件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .484.3GLM, GAM 和其他模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .484.3.1⾮⾼斯结果输出- GLM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .504.3.2交互. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .554.3.3⾮线性效应- GAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .584.3.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .624.3.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .634.3.6软件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .634.3.7进⼀步扩展. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .634.4决策树. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .654.4.1解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .674.4.2⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .674.4.3优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .694.4.4缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .704.4.5软件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .704.5决策规则. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .704.5.1从单个特征学习规则(OneR) . . . . . . . . . . . . . . . . . . . . . . . . . . . .724.5.2顺序覆盖. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .764.5.3贝叶斯规则列表. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .794.5.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 4, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录34.5.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .844.5.6软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .854.6RuleFit. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .854.6.1解释和⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .864.6.2理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .884.6.3优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .904.6.4缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .914.6.5软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .914.7其他可解释模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .924.7.1朴素贝叶斯分类器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .924.7.2k-最近邻. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92第五章模型无关方法935.1部分依赖图. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .955.1.1⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .965.1.2优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1005.1.3缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015.1.4软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015.2个体条件期望. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015.2.1⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1025.2.2优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1055.2.3缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.2.4软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.3累积局部效应图. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.3.1动机和直觉. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065.3.2理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1095.3.3估计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1115.3.4⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1135.3.5优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1235.3.6缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1245.3.7实现和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1245.4特征交互. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1255.4.1特征交互. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1255.4.2理论：弗⾥德曼的H 统计量. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1265.4.3⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1275.4.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1305.4.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 5, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录45.4.6实现. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1315.4.7替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1315.5置换特征重要性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1325.5.1理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1325.5.2我应该计算训练数据还是测试数据的重要性？. . . . . . . . . . . . . . . . . . 1325.5.3⽰例和解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1355.5.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365.5.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1375.5.6软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1385.6全局代理模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1385.6.1理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1385.6.2⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1405.6.3优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1415.6.4缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1425.6.5软件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1425.7局部代理(LIME). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1425.7.1表格数据的LIME. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1435.7.2⽂本的LIME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1465.7.3图像的LIME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1485.7.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1485.7.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1495.8Shapley 值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1505.8.1总体思路. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1505.8.2⽰例与解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1535.8.3详细的Shapley 值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1545.8.4优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1575.8.5缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1585.8.6软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1595.9SHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1595.9.1定义. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1595.9.2KernelSHAP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1615.9.3TreeSHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1645.9.4⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1655.9.5SHAP 特征重要性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1665.9.6SHAP 概要图. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1675.9.7SHAP 依赖图. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1675.9.8SHAP 交互值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 6, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录55.9.9聚类SHAP 值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1695.9.10 优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1705.9.11 缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1705.9.12 软件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171第六章基于样本的解释1726.1反事实解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1736.1.1⽣成反事实解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1756.1.2⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1766.1.3优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1776.1.4缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1786.1.5软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1786.2对抗样本. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1796.2.1⽅法与⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806.2.2⽹络安全视⾓. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1866.3原型与批评. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1886.3.1理论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1896.3.2⽰例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1926.3.3优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1936.3.4缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1946.3.5代码和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1946.4有影响⼒的实例. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1946.4.1删除诊断. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1986.4.2影响函数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016.4.3识别有影响⼒的实例的优点. . . . . . . . . . . . . . . . . . . . . . . . . . . . 2066.4.4识别有影响⼒的实例的缺点. . . . . . . . . . . . . . . . . . . . . . . . . . . . 2076.4.5软件和替代⽅法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207第七章水晶球2097.1机器学习的未来. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2107.2可解释性的未来. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212参考文献214', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 7, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第一章前言本书向你介绍了如何使(监督) 机器学习模型可解释。虽然本书中包含⼀些数学公式，但是即使没有公式，你也需要能够理解这些⽅法背后的思想。本书不适合机器学习初学者。如果你不熟悉机器学习，则有很多书籍和其他资源可以学习基础知识。关于机器学习的⼊门学习，我推荐在线学习平台coursera.com 上的Hastie，Tibshirani 和Friedman[1] 所著的《The Elements of StatisticalLearning》⼀书和Andrew Ng 的“机器学习” 在线课程。这些书本和在线课程都是免费的！⽬前解释机器学习模型的新⽅法以惊⼈的速度发表，跟上所有已发布的内容⾮常不现实。这就是为什么你不会在本书中看到最新颖、最奇特的⽅法，⽽是看到机器学习可解释性的成熟⽅法和基本概念。这些基础知识可以帮助你使机器学习模型具有可解释性。你阅读本书后，内化基础知识还使你能够更好地理解和评估arxiv.org 上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是理解这本书所必需的，但希望它们能使你愉悦并引起思考。然后，本书探讨了机器学习可解释性的概念。我们将讨论可解释性何时重要，以及有哪些不同类型的解释。本书中使⽤的术语可以在“术语” 章中查找。所描述的⼤多数模型和⽅法都是使⽤“数据集” ⼀章中描述的真实数据集来介绍的。使机器学习可解释的⼀种⽅法是使⽤可解释的模型，例如线性模型或决策树。另⼀个选择是使⽤与模型⽆关的解释⼯具，这样就可以应⽤于任何监督机器学习模型。“模型⽆关⽅法” ⼀章处理诸如部分依赖图和置换特征重要性之类的⽅法。与模型⽆关的⽅法通过改变机器学习模型的输⼊并观察模型的输出变化来⼯作。返回实例作为解释的模型⽆关的⽅法，将在“基于样本的解释” ⼀章中进⾏讨论。所有与模型⽆关的⽅法都可以根据它们是在所有数据实例中解释全局模型⾏为还是单个实例预测来进⼀步区分。以下⽅法解释了模型的整体⾏为：部分依赖图(Partial Dependence Plots)，累积局部效应(AccumulatedLocal Effects)，特征交互(Feature Interaction)，特征重要性(Feature Importance)，全局代理模型(Global Surrogate Models) 以及原型和批判(Prototypes and Criticisms)。为了解释单个实例预测，我们有局部代理模型(Local Surrogate Models)，Shapley 值解释(Shapley ValueExplanations)，反事实解释(Counterfactual Explanations) (密切相关：对抗样本)。可以使⽤⼀些⽅法来解释全局模型⾏为和单个实例预测两个⽅⾯：个体条件期望(Individual ConditionalExpectation) 和有影响⼒的实例(Influential Instances)。这本书以对可解释机器学习的未来前景持乐观态度作为结尾。1', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 8, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言2你可以从头到尾阅读这本书，也可以直接跳到你感兴趣的⽅法。希望你喜欢阅读！1.1故事时间我们将从⼀些短篇故事开始。每个故事都是对可解释的机器学习的的夸张表现。如果你赶时间，你可以跳过它们。如果你想获得娱乐和动⼒，请继续阅读！该排版的灵感来⾃杰克·克拉克(Jack Clark) 在他的Import AI Newsletter。如果你喜欢此类故事或对AI 感兴趣，我建议你注册。1.1.1闪电永不两次2030 年：瑞士的医学实验室“这绝对不是最糟糕的死亡⽅式！” Tom 总结说，试图在这场悲剧中找到积极的东西。他从静脉输液架上拆下了泵。Lena 补充说：“他只是因为错误的原因死了。”“当然还有错⽤的吗啡泵！为我们增⼤了⼯作量！” Tom ⼀边拧下泵的后板⼀边抱怨。卸下所有螺钉后，他把盘⼦举起放在⼀边。他将电缆插⼊诊断端⼜。“你不只是抱怨⼯作，是吗？” Lena 笑了笑。“当然不是。从未！” 他⽤讽刺的语⽓惊呼。他启动了泵的计算机。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 9, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言3Lena 将电缆的另⼀端插⼊平板电脑。“好的，诊断程序正在运⾏。” 她说，“我真的很好奇出了什么问题。”“它确实为我们的John Doe 注射了Nirvana。那吗啡浓度很⾼。伙计，我是说…这是第⼀次，对吧？通常情况下，⼀个坏泵只会散发出很少的甜味或者没有味道。但是永远不会，像那疯狂的注射。”Tom 解释道。“我知道。你不必说服我……嘿，看那个。” Lena 举起她的平板电脑。“你看到这个峰值了吗？这就是⽌痛药的功效。看！这条线显⽰参照⽔平。这个可怜的家伙在他的⾎液系统中混合了多种⽌痛药，可以杀死他17 次以上。在这⾥由我们的泵注⼊。然后在这⾥……” 她轻扫，“在这⾥你可以看到病⼈死亡的那⼀刻。“那么，你知道发⽣什么了吗，⽼板？” Tom 问他的上司。“嗯……传感器似乎很好。⼼率，氧⽓⽔平，葡萄糖等……数据已按预期收集。⾎液氧数据中有些缺失值，但这并不罕见。看这⾥，传感器还检测出了吗啡衍⽣物和其他⽌痛药引起的病⼈⼼律减慢和⽪质醇⽔平降低。” 她继续浏览诊断报告。Tom 着迷地盯着屏幕。这是他对真实设备故障的⾸次调查。“好，这是我们的第⼀个难题。系统未能向医院的通信信道发送警告。警告已触发，但应急⽅案未响应。这可能是我们的错，但也可能是医院的错。请将⽇志发送给IT 团队。”Lena 对Tom 说。Tom 点了点头，眼睛仍然盯着屏幕。Lena 继续说：“这很奇怪。该警告也应该导致泵关闭。但是它显然没有这样做，那⼀定是个错误。质量团队错过了⼀些东西。真的很糟糕。也许与应急⽅案有关。”“因此，泵的应急系统不知何故发⽣了故障，但是为什么泵如此疯狂并向John Doe 注⼊了很多⽌痛药？” Tom 想知道。“好问题。你是对的。除了应急紧急故障之外，泵根本不应该使⽤那么多的药物。鉴于⽪质醇和其他警告信号的低⽔平，该算法应该早点停⽌。”Lena 解释说。“也许有些不幸，⽐如百万分之⼀，就像被闪电击中⼀样？” Tom 问她。“不，Tom。如果你阅读了我发给你的⽂档，你就会知道这个泵⾸先是在动物实验中测试的，然后是在⼈类⾝上测试的，以学习根据感觉输⼊来注射完美数量的⽌痛药。泵的算法可能是不透明且复杂的，但不是随机的。这意味着在相同情况下，泵将再次以完全相同的⽅式运⾏，我们的病⼈会再次死亡。感觉输⼊的组合或不希望有的交互作⽤必定已经触发了泵的错误⾏为。这就是为什么我们必须深⼊挖掘，找出这⾥发⽣的事情。”Lena 解释说。“我明⽩了……”Tom 迷惑地回答，“病⼈不是很快就会死吗？因为癌症什么的。”', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 10, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言4Lena 在阅读分析报告时点了点头。Tom 起⾝去窗前。他向外看，⽬光注视着远处的某个点。“也许，这台机器使他摆脱了痛苦，帮了他⼀个忙，不再受苦。也许它只是做了正确的事，就像闪电，但是，你知道，⼀个好闪电。我的意思是就像彩票，但不是随机的。但出于某种原因。如果我是泵，我也会做同样的事情。”她终于抬起头看着他。他⼀直在看外⾯的东西。他们都沉默了⽚刻。Lena 再次低下头，继续分析。“不，Tom。这是⼀个错误……只是该死的错误。”1.1.2信任倒下2050 年：新加坡地铁站她赶到Bishan 地铁站。按照她的想法，她已经在⼯作了。新的神经架构的测试现在应该已经完成了。她领导了重新设计政府的“个体纳税情况预测系统”，该系统可以预测⼀个⼈是否会向税务部门隐瞒资⾦。她的团队提出了⼀个优雅的想法。如果成功的话，该系统不仅可以为税务局提供服务，还可以提供给其他系统，例如反恐警报系统和商业登记处。有⼀天，政府甚⾄可以将这些预测纳⼊公民信任评分。公民信任评分估计⼀个⼈的可信度，这个估计会影响你⽇常⽣活的各个⽅⾯，例如贷款或你要等多久才能拿到新护照。当她⾛下⾃动扶梯时，她想象着她的团队系统与公民信任评分系统的整合会是怎样的。她经常在不降低步⾏速度的情况下⽤⼿擦拭RFID 阅读器。她的思绪被占据了，但是感觉与现实的不⼀致在她的⼤脑中敲响了警钟。太晚了。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 11, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言5⿐⼦先是撞到地铁⼊⼜，然后屁股跌在地上。门应该被打开……但是没有打开。她傻眼了，站起来看着门旁的屏幕。“请再试⼀次，” 屏幕上友好的笑脸建议。有⼀个⼈路过，⽆视她，并把他的⼿覆盖在读取系统上。门开了，他穿过了。门又关了。她擦了擦⿐⼦。⿐⼦很痛，但⾄少没有出⾎。她试图打开门，但又被拒绝了。真奇怪，也许她的公共交通帐户没有⾜够的代币。她看着智能⼿表检查帐户余额。“登录被拒绝。请联系您的公民咨询局！” 她的⼿表通知了她。⼀种恶⼼的感觉像拳头⼀样打在她的肚⼦上。她怀疑发⽣了什么事。为了证实她的想法，她启动了移动游戏“狙击⼿协会”，⼀个⾃我射击游戏。该应⽤程序又直接⾃动关闭了，这证实了她的想法。她头晕⽬眩，再次坐在地上。仅有⼀种可能的解释：她的公民信任分数下降了。基本上，⼩幅度的下降意味着⼀些⼩的不便，例如没有乘坐头等舱的航班或者需要等待更长的时间才能获得官⽅⽂件。较低的信任度很罕见，这意味着你被视为对社会的威胁。对付这些⼈的⼀个措施是让他们远离地铁等公共场所。政府限制公民信任分数低的对象的⾦融交易，他们还开始积极监视你在社交媒体上的⾏为，甚⾄还限制了某些内容，例如暴⼒游戏。你的公民信任分数越低，就越难增加公民信任分。得分很低的⼈通常不会恢复原来分数。她想不出任何原因为什么她的分数会下降。分数基于机器学习。公民信任评分系统就像⼀台运转良好的发动机，服务于社会。信任评分系统的性能始终受到密切监控。⾃本世纪初以来，机器学习已经变得越来越好。它的效率如此之⾼，以⾄于信任评分系统做出的决定将不再引起争议。绝对可靠的系统。她绝望地笑了。绝对可靠的系统——只要，系统很少出故障。但失败了。她⼀定是其中的特例之⼀；系统错误；从现在开始她就成了被抛弃的⼈。没有⼈敢质疑这个系统。它过于与政府、社会本⾝融为⼀体，这是不可质疑的。在仅存的少数⼏个民主国家中，禁⽌开展反民主运动，不是因为这些国家在本质上是恶意的，⽽是因为它们会破坏当前制度的稳定。同样的逻辑适⽤于现在更常见的算法。由于对现状的危害，禁⽌对算法进⾏批判。对算法的信任是社会秩序的基础。为了共同利益，默认接受了罕见的虚假信任评分。成百上千的其他预测系统和数据库也加⼊了评分，使得⽆法知道是什么原因导致了评分下降。她感到⾃⼰和她的下⽅好像有⼀个⼤⿊洞。她惊恐地看着虚空。她的纳税情况系统最终被纳⼊公民信托评分系统，但是她从此⼀⽆所知。1.1.3费米的回形针612 年AMS (在火星定居后)：火星博物馆', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 12, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言6“历史很⽆聊。”Xola 对她的朋友⼩声说道。Xola，⼀个蓝头发的⼥孩，正懒洋洋地⽤左⼿追着房间⾥嗡嗡作响的放映机。“历史很重要。” ⽼师沮丧地看着⼥孩们说道。Xola 脸红了。她没想到她的⽼师会偷听她。“Xola，你刚刚学到了什么？” ⽼师问她。“古代⼈⽤尽了地球⼈的所有资源然后死了吗？” 她仔细地问。“不。他们使⽓候变热，不是⼈，⽽是计算机和机器。这是⾏星地球，⽽不是地球⾏星。” 另⼀个名叫Lin 的⼥孩补充道。Xola 点了点头。⽼师满怀⾃豪，笑了笑，点了点头。“你们都是对的。你知道为什么会这样吗？” “因为⼈们⽬光短浅和贪婪？” Xola 问。“⼈们⽆法停⽌他们的机器！”Lin 脱⼜⽽出。“再次，你们都是对的，” ⽼师肯定道，“但是要复杂得多。当时⼤多数⼈都不知道发⽣了什么。有些⼈看到了巨⼤的变化，但⽆法扭转它们。这⼀时期最著名的作品是⼀位匿名作家的诗。它最好地记录了当时发⽣的事情。仔细听！”⽼师开始了这⾸诗。⼗⼏架⼩型⽆⼈机将⾃⼰放置在孩⼦们⾯前，并开始将视频直接投射到他们的眼睛中。它显⽰⼀个穿着西装的⼈站在森林⾥，只剩下树桩。他开始说道：机器计算；机器预测。我们继续前进，因为我们是其中的⼀部分。我们追求经过训练的最佳状态。最优解是⼀维的、局部的、⽆约束的。硅元素和⾎⾁，追逐着指数。发展是我们的精神。当收集所有奖励，和忽略副作用之后；当所有硬币都被开采，自然已经落后之后；我们会有麻烦的，毕竟，指数式发展是⼀个泡沫。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 13, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言7公地悲剧在不断上演，爆炸，就在我们眼前。冰冷的计算和贪婪，让地球充满热量。⼀切都在死亡，⽽我们是顺从的。就像蒙着眼睛的马⼀样，我们参加自⼰创造的比赛，迈向⽂明的⼤过滤器。因此，我们坚持不懈地前进。因为我们是机器的⼀部分。拥抱熵。“⿊暗的记忆，” ⽼师说道，打破了房间的沉默。“它将被上传到你们的图书馆。记住你们的作业下周前交。” Xola 叹了⼜⽓。她设法捉住了其中⼀架⼩型⽆⼈机。⽆⼈机从CPU 和引擎中发出的声⾳很温暖。Xola 喜欢它温暖她的⼿。1.2什么是机器学习？机器学习是计算机基于数据做出和改进预测或⾏为的⼀套⽅法。例如，为了预测房屋的价格，计算机将从过去的房屋销售中学习模式。本书关注于监督机器学习，它涵盖了所有的预测问题，其中我们会有⼀个数据集，我们已经知道感兴趣的结果(如过去的房价)，并希望学习预测新数据的结果。有监督的学习不包括例如聚类任务(相当于⽆监督学习)，在这些任务中我们没有感兴趣的特定结果，但希望找到数据点的聚类。此外，诸如强化学习之类的也被排除，在这种情况下，智能体(Agent) 基于环境(Environment) 做出动作(Action) 来学习优化某种奖励(Reward) (例如玩俄罗斯⽅块的计算机)。监督学习的⽬标是学习⼀个预测模型，将数据的特征(如房屋⼤⼩、位置、楼层类型等) 映射到输出(如房屋价格)。如果输出的是类别，则任务称为分类；如果输出是数值，则任务称为回归。机器学习算法通过估计参数(如权重) 或学习结构(如树) 来学习模型，且算法由⼀个最⼩化的分数或损失函数指导。例如，在房屋价格预测中，机器将最⼩化房屋的估计价格和预测价格之间的差值，然后，就可以使⽤经过充分训练的机器学习模型来预测新实例。房价估计、产品推荐、路牌检测、信⽤违约预测和欺诈检测：所有这些⽰例有着共同点，都可以通过机器学习来解决。任务不同，但⽅法相同：• 步骤⼀：数据采集，越多越好。数据必须包含你要预测的结果以及要从中进⾏预测的其他信息。对于路牌检测(即“图像中有路牌吗？”)，你需要收集街道图像并标记是否存在路牌；对', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 14, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言8于信⽤违约的预测，你需要过去实际贷款的数据、客户是否拖⽋贷款的信息以及有助于做出预测的数据，例如收⼊、过去信⽤违约等；对于房屋价格估计，你可以从过去的房屋销售中收集数据和有关房地产的信息，如⼤⼩、位置等。• 步骤⼆：将这些信息输⼊机器学习算法，⽣成路牌检测模型、信⽤评级模型或房屋价格估计模型。• 步骤三：将新数据输⼊模型。将模型集成到产品或流程中，例如⾃动驾驶汽车、信贷申请流程或房地产市场⽹站。在许多任务上机器都超过⼈类，例如下棋(或最近的围棋) 或天⽓预测。即使机器的性能和⼈类⼀样好，或者在某个任务上略微逊⾊，但在速度、可重复性和规模化⽅⾯仍然有很⼤的优势。⼀个曾经实施过的机器学习模型可以⽐⼈类更快地完成⼀项任务，可靠地提供⼀致的结果，并且可以⽆限地复制。在另⼀台机器上复制机器学习模型既快速又廉价。对应的，培训⼈员去完成⼀项任务则可能需要⼏⼗年(尤其是当他们年轻时)，⽽且成本很⾼。使⽤机器学习的⼀个主要缺点是，关于数据和机器解决的任务的思路被隐藏在愈加复杂的模型中。你需要数以百万计的参数来描述⼀个深层的神经⽹络，⽽没有办法完全理解这个模型。其他模型，如随机森林，由数百个决策树组成，它们“投票” 获得预测。为了理解这个决策是如何做出的，你必须查看数百棵树中每棵树的投票和结构。⽆论你多么聪明或记忆多么好，这都是⾏不通的。即使每个模型都可以被解释，但性能最好的模型通常是多个模型的集成，这就变得⽆法解释了。如果只关注性能，你就会获得越来越多的不透明的模型。例如，看看Kaggle.com 机器学习竞赛平台对获胜者的采访：胜出的模型⼤多是模型的集成，或是⾮常复杂的模型，如提升树或深层神经⽹络。1.3术语为避免歧义引起混淆，本书中使⽤的术语定义如下：算法(Algorithm) 是机器为达到特定⽬标⽽遵循的⼀组规则[2]。可以将算法视为定义输⼊、输出以及从输⼊到输出所需的所有步骤的配⽅：配⽅是⼀种算法，其中材料是输⼊，熟⾷是输出，准备和烹饪步骤是算法指令。机器学习(Machine Learning) 是⼀套⽅法，能够允许计算机从数据中学习，以做出和改进预测(例如癌症、每周销售、信⽤违约)。机器学习是从“常规编程” (Normal Programming) 到“间接编程”(Indirect Programming) 的⼀种范式转换，“常规编程” 是指所有指令都必须显式地提供给计算机，⽽“间接编程” 是通过提供数据来实现的。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 15, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言9学习器(Learner) 或机器学习算法(Machine Learning Algorithm) 是⽤来从数据中学习机器学习模型的程序。另⼀个名字是“诱导器” (Inducer) (例如“树诱导器”)。机器学习模型(Machine Learning Model) 是将输⼊映射到预测的学习程序，这可以是线性模型或神经⽹络的⼀组权重。“模型” (Model) 也可以称作“预测器” (Predictor)，基于任务可以再分为“分类器” (Classifier) 或者“回归模型” (Regression Model)。在公式化描述中，经过训练的机器学习模型称为ˆf 或ˆf(x)。图1.1. 学习器从标记的训练数据学习模型。该模型⽤于预测。黑盒模型(Black Box Model) 是⼀个不揭⽰其内部机制的系统。在机器学习中，“⿊盒模型” 或称“⿊匣⼦” 描述了通过查看参数(例如深度神经⽹络的参数) 却⽆法理解的模型。⿊盒的对⽴⾯有时被称为白盒(White Box)，在本书中被称为可解释模型。模型⽆关的解释⽅法将机器学习模型视为⿊盒(即使这些模型本⾝不是⿊盒)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 16, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼀章前言10可解释的机器学习(Interpretable Machine Learning) 是指使机器学习系统的⾏为和预测对⼈类可理解的⽅法和模型。数据集(Dataset) 是⼀个表格，其中包含机器要从中学习的数据。数据集包含要预测的特征和⽬标。当⽤于训练模型时，数据集称为训练数据。实例(Instance) 表现为数据集中的⼀⾏。实例也可以称作：(数据) 点(Data Point)、样本(Example)或观测(Observation)。实例由特征值(向量) x(i) 和⽬标结果yi 组成。特征(Features) 是⽤于对输⼊进⾏预测或分类的。特征表现为数据集中的列。本书中认为特征是可解释的，这意味着很容易理解它们的含义，⽐如某⼀天的温度或⼀个⼈的⾝⾼。当然特征的可解释性是⼀个很⼤的假设，但如果很难理解输⼊的特征，那么就更难理解模型的⾏为。对于单个实例，具有所有特征的矩阵记为X 和x(i)；所有实例的单个特征向量是xj；⽽第i 个实例的第j 个特征对应的值是x(i)j 。目标(Target) 是机器要去学会预测的信息。在数学公式中，对于单个实例，⽬标通常记为y(i) 或yi。机器学习任务(Machine Learning Task) 是⼀个具有特征和⽬标的数据集的组合。根据⽬标的类型，任务可以是分类、回归、⽣存分析、聚类或异常值检测。预测(Prediction) 是机器学习模型根据给定的特征“猜测” ⽬标值应该是什么。在本书中，模型预测⽤ˆf(x(i)) 或ˆy 表⽰。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 17, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez 和Kim (2017)，Miller (2017))：• 人类的好奇心和学习能力，这是指⼈类有着对周围环境的⼼理模型(Mental Model)，当有意外的事情发⽣时，⼈类就会更新这个⼼理模型，通过为这样的意外事件找到个解释来更新模型。例如，⼀个⼈突然感到不舒服，就问⾃⼰：“为什么我感到如此不舒服？”。他得出每次11', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 18, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性12吃那些红⾊浆果后他都会⽣病，于是他更新了他的⼼理模型，认为浆果导致了疾病，因此应该避免⾷⽤。在研究中使⽤不透明的机器学习模型时，如果模型仅给出预测⽽没有解释，则科学发现仍是完全不可知的。为了促进学习并满⾜⼈们的好奇⼼——为什么机器产⽣了某些预测或⾏为，那么此时可解释性和解释⾄关重要。当然，⼈们不需要对所有发⽣的⼀切都进⾏解释。对于⼤多数⼈来说，他们可以不理解计算机是如何⼯作的。但意外事件使我们好奇，例如说，“为什么我的计算机意外关闭？”• 与学习密切相关的是⼈类渴望找到事物存在的意义。我们希望协调我们知识结构要素之间的⽭盾或不⼀致，例如，“为什么我的狗会咬我，尽管它以前从来没有这样做过。” 狗过去的⾏为与新发⽣的、令⼈不快的咬伤经历之间存在着⽭盾。兽医的解释调和了狗主⼈的⽭盾：“那只狗因为紧张⽽咬⼈。” 机器的决策对人的生活影响越大，机器对它行为的解释就越重要。如果机器学习模型拒绝贷款申请，这对于申请者来说可能是完全意外的。他们只能⽤某种说法来调和期望和现实之间的这种不⼀致。这些解释实际上并不需要完全说清楚情况，但应该⾄少提供⼀个主因。另⼀个例⼦是算法产品推荐，就我个⼈⽽⾔，我⼀直在思考为什么某些产品或电影被算法推荐给我。通常情况是：⽹络上的⼴告跟踪着我，因为我最近买了⼀台洗⾐机，我知道在接下来的⼏天⾥，洗⾐机⼴告会推送给我；由于我们的购物车中已经有冬帽，建议戴⼿套是很合理的；算法推荐某部电影，是因为和我喜欢相同的其他电影的⽤户也喜欢着这部电影。互联⽹公司越来越多地在推荐中添加解释。亚马逊产品推荐就是⼀个很好的例⼦，它基于经常购买的产品组合。图2.1. 从亚马逊购买颜料时的推荐产品。• 在许多科学学科中，从定性⽅法到定量⽅法(例如社会学、⼼理学)，以及到机器学习(⽣物学、基因组学) 都发⽣了变化。科学的目标是获取知识，但是许多问题都是通过⼤数据集和⿊盒机器学习模型来解决的。模型本⾝应该成为知识的来源，⽽不是数据。可解释性使得可以提取模型捕获的这些额外知识。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 19, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性13• 机器学习模型承担需要安全措施和测试的实际任务。想象⼀下，⼀辆⾃动驾驶汽车根据深度学习系统⾃动检测骑⾃⾏车的⼈。你希望100% 确定系统所学到的抽象是⽆错误的，因为要是汽车直接碾过骑车⼈，这种情况是⾮常糟糕的。⼀种解释可能会指出，最重要的学习特征是识别⾃⾏车的两个轮⼦，这种解释可以帮助你思考极端情况，例如⾃⾏车的侧袋部分挡住了车轮。• 默认情况下，机器学习模型从训练数据中学习到了某种偏见，这可能把你的机器学习模型变成歧视受保护群体的种族主义者。可解释性是机器学习模型中⼀种有效检测偏见的调试⼯具。训练好的⾃动批准或拒绝信⽤申请的机器学习模型可能会歧视少数⼈。⽽你的主要⽬标是只向最终会偿还贷款的⼈提供贷款。在这种情况下，问题表述的不完整性在于，你不仅想要最⼤程度地减少贷款违约，⽽且也有义务不根据某些⼈⼜统计数据加以区分。这是问题制定中的⼀个附加约束(以低风险和合规的⽅式发放贷款)，⽽机器学习模型优化所针对的损失函数并没有包含这⼀约束。• 将机器和算法整合到⽇常⽣活中的过程需要可解释性，以增加社会认可度。⼈们把信仰、欲望、意图等等归因到物体上。在⼀个著名的实验中，Heider 和Simmel (1944)[6] 向参与者展⽰了⼀些形状的视频，其中⼀个圆圈打开⼀扇“门” 进⼊⼀个“房间” (这只是⼀个长⽅形)。参与者描述了形状的动作，就像描述⼈的⾏为⼀样，为形状分配了意图、甚⾄情感和性格特征。机器⼈就是⼀个很好的例⼦，⽐如说我的⼀个吸尘器，名字就叫“Doge”。如果Doge 被卡住了，我可以这么想：“Doge 想继续打扫，但它因为卡住了⽽向我求助。” 后来，当Doge 打扫完并寻找插座充电时，我想：“Doge 想充电，并打算找到插座。” 我还可以给它赋予性格特征：“Doge 有点笨，但很可爱。” 这些是我的想法，尤其是当我发现Doge 在尽职地打扫房⼦的时候撞倒了⼀棵植物时。能解释其预测的机器或算法会得到更多的认可，同时在后⾯，我们还会讨论解释是⼀个社会化过程。• 解释⽤于管理社交互动。通过创造某个事物的共同含义，解释者影响着解释的接收者的⾏为、情感和信念。对于⼀个要与我们互动的机器，它可能需要塑造我们的情感和信念。机器必须“说服” 我们，这样它们才能达到预期的⽬标。如果我的扫地机器⼈没有在某种程度上解释它的⾏为，我不会完全接受它。扫地机器⼈创造了⼀个共同的含义，例如，对于“事故” (如再次被困在浴室地毯上)，解释说它被卡住了，⽽不是简单地停⽌⼯作却不发表评论。有趣的是，解释机器的⽬标(构建信任) 和接收者的⽬标(理解预测或⾏为) 之间可能存在偏差。也许对Doge 被卡住的原因的完整解释可能是电池电量⾮常低，其中⼀个轮⼦⼯作不正常，还有⼀个bug 让机器⼈即使有障碍物也⼀次又⼀次地⾛到同⼀个地⽅。这些原因(还有其他⼀些) 导致机器⼈被卡住，但它只解释了有什么东西挡在路上，这⾜以让我相信它的⾏为，并得到事故的共同含义。顺便说⼀下，Doge 又被困在浴室⾥了。我们每次都要把地毯拆下来，然后让Doge清理。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 20, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性14图2.2. 我们的真空吸尘器Doge 卡住了。作为事故的解释，Doge 告诉我们，它需要在⼀个平⾯上。• 机器学习模型只有在可以解释时才能进⾏调试和审核。即使在低风险环境中，例如电影推荐，在研究和开发阶段以及部署之后，解释能⼒也是很有价值的。之后，当模型⽤于产品时，可能会出错。对错误预测的解释有助于理解错误的原因，它为如何修复系统提供了指导⽅向。考虑⼀个哈⼠奇与狼分类器的例⼦，分类器将⼀些哈⼠奇误分类为狼。使⽤可解释的机器学习⽅法，你会发现错误分类是由于图像上的雪造成的。分类器学会了使⽤雪作为⼀个特征来将图像分类为狼，这对于在训练数据集中分离狼和哈⼠奇可能是有道理的，但在实际使⽤中则不然。如果能够确保机器学习模型能够解释决策，我们还可以更容易地检查以下性质(Doshi-Velez 和Kim，2017) ：• 公平性(Fairness)：确保预测是公正的，不会隐式或显式地歧视受保护的群体。可解释的模型可以告诉你为什么它决定某个⼈不应该得到贷款，并且使⼈们更容易判断该决策是否基于学习⼈⼜统计学偏见(例如种族)。• 隐私(Privacy)：确保保护数据中的敏感信息。• 可靠性(Reliability) 或鲁棒性(Robustness)：确保输⼊的⼩变化不会导致预测发⽣剧烈变化。• 因果关系(Causality)：检查是否只找到因果关系。• 信任(Trust)：与⿊匣⼦相⽐，⼈们更容易信任解释其决策的系统。何时我们不需要解释', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 21, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。• 当问题被研究得很深入时，就不需要解释性了。⼀些应⽤已经得到了充分的研究，因此有⾜够的模型实践经验，随着时间的推移，模型的问题已经得到解决。⼀个很好的例⼦是光学字符识别的机器学习模型，它处理信封中的图像并提取地址。这些系统有多年的使⽤经验，很明显它们是有效的。此外，我们对获取有关这上⾯的任务的更多信息并不真正感兴趣。• 可解释性可能使⼈或程序能够操纵系统。欺骗系统的⽤户问题是由模型的创建者和⽤户的⽬标不匹配造成的。信⽤评分就是这样⼀个系统，因为银⾏希望确保贷款只发放给可能归还贷款的申请⼈，⽽申请⼈的⽬标是获得贷款，即使银⾏不想提供给他们。这两个⽬标之间的不匹配⿎励申请者对系统进⾏博弈，以增加他们获得贷款的机会。如果申请⼈知道拥有两张以上的信⽤卡会对他的分数产⽣负⾯影响，他只需退掉第三张信⽤卡来提⾼分数，并在贷款获得批准后申请新的信⽤卡。虽然他的分数有所提⾼，但偿还贷款的实际可能性并没有改变。只有当输⼊是因果特征的代理，⽽不是实际导致结果时，系统才能被博弈。尽可能避免使⽤代理特征，因为它们使模型有可能被博弈。例如，Google 开发了⼀个名为“Google 流感趋势”的系统来预测流感爆发。该系统将Google 搜索与流感爆发相关联，但其表现不佳。搜索查询的分布发⽣了变化，Google 流感趋势错过了许多次流感爆发。Google 搜索不会导致流感，当⼈们搜索“发烧” 这样的症状时，这仅仅是与实际流感爆发的关联。理想情况下，模型只使⽤因果特征，因为它们不可博弈。2.2可解释性方法的分类可以根据各种标准对机器学习可解释性的⽅法进⾏分类。本质的(Intrinsic) 还是事后的(Post-hoc)？该标准通过限制机器学习模型的复杂性(本质的，亦可称内在的) 或在训练后分析模型的⽅法(事后的) 来区分是否实现了可解释性。本质的可解释性是指由于结构简单⽽被认为是可解释的机器学习模型，如短的决策树或稀疏线性模型；事后解释性是指模型训练后运⽤解释⽅法，例如，置换特征重要性是⼀种事后解释⽅法。事后⽅法也可以应⽤于本质上是可解释的模型上。例如，可以计算决策树的置换特征重要性。本书各章节的组织⽅式是由本质上可解释模型(Intrinsically Interpretable Models) 和事后的(模型⽆关的) 解释⽅法', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 22, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性16(Post-hoc Interpretation Methods) 之间的区别决定。解释方法的输出——可以根据解释⽅法的输出⼤致区分各种解释⽅法。• 特征概要统计量(Feature Summary Statistic)：许多解释⽅法为每个特征提供概要统计量。有些⽅法为每个特征返回⼀个数字，例如特征重要性，或者更复杂的输出，例如成对特征交互强度，每个特征对表⽰为⼀个数字。• 特征概要可视化(Feature Summary Visualization)：⼤多数特征概要统计信息也可以可视化。有些特征概要实际上只有在可视化的情况下才有意义，并且表格不能满⾜要求。特征的部分依赖就是这样⼀种情况。部分依赖图是显⽰特征和平均预测结果的曲线。呈现部分依赖关系的最佳⽅法是实际绘制曲线，⽽不是打印坐标。• 模型内部(例如学习的权重) (Model Internals)：对于本质上可解释的模型的解释属于这⼀类，如线性模型中的权重或决策树的学习树结构(⽤于分割的特征和阈值)。但对于像线性模型，因为权重同时是模型内部和特征概要统计量，所以此时两者的界限是模糊的。输出模型内部结构的另⼀种⽅法是在卷积神经⽹络中将学习的特征检测器可视化。根据定义，输出模型内部的可解释性⽅法是特定于模型的(请参阅下⼀个标准)。• 数据点(Data Point)：这个类别的⽅法是返回数据点(已经存在的或新创建的) 以使模型可解释。⼀种⽅法叫做反事实解释(Counterfactual Explanations)，为了解释对数据实例的预测，该⽅法通过⽤⼀些⽅式改变某些特征以改变预测结果(例如预测类别的翻转)，找到相似的数据点。另⼀个⽅法是识别预测类的原型，这⾥输出新数据点的解释⽅法要求可以解释数据点本⾝。这对图像和⽂本很有效，但对于具有数百个特征的表格数据不太有⽤。• 本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。⽽这些可解释模型本⾝可以通过查看模型内部参数或特征概要统计量来解释。特定于模型(Model-specific) 还是模型无关(Model-agnostic)？特定于模型的解释⽅法仅限于特定的模型类，例如线性模型中回归权重的解释就是特定于模型的解释，因为根据定义，本质上可解释模型的解释通常是特定于模型的解释。仅应⽤于解释如神经⽹络的⼯具也是特定于模型的。相对应的，与模型⽆关的⼯具可以⽤于任何机器学习模型，并在模型经过训练后应⽤(事后的)。这些模型⽆关的⽅法通常通过分析特征输⼊和输出来⼯作。根据定义，这些⽅法是不能访问模型的内部信息，如权重或结构信息。局部(Local) 还是全局(Global)？解释⽅法是否解释单个实例预测或整个模型⾏为？还是范围介于两者之间？在下⼀节中会有关于范围标准的更多信息。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 23, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性172.3可解释性范围算法训练产⽣预测模型，每个步骤都可以根据透明度(Transparency) 或可解释性进⾏评估。2.3.1算法透明度算法是如何创建模型的？算法透明度是指算法如何从数据中学习模型，以及它可以学习到什么样的关系。如果使⽤卷积神经⽹络对图像进⾏分类，则可以解释该算法在最底层学习边缘检测器和滤波器。这是对算法如何⼯作的理解，但既不是对最终学习的特定模型的理解，也不是对如何做出单个预测的理解。算法的透明度只需要对算法的了解，⽽不需要对数据或学习模型的了解。这本书的重点是模型的可解释性，⽽不是算法的透明度。线性模型的最⼩⼆乘法等算法已被深⼊地研究和理解，它们的特点是透明度⾼。深度学习⽅法(通过具有数百万权重的⽹络推动梯度) 不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3 维的特征空间都是⼈们⽆法想象的。通常，当⼈们试图理解⼀个模型时，他们只考虑其中的⼀部分，例如线性模型中的权重。2.3.3模块层面上的全局模型可解释性模型的某些部分如何影响预测？具有数百个特征的朴素贝叶斯模型对我们来说太⼤了，⾄少很难保存在我们的记忆中。即使我们能够记住所有的权重，我们也⽆法快速预测新的数据点。此外，我们还需要在头脑中有所有特征的联合分布，以估计每个特征的重要性以及特征平均如何影响预测。所以这基本是⼀项不可能的任务。但是你能够很容易地理解⼀个权重。虽然全局模型可解释性通常是⽆法达到的，但⾄少有机会在模', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 24, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可解释部分是权重，对于树来说，是分裂节点和叶节点预测。例如，线性模型看起来似乎可以在模块化层⾯上完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释总是伴随着脚注，即“其他输⼊特征保持相同的值”，这在许多实际应⽤中并不现实。⼀个预测房屋价格的线性模型，考虑到房屋⾯积⼤⼩和房间数量，对于房间数量的特征可能具有负权重。之所以这种情况可能发⽣，是因为已经存在⾼度相关的房屋⼤⼩这个特征。在⼈们更喜欢⼤房间的市场中，如果两个房屋的⾯积相同的话，那么房间少的房屋⽐房间多的房屋更值钱。权重仅在模型中其他特征的上下⽂中有意义。当然，线性模型中的权重仍然可以⽐深层神经⽹络中的权重更好解释。2.3.4单个预测的局部可解释性为什么模型会对⼀个实例做出某种预测？当然，你可以着眼于⼀个实例，检查模型对某个输⼊的预测，并解释原因。如果你查看单个预测，那么这个原本复杂的模型的⾏为可能会更令⼈愉悦。在局部上，预测可能只依赖于线性或单调的某些特征，⽽不是对它们有着复杂的依赖性。例如，房屋的价格可能与它的⾯积⼤⼩成⾮线性关系。但是，如果我们只查看⼀个特定的100 平⽅⽶的房屋，那么对于该数据⼦集，模型预测可能与⾯积成线性关系。你可以通过模拟当增加或减少10 平⽅⽶的⾯积尺⼨时，预测的价格是如何变化的来发现这⼀点。因此，局部解释⽐全局解释更准确。在后⾯的介绍中，在“模型⽆关⽅法” ⼀章可以使单个实例的预测更容易解释。2.3.5一组预测的局部可解释性为什么模型对⼀组实例进⾏特定的预测？多个实例的模型预测可以⽤全局模型解释⽅法(模块级别) 或单个实例的解释来解释。全局⽅法可以通过获取实例组，将其视为⼀个完整的数据集以及使⽤全局⽅法处理这个数据集。也可以对每个实例使⽤单独的局部解释⽅法，然后为整个组列出其结果或对结果进⾏聚合。2.4可解释性评估对于机器学习中的可解释性⾄今没有达成共识，如何衡量也不清楚。但有⼀些初步的研究，并试图制定⼀些评估⽅法。Doshi Velez 和Kim (2017) 为评估可解释性提出了三个主要层次：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 25, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性19应用级评估(实际任务) (Application Level Evaluation)：将解释放⼊产品中，由最终⽤户进⾏测试。想象⼀下，带有机器学习组件的⼀个⾻折检测软件，它可以定位和标记X 光⽚中的⾻折。在应⽤层⾯，放射科医⽣将直接测试⾻折检测软件来评估模型，这需要⼀个良好的实验装置和对如何评估质量的理解。⼀个很好的基准是⼈类在解释相同决策时的表现。人员级评估(简单任务) (Human Level Evaluation)：简化的应⽤级评估。不同的是，这些实验不是由领域专家进⾏的，⽽是由⾮专业⼈员进⾏的。这使得实验更廉价(特别是如果领域专家是放射科医⽣的话)，并且更容易找到更多的测试⼈员。例如，向⽤户展⽰不同的解释，⽽⽤户会选择最好的解释。功能级评估(代理任务) (Function Level Evaluation)：不需要⼈⼯。当所使⽤的模型类已经由其他⼈在⼈员级评估中进⾏了评估时，这是最有效的。例如，可能知道最终⽤户了解决策树。在这种情况下，树的深度可能可以来表⽰解释质量的好坏。较短的树将获得更⾼的可解释性得分。增加这种约束条件是有意义的：与较深的树相⽐，树的预测性能保持良好且不会降低太多。下⾯我们将着重于对功能级上的单个预测的解释进⾏评估。我们将在评估中考虑解释的相关性质是什么？2.5解释的性质我们要解释机器学习模型的预测。为了实现这⼀点，我们依赖于某个解释⽅法，⼀种⽣成解释的算法。解释(Explanation) 通常以一种人类可理解的方式将实例的特征值与其模型预测联系起来。其他类型的解释包括⼀组数据实例(例如，对于k-最近邻模型)。例如，我们可以使⽤⽀持向量机预测癌症风险并使⽤局部代理⽅法(如使⽤决策树) 来解释预测结果；或者我们可以⽤线性回归模型作为⽀持向量机的代理，毕竟线性回归模型的权重可⽤于解释。我们仔细研究了解释⽅法和解释的性质(Robnik-Sikonja 和Bohanec，2018[8])，这些性质可⽤于判断解释⽅法或解释的好坏。但针对这些性质，如何正确地衡量它们尚不清楚，因此⽬前的⼀个挑战是如何规范地计算它们。解释方法的性质• 表达能力(Expressive Power)：是该⽅法能够产⽣的解释的“语⾔” 或结构。解释⽅法可以⽣成IF-THEN 规则、决策树、加权和、⾃然语⾔或其他东西。• 半透明度(Translucency)：描述了解释⽅法依赖于查看机器学习模型(如其参数) 的程度。例如，依赖于本质上可解释模型(如线性回归模型，这是特定于模型的) 的解释⽅法是⾼度透明的。⽽⽅法仅依赖于修改输⼊和观察预测，其半透明度为零。根据具体情况，可能需要不同程度的半透明度。⾼半透明度的优点是该⽅法可以依赖更多的信息来⽣成解释。低半透明度', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 26, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性20的优点是解释⽅法更易于移植。• 可移植性(Portability)：描述了使⽤解释⽅法的机器学习模型的范围。低半透明度的⽅法具有较⾼的可移植性，因为它们将机器学习模型视为⿊盒。代理模型可能是具有最⾼可移植性的解释⽅法。⽽仅适⽤于递归神经⽹络的⽅法具有低可移植性。• 算法复杂度(Algorithmic Complexity)：描述了⽣成解释的⽅法的计算复杂性。当计算时间成为⽣成解释的瓶颈时，必须考虑此性质。单个解释的性质• 准确性(Accuracy)：解释预测看不见的数据会如何？如果将解释代替机器学习模型进⾏预测，那么⾼准确性尤为重要。如果机器学习模型的准确性也很低，并且⽬标是解释⿊盒模型的作⽤，那么低准确性就很好了。在这种情况下，只有保真度才是重要的。• 保真度(Fidelity)：解释对⿊盒模型预测的近似程度如何？⾼保真度是解释的重要性质之⼀，毕竟低保真度的解释对解释机器学习模型是⽆⽤的。准确性和保真度密切相关。如果⿊盒模型具有较⾼的准确性并且解释有⾼保真度，则解释也具有较⾼的准确性。⼀些解释只提供局部保真度，这意味着该解释仅⾮常适合于数据⼦集的模型预测(例如局部代理模型)，甚⾄仅适⽤于单个数据实例(例如Shapley 值)。• 一致性(Consistency)：经过相同任务训练并产⽣相似预测的模型之间的解释有多少不同？例如，我们在同⼀个任务上训练⽀持向量机和线性回归模型，两者都产⽣⾮常相似的预测。然后我们选择⼀种解释⽅法去计算解释，并分析这些解释之间的差异。如果解释⾮常相似，说明是⾼度⼀致的。但这个性质可能会有点棘⼿，因为这两个模型可以使⽤不同的特征，但得到相似的预测(也叫“罗⽣门效应”)。在这种情况下，⾼度⼀致性又是不可取的，因为解释必须⾮常不同。但如果模型确实依赖于相似的关系，则需要⾼⼀致性。• 稳定性(Stability)：类似实例之间的解释会有多相似？⼀致性是⽐较模型之间的解释，⽽稳定性则⽐较同⼀模型的相似实例之间的解释。⾼稳定性意味着实例特征的细微变化基本上不会改变解释(除⾮这些细微变化也会强烈改变预测)。缺乏稳定性可能是解释⽅法差异很⼤的结果。换句话说，解释⽅法受到待解释实例的特征值的微⼩变化的强烈影响。解释⽅法的不确定性部分也可能导致稳定性不⾜，例如数据采样步骤，就像局部代理模型使⽤的那样。⾼稳定性始终是可取的。• 可理解性(Comprehensibility)：⼈类对解释的理解程度如何？这很难定义和衡量，但⾮常重要。⽐较容易接受的观点是可理解性取决于读者和观众。衡量可理解性的想法包括测量解释的⼤⼩(线性模型中⾮零权重的特征的数量，决策规则的数量等等) 或测试⼈们如何从解释中预测机器学习模型的⾏为。还应考虑解释中使⽤的特征的可理解性，特征的复杂转换可能还不如原来的特征容易理解。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 27, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多机器学习模型只给出预测，⽽没有关于预测正确的模型置信度的描述。如果模型预测⼀个病⼈患癌症的概率为4%，那么是否可以确定另⼀位特征值不同的病⼈患癌症的概率为4%？⼀个包含模型确定性的解释是⾮常有⽤的。• 重要程度(Degree of Importance)：解释在多⼤程度上反映了解释的特征或部分的重要性？例如，如果⽣成决策规则作为对单个预测的解释，那么是否清楚该规则的哪个条件最重要？• 新颖性(Novelty)：解释是否反映了待解释的数据实例来⾃远离训练数据分布的“新” 区域？在这种情况下，模型可能不准确，解释可能毫⽆⽤处。新颖性的概念与确定性的概念有关。新颖性越⾼，由于缺乏数据，模型的确定性就越低。• 代表性(Representativeness)：⼀个解释能覆盖多少个实例？解释可以覆盖整个模型(例如线性回归模型中的权重解释)，也可以只表⽰单个预测。2.6人性化的解释让我们更深⼊挖掘，发现⼈类所认为的“好的” 解释，以及对可解释机器学习的意义。⼈⽂科学研究可以帮助我们找到答案。Miller (2017) 对有关解释的出版物进⾏了⼤量调查研究，本节以他的总结为基础。在这⼀节中，我想让你相信以下内容：作为事件的解释，⼈类更喜欢简短的解释(只有1 或2 个原因)，这些解释将当前的情况与事件不会发⽣的情况进⾏了对⽐，特别是异常原因提供了很好的解释。解释是解释者与被解释者(解释的接收者) 之间的社会互动，因此社会背景对解释的实际内容有很⼤的影响。当你考虑需要⼀个预测或⾏为的所有因素的解释时，你不需要⼈性化的解释，⽽是完整的因果归因。如果在法律上要求你指出所有有影响的特征或调试机器学习模型，则可能需要因果归因。在这种情况下，请忽略以下⼏点。⽽在所有的其他情况下，当外⾏⼈或时间很少的⼈作为解释的接收者时，则以下部分对你来说应该很有趣。2.6.1什么是解释解释是“为什么” 这个问题的答案(Miller，2017)。• 为什么治疗不起作⽤？• 为什么我的贷款被拒绝了？• 为什么外星⼈还没有联系我们？', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 28, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性22前两个问题可以⽤“⽇常” 的解释来回答，⽽第三个问题则来⾃“更普遍的科学现象和哲学问题”。我们关注“⽇常” 类型的解释，因为这些解释与可解释机器学习相关。以“如何” 开头的问题通常可以改为“为什么” 问题：“我的贷款是如何被拒绝的？” 可以变成” 为什么我的贷款被拒绝了？”。在下⽂中，“解释” (Explanation) ⼀词是指解释的社会和认知过程，也指这些过程的产物。解释者(Explainer) 可以是⼈或机器。2.6.2什么是好的解释？本节进⼀步浓缩了Miller 关于“好的” 解释的总结，并为可解释机器学习添加了具体含义。1. 解释具有对比性(Lipton，1990[9])。⼈类通常不会问为什么会做出某种预测，但会问为什么会做出这种预测⽽不是另⼀种预测。我们倾向于在反事实的情况下思考，即“如果输⼊X 不同，预测会是怎样的？”。对于房屋价格预测，业主可能会感兴趣，为什么预测价格⽐他们预期的低价要⾼。如果我的贷款申请被拒绝，我不想听到所有的⽀持或反对的因素。⽽是我对申请中需要更改才能获得贷款的因素感兴趣。我想知道我的申请和我能被接受的申请之间的对⽐。认识到对⽐性的解释是可解释机器学习的⼀个重要发现。从⼤多数可解释的模型中，你可以提取⼀个解释，它隐式地将实例的预测与⼈⼯数据实例或⼀些实例平均值进⾏对⽐。医⽣可能会问：“为什么这种药对我的病⼈不起作⽤？”。他们可能需要⼀种解释，将他们的病⼈与药物起作⽤的病⼈以及与药物不作⽤的病⼈进⾏对⽐。对⽐性的解释⽐完整的解释更容易理解。医⽣对药物为什么不起作⽤的问题的⼀个完整的解释可能包括：病⼈已经有10 年的疾病，11 个基因过表达，病⼈⾝体很快将药物分解成⽆效的化学物质等等。但⼀个对⽐的解释可能是更简单：与有反应的患者相⽐，⽆反应的患者具有⼀定的基因组合，使药物的疗效降低。最好的解释是强调感兴趣的对象和参照对象之间最⼤的差异。它对于可解释机器学习意味着什么：⼈类不希望对预测有⼀个完整的解释，⽽是希望将不同之处与另⼀个实例(可以是⼈⼯的) 的预测进⾏⽐较。创建对⽐性的解释依赖于应⽤程序，因为它需要⼀个参照点来进⾏⽐较。这可能取决于要解释的数据点，也取决于接受解释的⽤户。⼀个房屋价格预测⽹站的⽤户可能想要得到房屋价格预测的解释，使其能将他们⾃⼰的房屋或⽹站上的另⼀个房屋或附近的⼀个普通房屋形成对⽐。⾃动创建对⽐性解释的解决⽅案还可能涉及在数据中寻找原型。2. 选择性的解释。⼈们不希望对涵盖事件的实际原因和完整原因进⾏解释。我们习惯于从各种可能的原因中选择⼀个或两个原因作为解释。作为证明，打开电视新闻：“股票价格的下跌被归咎于由于最新软件更新的问题⽽对该公司产品越来越强烈的反弹。”“Tsubasa 和他的球队因为防守薄弱⽽输掉了⽐赛：他们给对⼿太多的空间来发挥他们的战术。”“对既有机构和政府的不信任感⽇增是选民投票率降低的主要因素。”⼀个事件可以由各种原因解释，则被称为“罗⽣门效应”。《罗⽣门》是⼀部⽇本电影，讲述了有关武⼠之死的另类⽭盾故事。对于机器学习模型，如果能根据不同的特征做出⼀个好的预测是有利的。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 29, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性23将使⽤不同的特征(不同的解释) 的多个模型结合在⼀起的集成⽅法通常表现良好，进⾏平均可以使预测更加鲁棒和准确。但这也意味着有不⽌⼀个选择性的解释——为什么做出了某种预测。它对可解释机器学习意味着什么：解释要简短，即使真实情况很复杂，但只给出1 到3 个原因。LIME 在这⽅⾯就做得很好。3. 解释是社会性的。它们是解释者和解释的接收者之间对话或交互的⼀部分。社会背景决定了解释的内容和性质。如果我想向技术⼈员解释为什么数字加密货币价值如此之⾼，我会这样说：“分散的、分布式的、基于区块链的账本，不能由⼀个中央实体控制，与那些想确保财富安全的⼈产⽣共鸣。” 这就解释了。但对我的祖母，我会说：“看，祖母，加密货币有点像计算机黄⾦。⼈们喜欢并会花很多钱买黄⾦，年轻⼈喜欢并会花很多钱买计算机黄⾦。”它对可解释机器学习意味着什么：注意机器学习应⽤程序的社会环境和⽬标受众。正确地使⽤机器学习模型的社交部分完全取决于你的特定应⽤程序。这⽅⾯可以找⼈⽂学科的专家(如⼼理学家和社会学家) 帮助你。4. 解释的重点是异常。⼈们更关注异常原因来解释事件(Kahnemann 和Tversky，1981[10])。这些原因发⽣的可能性很⼩，但还是发⽣了。消除这些异常原因将⼤⼤改变结果(反事实解释)。⼈类认为这些“异常” 原因是很好的解释。Trumbelj 和Kononenko (2011)[11] 的⼀个例⼦是：假设我们有教师和学⽣之间的测试情况数据集。学⽣们参加⼀门课程，并在成功做⼀个演⽰后直接通过该课程。⽼师可以额外选择通过问学⽣问题的⽅式来测试学⽣的知识。回答不上这些问题的学⽣将不及格。学⽣可以有不同程度的准备，这意味着可以正确回答⽼师问题的概率也不同。我们要预测⼀个学⽣是否会通过这门课，并解释我们的预测。如果⽼师没有提出任何额外的问题，通过的概率是100%，否则通过的概率取决于学⽣的准备⽔平和正确回答问题的概率。情景1：教师通常会向学⽣提出额外的问题(例如，100 次中有95 次)。⼀个没有准备的学⽣(10%的机会通过问题部分) 他不幸得到了额外的问题，但他没有正确回答。学⽣为什么不及格？我想说不准备是学⽣的错。情景2：教师很少问其他问题(例如，100 次中有2 次)。对于⼀个没有为这些问题准备过的学⽣来说，我们预测通过课程的可能性很⾼，因为提额外问题不太可能。当然，其中⼀个学⽣没有准备好这些问题，这给了他10% 的机会通过这些问题。他很倒霉，⽼师又问了⼀些学⽣不能回答的问题，结果他没能通过这门课。失败的原因是什么？我认为现在更好的解释是“因为⽼师测试了学⽣”。⽼师不太可能提问测试，所以⽼师表现异常。这对于可解释机器学习意味着什么：如果⼀个预测的输⼊特征在任何意义上都是异常的(⽐如分类特征的⼀个罕见类别)，并且该特征影响了预测，那么应该将其包括在解释中，即使此时其他“正常” 特征具有对预测的影响与异常预测相同。在我们的房屋价格预测的例⼦中，⼀个不正常的特征可能是⼀个相当昂贵的房⼦有两个阳台。即使某种归因⽅法发现，这两个阳台对价格差异的影响与住房⾯积、良好的邻⾥环境或近期装修⼀样⼤，但“两个阳台” 的异常特征可能是解释为什么房⼦', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 30, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第⼆章可解释性24如此昂贵的最好解释。5. 解释是真实的。事实证明，良好的解释是真实的。但这并不是“好的” 解释的最重要因素。例如，选择性似乎⽐真实性更重要。仅选择⼀个或两个可能原因的解释很少涵盖相关原因的整个列表。选择性忽略了事实的⼀部分。例如，只有⼀个或两个因素导致了股市崩溃，这是不正确的，但事实是，有数百万个原因影响着数百万⼈的⾏事⽅式，最终导致了股市崩溃。它对可解释机器学习意味着什么：解释应该尽可能真实地预测事件，在机器学习中有时被称为保真度。所以如果我们说第⼆个阳台增加了⼀套房屋的价格，那么这也应该适⽤于其他的房屋(或⾄少适⽤于类似房屋)。对⼈类来说，解释的保真度不如它的选择性、对⽐性和社会性重要。6. 好的解释与被解释者的先验知识是一致的。⼈类往往忽视与他们先验知识不⼀致的信息，这种效应被称为确认偏差(Confirmation Bias) (Nickerson，1998[12])。这种偏差不能幸免，⼈们往往会贬低或忽视与他们先验知识不⼀致的解释。这套先验知识因⼈⽽异，但也有基于群体的先验知识，如政治世界观。它对可解释机器学习意味着什么：“好的” 解释与先验知识是⼀致的。这很难整合到机器学习中，可能会⼤⼤损害预测性能。我们先前认为房屋⾯积⼤⼩对预测价格的影响是，房屋越⼤，价格越⾼。假设⼀个模型还显⽰了房屋⼤⼩对⼀些房屋的预测价格的负⾯影响。模型之所以了解到这⼀点，是因为它提⾼了预测性能(由于⼀些复杂的交互作⽤)，但这种⾏为与我们先验知识强烈⽭盾。你可以强制执⾏单调性约束(⼀个特征只能影响⼀个⽅向的预测)，或者使⽤具有此性质的线性模型之类的东西。7. 好的解释是普遍性的和很可能的。可以解释许多事件的原因是⾮常普遍的，可以被认为是⼀个好的解释。请注意，这与认为异常原因能够做出好的解释的说法相⽭盾。如我所见，异常原因胜过普遍原因。根据定义，在给定的情况下，异常原因是罕见的。在没有异常事件的情况下，普遍性的解释被认为是⼀个好的解释。还要记住的是，⼈们往往会误判共同事件的可能性(例如，Joe 是图书管理员，他更可能成为⼀个害羞的⼈还是⼀个喜欢读书的害羞的⼈？)。对于这类解释，⼀个很好的例⼦是“房屋之所以昂贵是因为它很⼤”，这是⼀个⾮常普遍性的、很好的解释——为什么房屋昂贵或便宜。它对可解释机器学习意味着什么：普遍性可以很容易地通过特征的“⽀持” 来衡量，即解释应⽤到的实例数除以实例总数。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 31, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第三章数据集在整本书中，所有的模型和技术都是应⽤在可在线免费获得的真实数据集上。我们将为不同的任务使⽤不同的数据集：分类，回归和⽂本分类。3.1自行车租赁(回归)该数据集包含来⾃华盛顿特区的⾃⾏车租赁公司Capital-Bikeshar 的⾃⾏车租赁的每⽇计数，以及天⽓和季节信息。该数据由Capital-Bikeshare 公开提供。Fanaee-T 和Gama (2013)[13] 添加了天⽓数据和季节信息。⽬的是根据天⽓和天数来预测将租⽤多少辆⾃⾏车。这些数据都可以从UCI机器学习数据库⾥下载。有许多新特征被添加到数据集中，但本书的⽰例并没有使⽤所有原始特征，下⾯列出使⽤到的特征列表：• ⾃⾏车租赁的数量，包括来⾃于游客⽤户和注册⽤户。这个数量也是回归任务的预测⽬标。• 季节，包括春、夏、秋、冬。• 指⽰⼀天是否为假期。• 年份，2011 年或2012 年。• ⾃2011 年1 ⽉1 ⽇(数据集中的第⼀天) 起的天数。引⼊此特征是为了考虑随时间变化的趋势。• 指⽰⼀天是⼯作⽇还是周末。• 当天的天⽓状况。是下⾯⼏种情况中的⼀个:– 晴，少云，部分多云，多云– 雾+ 云，雾+ 碎云，雾+ 少云，雾– ⼩雪，⼩⾬+ 雷⾬+ 散云，⼩⾬+ 散云– ⼤⾬+ 冰雹+ 雷⾬+ 薄雾，雪+ 薄雾• 温度(摄⽒度)。• 相对湿度百分⽐(0 到100)。• 风速，单位：km/h。25', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 32, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第三章数据集26对于本书中的⽰例，数据已进⾏了少量处理。你可以在本书的Github 存储库中找到正在处理的R脚本以及最终的RData ⽂件。3.2YouTube 垃圾评论(文本分类)以⽂字分类为例，我们使⽤了来⾃5 个不同YouTube 视频的1956 条评论。值得庆幸的是，在有关垃圾评论分类的⽂章中使⽤此数据集的作者免费提供了这些数据(Alberto，Lochter 和Almeida，2015[14])。这些评论是通过YouTube API 从2015 年上半年YouTube 上观看次数最多的⼗个视频中的五个收集的。所有五个都是⾳乐视频。其中之⼀就是韩国艺术家Psy 创作的“Gangnam Style”。其他艺术家是Katy Perry，LMFAO，Eminem 和Shakira。查看⼀些评论。这些评论被⼿动标记为垃圾评论或正常评论。垃圾评论的编码为“1”，正常评论的编码为“ 0”。评论类别Huh, anyway check out this you channel: kobyoshi021Hey guys check out my new channel and our first vid THIS IS US THEMONKEYS!!! I’m the monkey in the white shirt, please leave a like commentand please subscribe!!!!1just for test I have to say murdev. com1me shaking my sexy ass on my channel enjoy ˆ_ˆ1watch?v=vtaRGgvGtWQ Check this out .1Hey, check out my new website!! This site is about kids stuff. kidsmediausa .com1Subscribe to my channel1i turned it on mute as soon is i came on i just wanted to check the views…0You should check my channel for Funny VIDEOS!!1and u should.d check my channel and tell me what I should do next!1你也可以转到YouTube 并查看评论部分。但是可千万别舍本逐末，最终观看了猴⼦从海滩上的游客那⾥偷喝鸡尾酒的视频。⾃2015 年以来，⾕歌垃圾评论探测器也可能发⽣了很⼤变化。点击观看Psy「江南Style」如果你想使⽤数据，可以在本书的Github 存储库中找到RData ⽂件以及R 脚本。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 33, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第三章数据集273.3宫颈癌的危险因素(分类)宫颈癌数据集包含预测⼥性是否会患宫颈癌的指标和危险因素。这些特征包括⼈⼜统计学数据(如年龄)、⽣活⽅式和病史。数据可从UCI 机器学习库下载，并由Fernandes、Cardoso 和Fernandes(2017) 整理[15]。本书中使⽤到的数据集的部分特征如下：• 年龄(岁)• 性伴侣数量• ⾸次性⾏为(岁)• 怀孕次数• 是否吸烟• 烟龄(单位：年)• 是否服⽤激素避孕药• 服⽤激素避孕药的时间(单位：年)• 是否有宫内节育器(IUD)• 使⽤宫内节育器(IUD) 的时间(年数)• 是否患有性传播疾病(STD)• 性病诊断次数• 第⼀次性病诊断后到现在的时间• 上次性病诊断到现在的时间• 活检结果为“健康” 或“癌症”。这是⽬标输出。活检结果作为判断是否患癌症的最终结果。对于本书中的例⼦，活检结果被⽤作⽬标。数据中每列的缺失值都是由众数(最常见的值) 来代替，这可能并⾮是⼀个好办法，因为真正的答案可能与某个值缺失的概率相关。可能会有偏差，因为这些问题是⾮常私⼈的。但这并不是⼀本关于缺失数据插补的书，所以我们必须认为众数插补是⾜以作为回归分析来使⽤的。要使⽤该数据集重现本书的⽰例，请在本书的Github 存储库中找到预处理的R 脚本和最终的RData ⽂件。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 34, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型实现可解释性的最简单⽅法是只使⽤创建可解释模型的算法⼦集。线性回归、逻辑回归和决策树是常⽤的可解释模型。在接下来的内容中，我们将讨论这些模型。没有详细介绍，仅提供基础知识，因为已经有⼤量书籍、视频、教程、论⽂和更多材料可供使⽤。我们将专注于如何解释模型。该书更详细地讨论了线性回归、逻辑回归、其他线性回归扩展、决策树、决策规则和RuleFit 算法，还列出了其他可解释的模型。除k-最近邻算法外，本书中解释的所有可解释的模型都可以在模块级别上解释。下表概述了可解释的模型类型及其性质。如果特征和⽬标之间的关联是线性建模的，那么模型就是线性的。具有单调性约束的模型可确保特征和⽬标结果之间的关系在整个特征范围内始终朝着相同的⽅向：特征值的增加要么总是导致⽬标结果的增加，要么总是导致⽬标结果的减少。单调性对于模型的解释是有⽤的，因为它使理解关系变得更容易。⼀些模型可以⾃动地包含特征之间的交互，来预测⽬标结果。你可以通过⼿动创建交互特征，可以将交互包括在任何类型的模型中。交互可以提⾼预测性能，但太多或太复杂的交互都会损害可解释性。有些模型只处理回归，有些只处理分类，还有⼀些模型两者都处理。下⾯这个表你可以⽤来为任务选择合适的可解释模型，⽆论回归(regr) 问题还是分类(class) 问题：算法线性单调性交互任务线性回归YesYesNoregr逻辑回归NoYesNoclass决策树NoSomeYesclass,regrRuleFitYesNoYesclass,regr朴素贝叶斯NoYesNoclassk-最近邻NoNoNoclass,regr28', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 35, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型294.1线性回归线性回归(Linear Regression) 模型将⽬标预测为特征输⼊的加权和，⽽所学习关系的线性使解释变得容易。统计学家、计算机科学家以及其他解决定量问题的⼈长期以来都使⽤线性回归模型。线性模型可⽤于建模回归⽬标y 对某些特征x 的依赖性。由于学到的关系是线性的，可以针对第i个实例写成如下：y = β0 + β1x1 + . . . + βpxp + ϵ实例的预测结果是其p 个特征的加权和。参数βj 表⽰要学习的特征权重或系数，其中第⼀项β0称为截距，不与特征相乘。ϵ 表⽰误差，即预测结果与真实结果之间的差。假设这些误差服从⾼斯分布，这意味着我们在正负⽅向上都会产⽣误差，并且会产⽣很多⼩的误差和少量⼤的误差。可以使⽤各种⽅法来估计最佳权重。通常使⽤最⼩⼆乘法来找到使真实结果和预测结果之间平⽅差最⼩化的权重：ˆβ = arg minβ0,...,βpn∑i=1\\uf8eb\\uf8edy(i) −\\uf8eb\\uf8edβ0 +p∑j=1βjx(i)j\\uf8f6\\uf8f8\\uf8f6\\uf8f82我们不会详细讨论如何找到最佳权重，但如果你感兴趣，可以阅读《The Elements of StatisticalLearning》[1] ⼀书的第3.2 章或者线性回归模型的其他⽹上资源。线性回归模型的最⼤优点是线性：它使估计过程变得简单，⽽最重要的是，这些线性⽅程在模块化⽔平(即权重) 上具有易于理解的解释。这是线性模型和所有类似模型在医学、社会学、⼼理学等许多定量研究领域应⽤如此⼴泛的主要原因之⼀。例如，在医学领域，不仅要预测病⼈的临床结果，⽽且要量化药物的影响，同时以可解释的⽅式考虑性别、年龄和其他特征。估计的权重带有置信区间。置信区间是权重估计的范围，它以⼀定的置信度覆盖“真实” 权重。例如，权重为2 的95% 置信区间可能为1 到3。这个区间的解释是：如果我们⽤新的抽样数据重复估计100 次，假设线性回归模型是正确的数据模型情况下，置信区间将包括100 个案例中95 个的真实权重。模型是否为“正确” 模型取决于数据中的关系是否满⾜某些假设，即线性、正态性、同⽅差性、独⽴性、固定特征和不存在多重共线性。线性线性回归模型使预测成为特征的线性组合，这既是其最⼤的优势，也是其最⼤的局限。线性导致其为可解释模型。线性效应易于量化和描述，它们是可加的，因此很容易分离效应。如果你怀疑有特征交互或特征与⽬标值的⾮线性关联，则可以考虑添加交互项或使⽤回归样条。正态性', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 36, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型30假设给定特征的⽬标结果服从正态分布。如果违反此假设，则特征权重的估计置信区间⽆效。同方差性假设误差项的⽅差在整个特征空间内是恒定的。例如，假设你要根据居住⾯积(平⽅⽶) 来预测房屋的价格。你估计⼀个线性模型，该线性模型假设不管房屋⾯积⼤⼩如何，预测响应周围的误差具有相同的⽅差。这种假设在现实中经常被违背。在房屋⽰例中，对于较⼤的房屋，由于价格较⾼且存在更⼤的价格波动空间，因此在较⼤的房屋周围，围绕预测价格的误差项的⽅差较⾼可能是合理的。假设线性回归模型中的平均误差(预测价格和真实价格之间的差异) 为50,000 欧元。如果假设同⽅差，则对于成本为100 万的房屋和成本仅为40,000 的房屋，平均误差都为50,000。显然这是不合理的，因为后者我们甚⾄能得到⼀个负的价格。独立性假设每个实例独⽴于任何其他实例。如果执⾏重复测量，例如每个患者进⾏多次⾎液测试，则样本点(数据点) 不是独⽴的。对于相关样本，你就需要特殊的线性回归模型，如混合效应模型(MixedEffect Model) 或GEE。如果使⽤正常的线性回归模型，可能会从模型中得出错误的结论。固定特征输⼊特征被认为是“固定的”。固定意味着它们被视为“给定常数”，⽽不是统计变量。这意味着它们没有测量误差。这是⼀个相当不切实际的假设。然⽽，如果没有这个假设，就将不得不拟合⾮常复杂的测量误差模型，这些模型会考虑输⼊特征的测量误差。当然通常你不想这样做。不存在多重共线性你不需要强相关的特征，因为这会扰乱对权重的估计。在两个特征强相关的情况下，由于特征效应(Feature Effects，也可以称为特征影响) 是累加的，因此估计权重变得很困难，并且⽆法确定哪⼀个相关特征归因了效应。4.1.1解释线性回归模型中权重的解释取决于相应特征的类型。• 数值特征：将数值特征增加⼀个单位会根据其权重改变估计结果。例如，房屋⾯积⼤⼩。• ⼆分类特征：每个实例都采⽤两个可能值之⼀的特征。例如，“房屋有花园” 特征(⽤1 编码)，⽽另⼀个值被视为参照类别，例如“房屋没有花园” (⽤0 编码)。将特征从参照类别更改为其他类别会根据特征的权重改变估计结果。• 具有多个类别的分类特征：具有固定数量的可能值的特征。例如，“地板类型” 特征，可能的类别为“地毯”、“层压板” 和“镶⽊地板”。⼀种处理多种类别的解决⽅案是one-hot 编码，这', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 37, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型31意味着每个类别都有⾃⼰的⼆进制列。对于具有L 个类别的分类特征，你只需要L −1 列，因为第L 列将具有冗余信息(例如，当列1 到L −1 的值都为0 时，我们知道此实例的分类特征为第L 个类别)。然后对每个类别的解释与对⼆分类特征的解释相同。某些语⾔(例如R)允许你以各种⽅式对分类特征进⾏编码，如本节稍后所述。• 截距项β0：截距是“常量特征” 的特征权重，对于所有实例都是1。⼤多数软件包会⾃动添加“1” 这个特征来估计截距。解释是：对于所有数值特征为零和分类特征为参照类别下的实例，模型预测是截距权重。截距的解释通常不相关，因为所有特征值都为零的实例通常没有意义。只有当特征标准化(均值为0，标准差为1) 时，这种解释才有意义。然后，截距就将反映当所有特征都处于其均值时的实例的预测结果。线性回归模型中特征的解释可以通过使⽤以下⽂本模板⾃动进⾏。数值特征的解释当所有其他特征保持不变时，特征xk 增加⼀个单位，预测结果y 增加βk。分类特征的解释当所有其他特征保持不变时，将特征xk 从参照类别改变为其他类别时，预测结果y 会增加βk。解释线性模型的另⼀个重要度量是R-平⽅度量(R-squared Measurement)。R-平⽅告诉你模型解释了⽬标结果的总⽅差中的多少。R-平⽅越⾼，模型对数据的解释就越好。R-平⽅的计算公式为：R2 = 1 −SSE/SSTSSE 是误差项的平⽅和：SSE =n∑i=1(y(i) −ˆy(i))2SST 是数据⽅差的平⽅和：SST =n∑i=1(y(i) −¯y)2SSE 会告诉你在拟合线性模型后还有多少⽅差，该⽅差是是通过预测结果和真实结果之间的平⽅差来衡量的。SST 是⽬标结果的总⽅差。R-平⽅告诉你有多少⽅差可以⽤线性模型来解释。对于根本⽆法解释数据的模型，R-平⽅的值为0；对于解释数据中所有⽅差的模型，R-平⽅的值为1。这⾥有⼀个陷阱，因为R-平⽅随模型中的特征数量的增加⽽增加，即便它们不包含任何关于⽬标值的信息也是如此。因此，最好使⽤调整后的R-平⽅，它考虑了模型中使⽤的特征数量。其计算如下：¯R2 = 1 −(1 −R2)n −1n −p −1其中p 是特征的数量，n 是实例的数量。解释⼀个(调整后的) R-平⽅很低的模型是没有意义的，因为这样的模型基本上不能解释⼤部分的⽅差，对权重的任何解释都没有意义。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 38, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型32特征重要性在线性回归模型中某个特征的重要性可以⽤它的t-统计量(t-statistic) 的绝对值来衡量。t-统计量是以标准差为尺度的估计权重。tˆβj =ˆβjSE(ˆβj)此公式告诉我们的内容：特征的重要性随着权重的增加⽽增加。估计权重的⽅差越⼤(= 我们对正确值的把握越⼩)，特征就越不重要。这也是有道理的。4.1.2示例在此⽰例中，给定天⽓和⽇历信息的情况下，我们使⽤线性回归模型来预测特定⽇期的⾃⾏车租赁数量。为了便于解释，我们检查了估计的回归权重。这些特征包括数值特征和分类特征。对于每个特征，该表显⽰估计的权重、估计的标准差(SE) 和t-统计量的绝对值(|t|)。WeightSE|t|(Intercept)2399.4238.310.1seasonSUMMER899.3122.37.4seasonFALL138.2161.70.9seasonWINTER425.6110.83.8holidayHOLIDAY-686.1203.33.4workingdayWORKING DAY124.973.31.7weathersitMISTY-379.487.64.3weathersitRAIN/SNOW/STORM-1901.5223.68.5temp110.77.015.7hum-17.43.25.5windspeed-42.56.96.2days_since_20114.90.228.5数值特征(温度特征/temp) 的解释：当所有其他特征保持不变时，将温度升⾼1 摄⽒度，⾃⾏车的预测数量增加110.7 。分类特征(天⽓状况特征/weathersit) 的解释：与好天⽓相⽐，当下⾬、下雪或暴风⾬时，⾃⾏车的估计数量减少了1901.5；再次假设所有其他特征不变，当天⽓有雾时，⾃⾏车的预测数量⽐正常天⽓少了379.4 。所有的解释总是伴随“所有其他特征保持不变”，这是因为线性回归模型的性质。预测⽬标是加权特征的线性组合。估计的线性⽅程是特征/⽬标空间中的超平⾯(在单个特征的情况下为线)。权重', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 39, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型33指定每个⽅向上超平⾯的斜率(梯度)。好的⼀⽅⾯是，可加性将单个特征的解释与所有其他特征隔离开来。这是可能的，因为⽅程式中的所有特征效应(= 权重乘以特征值) 都是⽤加号组合在⼀起。坏的⼀⾯是，这种解释忽略了特征的联合分布。增加⼀个特征，但不改变另⼀个特征，这可能不合实际或者是不太可能的数据点。例如，如果不增加房屋的⾯积⼤⼩，却增加房间的数量就可能是不现实的。4.1.3可视化解释各种可视化效果使线性回归模型易于快速掌握。权重图(Weight Plot)权重表的信息(权重和⽅差估计) 可以在权重图中可视化。下图显⽰了先前线性回归模型的结果。图4.1. 权重显⽰为点，95% 置信区间显⽰为线。权重图显⽰，下⾬/下雪/暴风⾬天⽓对预测的⾃⾏车数量有很⼤的负效应。“⼯作⽇” 特征(workingday) 的权重接近于零，并且95% 的区间中包含零，这意味着该效应在统计上不显著。⼀些置信区间很短，估计值接近于零，但特征效应在统计上是显着的。温度就是这样的⼀个例⼦。权', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 40, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型34重图的问题在于特征是在不同的尺度上测量的。虽然对于天⽓状况，估计的权重反映了好天⽓和下⾬/暴风⾬/下雪天⽓之间的差异，但温度只反映了1 摄⽒度的增加。在拟合线性模型之前，可以通过缩放特征(0 均值和1 标准差)，使估计的权重更具可⽐性。效应图(Effect Plot)当线性回归模型的权重与实际特征值相乘时，可以更有意义地进⾏分析。权重取决于特征的⽐例，⽐如有⼀个测量⼈的⾝⾼的特征，如果测量单位从⽶转换到厘⽶，那么权重会有所不同。权重会改变，但在数据中的实际效应不会改变。了解数据中特征的分布也是很重要的，因为如果⽅差⾮常⼩，这意味着这个特征⼏乎在所有的实例中都有类似的贡献。效应图可以帮助了解权重和特征的组合对数据预测的贡献程度。⾸先计算特征效应(也可称特征影响)，即每个特征的权重乘以实例的特征值：effect(i)j= wjx(i)j使⽤箱线图可以可视化效应。箱线图中的框包含⼀半数据的特征效应范围(效应值的1/4 到3/4 分位数)。框中的垂直线是中位数( 50% 的实例对预测的影响⼩于此值，另⼀半⾼于此值)。⽔平线延伸到±1.5IQR/√n，其中IQR 是四分位数之间的范围(3/4 分位数减去1/4 分位数)。这些点是离群点。与每个类别都单独⼀条线的权重图相⽐，分类特征的效应可以总结为⼀个单独的箱线图。图4.2. 特征效应图显⽰了每个特征的效应分布(= 特征值乘以特征权重)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 41, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型35对租⽤⾃⾏车预期数量的最⼤贡献来⾃于温度特征和天数特征，后者反映了⾃⾏车租赁随时间的趋势。温度在很⼤程度上有助于预测。天数特征从零到⼤的正的贡献，因为数据集中的第⼀天(2011年1 ⽉1 ⽇) 具有⾮常⼩的特征效应，并且该特征的估计权重为正(4.93)。这意味着该特征效应每天都在增加，并且在数据集中的最后⼀天最⾼(2012 年12 ⽉31 ⽇)。要注意负权重的特征效应，对于这些负权重，具有负特征值的实例表现出来的是正的效应。例如，风速有⾼负效应的时候就是风速⾼的⽇⼦。4.1.4解释单个实例预测单个实例的各个特征对预测有多⼤贡献？这可以通过计算这个实例的特征效应得到答案。特定实例的特征效应的解释仅与各个特征的特征效应分布相⽐较才有意义。我们要解释来⾃⾃⾏车数据集的第6 个实例的线性模型的预测。该实例具有以下特征值。FeatureValueseasonSPRINGyr2011mnthJANholidayNO HOLIDAYweekdayTHUworkingdayWORKING DAYweathersitGOODtemp1.604356hum51.8261windspeed6.000868cnt1606days_since_20115为了获得这个实例的特征效应，我们必须将它的特征值乘以线性回归模型中相应的权重。对于“⼯作⽇” 特征的值为WORKING DAY，特征效应为124.9。对于1.6 摄⽒度的温度，特征效应是177.6。我们将这些单独的特征效应作为“叉号” 添加到效应图中，效应图向我们展⽰了数据中的效应分布。这使我们能够将单个实例的特征效应与数据中的效应分布进⾏⽐较。如下图所⽰：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 42, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型36图4.3. ⼀个实例的效应图显⽰了效应分布，并突出了感兴趣实例的特征效应。如果我们对训练数据实例的预测求平均，得到的平均值是4504。相⽐之下，第6 个实例的预测值很⼩，因为预测的只有1571 辆⾃⾏车租⽤。效应图揭⽰了原因。箱线图显⽰数据集所有实例的效应分布，“叉号” 显⽰第6 个实例的效应。第6 个实例的温度特征的特征效应较⼩，因为这⼀天的温度为2 摄⽒度，与其他⼤多数⽇期相⽐较低(记住温度特征的权重为正)。此外，与其他数据实例相⽐，天数特征的特征效应也较⼩，因为该实例⾃第⼀天起仅过了5 天，⽽且这个特征也是正的权重。4.1.5分类特征的编码分类特征的编码有⼏种⽅法，不同的选择会影响权重的解释。线性回归模型的标准是Treatment Coding，这在⼤多数情况下已⾜够。使⽤不同的编码归根结底就是从具有分类特征的单个列中创建不同的(设计) 矩阵。本节介绍了三种不同的编码，但还有更多。使⽤的⽰例有六个实例和三个类别的分类特征。对于前两个实例，该特征采⽤类别A；对于实例3 和4，采⽤类别B；对于后两个实例，采⽤类别C。Treatment Coding', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 43, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型37在Treatment Coding 中，每个类别的权重是对应类别和参照类别之间预测的估计差值。线性模型的截距是参照类别的平均值(当所有其他特征保持不变时)。设计矩阵的第⼀列是截距，它始终是1。第⼆列表⽰实例是否在B 类中，第三列表⽰实例是否在C 类中。A 类不需要列，因为此时只要知道⼀个实例既不属于B 类也不属于C 类就⾜够了，不然线性⽅程会被过度指定，并且找不到权重的唯⼀解。特征矩阵：\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed100100110110101101\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8Effect Coding每个类别的权重是从相应类别到总体均值的估计y 值差(假定所有其他特征为零或参照类别)。第⼀列⽤于估计截距。与截距相关联的权重β0 表⽰总体均值，第⼆列的权重β1 表⽰总体均值与B 类之间的差异。B 类的总体效应为β0 + β1。同样地可以得到C 类的解释。对于参照类别A，−(β1 + β2)表⽰和总体均值的差异，则β0 −(β1 + β2) 表⽰该类别的总体效应。特征矩阵：\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1−1−11−1−1110110101101\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8Dummy coding每个类别的β 是这个类别y 的估计均值(假定所有其他特征值为零或参照类别)。请注意这⾥省略了截距，这样就可以找到线性模型权重的唯⼀解。特征矩阵：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 44, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型38\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed100100010010001001\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8如果想深⼊了解分类特征的不同编码，可以参考⽹页和博客。4.1.6线性模型是否有很好的解释？从构成“好的” 解释需要的性质(见第⼆章⼈性化的解释) 来看，线性模型并不能创建最佳解释。它们是对⽐性的，但是参照实例是构造的⼀个数据点，其中所有数值特征都为零，分类特征设置为它们的参照类别，这通常是⼀个⼈⼯的、毫⽆意义的实例，不太可能出现在你的真实数据或现实中。有⼀种例外：如果所有数值特征均以均值为中⼼(特征减去特征的平均值)，并且所有的分类特征都采⽤Effect Coding 的，那么参照实例就是所有特征取平均特征值的数据点。这也可能是⼀个不存在的数据点，但⾄少更有可能或更有意义。在这种情况下，权重乘以特征值(特征效应) 解释了与“均值实例” 相⽐，对预测结果的贡献。⼀个“好的解释” 的另⼀个要求是选择性，这可以在线性模型中通过使⽤较少的特征或训练稀疏的线性模型来实现。但默认情况下，只要线性⽅程是建模特征与结果间关系的合适的模型，线性模型就不会创建选择性解释，⽽是创建真实的解释。⾮线性和交互作⽤越多，线性模型越不准确，解释越不真实。线性使得解释更为普遍性和简单。我认为，模型的线性性质是⼈们使⽤线性模型解释关系的主要因素。4.1.7稀疏线性模型前⾯我们选择的线性模型的例⼦看起来都很好，不是吗？但实际上，在真实的情况下你可能不只是拥有少数个特征，⽽是拥有成百上千个特征。这种情况下你的线性回归模型的解释能⼒就会下降。你甚⾄可能会发现处于这样⼀种情况，即特征⽐实例多，并且根本⽆法拟合标准线性模型。幸运的是，有很多⽅法可以将稀疏性(即很少的特征) 引⼊线性模型。LassoLasso 是⼀种将稀疏性引⼊线性回归模型的⾃动简便⽅法。Lasso 代表“最⼩绝对收缩和选择算⼦”，当应⽤于线性回归模型时，执⾏特征选择和所选特征权重的正则化。让我们考虑权重优化的最⼩化问题(注：这⾥的xi 表⽰的是第i 个实例)：minβ(1nn∑i=1(y(i) −xTi β)2)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 45, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型39Lasso 为这个优化问题添加了⼀项：minβ(1nn∑i=1(y(i) −xTi β)2 + λ||β||1)其中，||β||1 为特征向量的L1 范数，会对⼤权重进⾏惩罚。由于使⽤了L1 范数，许多权重的估计值为0，⽽其他权重的估计值则缩⼩。参数λ 控制正则化的强度，通常通过交叉验证进⾏调整。尤其是当λ 很⼤时，许多权重变为0。特征权重可以可视化为惩罚项λ 的函数，下图中每个特征权重⽤⼀条曲线表⽰。图4.4.随着权重惩罚的增加，越来越少的特征接受⾮零权重估计。这些曲线也称为正则化路径。图上的数字是⾮零权重的数⽬。我们应该为λ 选择什么值？如果将惩罚项视为⼀个优化参数，那么可以通过交叉验证找到将模型错误最⼩化的λ。还可以将λ 视为控制模型可解释性的参数，惩罚越⼤，模型中出现的特征就越少(因为它们的权重为零)，并且模型的解释效果越好。Lasso 示例我们将⽤Lasso 预测⾃⾏车的租赁。我们预先设置了模型中需要的特征数量，让我们先设置为2 个特征：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 46, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型40WeightseasonSPRING0.00seasonSUMMER0.00seasonFALL0.00seasonWINTER0.00holidayHOLIDAY0.00workingdayWORKING DAY0.00weathersitMISTY0.00weathersitRAIN/SNOW/STORM0.00temp52.33hum0.00windspeed0.00days_since_20112.15前两个在Lasso 路径中具有⾮零权重的特征是温度特征(temp) 和天数特征(days_since_2011)。现在，让我们选择5 个特征：WeightseasonSPRING-389.99seasonSUMMER0.00seasonFALL0.00seasonWINTER0.00holidayHOLIDAY0.00workingdayWORKING DAY0.00weathersitMISTY0.00weathersitRAIN/SNOW/STORM-862.27temp85.58hum-3.04windspeed0.00days_since_20113.82需要注意，温度和天数这两个特征的权重和前⾯那个模型的结果不⼀样。这是因为减少了λ，权重就会受到较少的惩罚，权重的绝对值就可能更⼤。Lasso 权重的解释与线性回归模型中权重的解释相对应。你只需要注意特征是否标准化，因为这会影响权重。在这个⽰例中，这些特征已经被软件', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 47, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型41标准化了，但权重会⾃动转换以匹配原始特征⽐例。线性模型中稀疏性的其他方法可以使⽤多种⽅法来减少线性模型中的特征数量。预处理⽅法：• ⼿动选择特征：你可以始终使⽤专家知识来选择或放弃某些特征。最⼤的缺点是它不能被⾃动化，需要与理解数据的⼈员取得联系。• 单变量选择：相关系数就是⼀个例⼦。你只考虑超过特征和⽬标之间相关性阈值的特征。缺点是它只是单独地考虑单个特征。在线性模型考虑了其他⼀些特征之前，某些特征可能不会显⽰相关性。⽽使⽤单变量选择⽅法你会错过这些特征。分步⽅法：• 向前选择：⽤⼀个特征拟合线性模型，对每个特征都执⾏此操作。选择最有效的模型(例如，最⾼R-平⽅)；然后，对于其余的特征，通过将每个特征添加到当前的最佳模型中来拟合模型的不同版本，选择表现最好的⼀个；重复操作，直到达到某个条件，例如最⼤特征数。• 向后选择：类似于向前选择。但是，不是添加特征，⽽是从包含所有特征的模型开始，尝试删除某个特征以达到性能最⼤程度的提⾼；重复此操作，直到达到某个停⽌标准。这⾥，我们建议使⽤Lasso，因为它可以⾃动化，同时考虑所有特征，并且可以通过λ 进⾏控制。它同样也适⽤于分类(逻辑回归)。4.1.8优点将预测建模为⼀个加权和，使预测的⽣成变得透明。使⽤Lasso，我们可以确保使⽤的特征数量仍然很⼩。许多⼈使⽤线性回归模型。这意味着在许多地⽅，预测建模和推理都被接受。有很高水平的集体经验和专业知识，包括线性回归模型和软件实现的教材。线性回归可以在R，Python，Java，Julia，Scala，Javascript 等等中找到。从数学上讲，估计权重很简单，⽽且可以保证找到最佳权重(假设数据满⾜线性回归模型的所有假设)。与权重⼀起，你还可以得到置信区间，检验和可靠的统计理论。线性回归模型也有许多扩展(请参阅有关GLM，GAM 等的章节内容)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 48, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型424.1.9缺点线性回归模型只能表⽰线性关系，即输⼊特征的加权和。⽽每一个非线性或交互都必须是人工构成的，并明确地作为输⼊特征提供给模型。从预测性能角度来说，线性模型通常也不是那么好，因为可以学习的关系很有限，⽽且通常过于简单化了复杂的现实。权重的解释可能不直观，因为它取决于所有其他特征。在线性模型中，与结果y 和另⼀个特征都⾼度正相关的特征可能会得到负权重，这是因为在另⼀个相关特征下，它与⾼维空间中的y 呈负相关。完全相关的特征使得我们甚⾄不可能找到线性⽅程的唯⼀解。例如，你⽤⼀个模型来预测房屋的价格，并且有⼀些特征，⽐如房间的数量和房屋的⾯积⼤⼩。房屋的⾯积⼤⼩和房间的数量是⾼度相关的：房屋⾯积越⼤，拥有的房间就越多。如果将这两个特征都纳⼊线性模型中，可能会发⽣这样的情况：房屋的⾯积⼤⼩是更好的预测因素，并且会得到很⼤的正权重。房间的数量最终可能会得到⼀个负的权重，因为考虑到房屋的⾯积⼤⼩相同，增加房间的数量可能会使它的价格降低，或者当相关性太强时，线性⽅程变得不稳定。4.2逻辑回归逻辑回归(Logistic Regression) 建模有两个可能结果的概率分类问题，它是针对分类问题的线性回归模型的扩展。4.2.1线性回归用于分类有什么问题？线性回归模型能很好地建模回归，但建模分类就失败。为什么呢？对于两个类，可以将其中⼀个类标记为0，另⼀个标记为1，然后使⽤线性回归。从技术上讲，它是可⾏的，⼤多数线性模型程序都会得到权重。但这种⽅法存在⼀些问题：线性模型不输出概率，但它将类视为数字(0 和1)，并拟合最佳超平⾯(对于单个特征，它是⼀条线) 以最⼩化点和超平⾯之间的距离。所以它只是在点之间插值，⽽你不能把它解释为概率。⼀个线性模型也会给出低于0 和⾼于1 的值，这就表明我们应该找到⼀个更好的分类⽅法。由于预测的结果不是概率，⽽是点之间的线性插值，因此没有⼀个有意义的阈值可以⽤来区分⼀个类和另⼀个类。这⼀问题可以参考Stackoverflow。线性模型不能扩展到具有多个类的分类问题，你必须⽤2、3 等等标记下⼀个类。类的顺序可能没有任何意义，但是线性模型会在特征和类别预测之间的关系上强制⼀个约束。具有正权重的特征值越⾼，它对具有更⾼编号的类的预测所起的作⽤就越⼤，即便相似编号的类并不⽐其他类更近。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 49, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型43图4.5.线性模型根据肿瘤的⼤⼩将其分类为恶性1 或良性0，这些线条显⽰了线性模型的预测。对于左边的数据，我们可以使⽤0.5 作为分类阈值；在引⼊更多的恶性肿瘤病例后，对于右边的数据，0.5 的阈值不再区分这两类。图中的点略有抖动，以减少过度绘图。4.2.2理论分类的解决⽅案是逻辑回归。逻辑回归不是拟合直线或超平⾯，⽽是使⽤逻辑函数将线性⽅程的输出挤压到0 和1 之间。逻辑函数定义如下：logistic(η) =11 + exp(−η)它看起来像这样：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 50, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型44图4.6. 逻辑函数。它输出0 到1 之间的数字。在输⼊0 时，它输出0.5。从线性回归到逻辑回归的步骤较为简单。在线性回归模型中，我们⽤⼀个线性⽅程来建模结果和特征之间的关系：ˆy(i) = β0 + β1x(i)1 + . . . + βpx(i)p对于分类，我们倾向于0 到1 之间的概率，因此我们将⽅程的右边包装成逻辑函数，这将强制输出仅为0 和1 之间的值。P(y(i) = 1) =11 + exp(−(β0 + β1x(i)1 + . . . + βpx(i)p ))让我们再次回顾⼀下肿瘤⼤⼩的例⼦。但我们使⽤的不是线性回归模型，⽽是逻辑回归模型。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 51, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型45图4.7.逻辑回归模型根据肿瘤的⼤⼩在恶性和良性之间找到正确的决策边界。曲线为拟合数据对逻辑函数进⾏了移位和压缩。使⽤逻辑回归进⾏分类效果更好，在两种情况下我们都可以使⽤0.5 作为阈值。附加的点并不会真正影响估计曲线。4.2.3解释由于逻辑回归的结果是0 到1 之间的概率，逻辑回归中权重的解释不同于线性回归中权重的解释。权重不再线性地影响概率，加权和由逻辑函数转换为概率。因此，我们需要为解释重新构造⽅程，以便只有线性项在公式的右边。log(P(y = 1)1 −P(y = 1))= log(P(y = 1)P(y = 0))= β0 + β1x1 + . . . + βpxp我们将log() 函数中的项称为⼏率(odds) (事件发⽣概率除以事件不发⽣概率)，并⽤对数表⽰，称为对数⼏率(log odds)。该公式表明，逻辑回归模型是对数⼏率的线性模型。现在，只要稍微改变⼀下这些项，我们就可以知道当特征xj 改变1 个单位时，预测是如何变化的。为此，我们⾸先可以将exp() 函数作⽤于公', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 52, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型46式的两边：P(y = 1)1 −P(y = 1) = odds = exp (β0 + β1x1 + . . . + βpxp)然后我们⽐较当我们将某个特征值增加1 时会发⽣什么。但是，我们不看差异，⽽是看两个预测的⽐率：oddsxj+1odds= exp (β0 + β1x1 + . . . + βj(xj + 1) + . . . + βpxp)exp (β0 + β1x1 + . . . + βjxj + . . . + βpxp)我们应⽤以下规则：exp(a)exp(b) = exp(a −b)然后删除⼀些项：oddsxj+1odds= exp (βj(xj + 1) −βjxj) = exp (βj)最后，我们得到了⼀个简单的特征权重exp()。⼀个特征改变1 个单位将会使⼏率⽐(odds ratio)改变exp(βj)。我们也可以这样解释：特征xj 改变1 个单位会增加对数⼏率⽐(log odds ratio)相应权重的值。⼤多数⼈解释⼏率⽐是因为⼈们觉得思考log() 对⼤脑来说很困难，⽽且光是解释⼏率⽐已经需要⼀些习惯。例如，如果⼏率(odds) 为2，则表⽰y=1 的概率是y=0 的两倍。如果你有⼀个权重为0.7，则将相应的特征增加1 个单位，⼏率将乘以exp(0.7) (约2)，也就是⼏率将变为4。但通常你不⽤处理⼏率，只要把权重解释为⼏率⽐。因为为了实际计算⼏率，你需要为每个特征设置⼀个值，这只有在想查看数据集的⼀个特定实例时才有意义。这些是具有不同特征类型的逻辑回归模型的解释：• 数值特征：如果你将特征xj 增加⼀个单位，则估计的⼏率将乘以因⼦exp(βj)。• ⼆分类特征：只取两种可能值的特征，其⼀是参照类别(⽤0 编码)。将特征xj 从参照类别更改为其他类别，则估计的⼏率将乘以因⼦exp(βj)。• 具有多个类别的分类特征：具有固定数量的可能值的特征。处理多个类别的解决⽅案是one-hot 编码，这意味着每个类别都有⾃⼰的列。对于具有L 个类别的分类特征，只需要L −1 列，因为第L 列将具有冗余信息(例如，当列1 到L −1 的值都为0 时，我们知道此实例的分类特征为第L 个类别)。然后对每个类别的解释与对⼆分类特征的解释相同。• 截距项β0：所有数字特征为零和分类特征为参照类别，则估计的⼏率是exp(β0)。截距的解释通常不相关。4.2.4示例我们使⽤逻辑回归模型基于⼀些风险因素来预测宫颈癌。下表显⽰了估计权重、相关的⼏率⽐和估计的标准差。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 53, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型47WeightOdds ratioStd. ErrorIntercept-2.910.050.32Hormonal contraceptives y/n-0.120.890.30Smokes y/n0.261.290.37Num. of pregnancies0.041.040.10Num. of diagnosed STDs0.822.260.33Intrauterine device y/n0.621.850.40数字特征的解释(性病诊断次数/Num. of diagnosed STDs)：当所有其他特征保持不变时，诊断的性传播疾病(STDs) 数量的增加改变(增加) 了癌症对⾮癌症的⼏率2.26 倍。要注意，相关性并不意味着因果关系。分类特征的解释(是否服⽤激素避孕药/Hormonal contraceptives y/n)：对于使⽤激素避孕药的⼥性，与没有激素避孕药的⼥性相⽐，癌症与没有癌症的⼏率低0.89 倍。当然，就像线性模型⼀样，我们在解释的时候总是伴随着“所有其他特征保持不变” 这⼀条件。4.2.5优缺点线性模型的很多优点和缺点也适⽤于逻辑回归模型。逻辑回归已被许多不同的⼈⼴泛使⽤，但它难以克服其限制性的表达能⼒(例如，必须⼿动添加交互)，其他模型可能具有更好的预测性能。逻辑回归模型的另⼀个缺点是解释更困难，因为权重的解释是乘法⽽不是加法。逻辑回归可能受到完全分离(Complete Separation) 的影响。如果有⼀个特征能够将两个类完全分开，那么逻辑回归模型就不能再被训练了。这是因为该特征的权重不会收敛，因为最佳权重将是⽆限的。这真的有点不幸，因为这样的特征真的很有⽤。但是如果你有⼀个简单的规则将两个类分开，其实就不需要机器学习了。完全分离问题可以通过引⼊权重的惩罚或定义权重的先验概率分布来解决。从好的⽅⾯来说，逻辑回归模型不仅是⼀个分类模型，⽽且还给出概率。与只能提供最终分类的模型相⽐，这是⼀个很⼤的优势。知道⼀个实例属于某个类有99% 的概率，和有51% 的概率是很⼤不同的。逻辑回归也可以从⼆分类扩展到多分类，后者被称为多分类回归(Multinomial Regression)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 54, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型484.2.6软件我在所有⽰例中都使⽤了R 中的glm 函数。你可以在任何可⽤于执⾏数据分析的编程语⾔中找到逻辑回归，例如Python，Java，Stata，Matlab 等等。4.3GLM, GAM 和其他模型线性回归模型最⼤的优点也是最⼤的缺点就是预测被建模为特征的加权和。此外，线性模型还有许多其他假设。坏的情况就是所有这些假设在现实中经常被违背：给定特征的结果可能具有⾮⾼斯分布，特征可能交互，特征和结果之间的关系可能是⾮线性的。好消息是，统计界已经进⾏了各种修改，将线性回归模型从简单的⼑⽚转换为瑞⼠⼑。本节不是扩展线性模型的明确指南，⽽是作为扩展的概述，例如⼴义线性模型(Generalized LinearModels，GLMs) 和⼴义加性模型(Generalized Additive Models，GAMs)。阅读之后，你可以对如何扩展线性模型有个全⾯的了解。如果你想⾸先了解有关线性回归模型的更多信息，建议你阅读有关线性回归模型的⼩节(如果还没有的话)。我们回顾下线性回归模型的公式：y = β0 + β1x1 + . . . + βpxp + ϵ线性回归模型假设⼀个实例的结果y 可以⽤它的p 个特征的加权和表⽰，其中误差项ϵ 遵循⾼斯分布。通过将数据强制到公式中，我们可以获得模型解释。特征的效应是加性的，意味着没有特征交互；⽽且关系是线性的，意味着某个特征增加1 个单位可以直接转化为预测结果的增加/减少。线性模型允许我们将特征和预期结果之间的关系压缩成⼀个单⼀数字，即估计的权重。但对于许多现实世界的预测问题来说，简单的加权和太过严格。在本节中，我们将学习经典线性回归模型的三个问题以及如何解决它们。可能违反的假设问题还有很多，但我们将重点关注下图所⽰的三个问题：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 55, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型49图4.8. 线性模型的三个假设(左侧)：给定特征的输出结果为⾼斯分布，可加性(相当于⽆交互) 和线性关系。现实通常不遵循这些假设(右侧) ：结果可能具有⾮⾼斯分布，特征可能会交互并且关系可能是⾮线性的。所有这些问题都有解决⽅案：1. 问题：给定特征的⽬标结果y 不遵循⾼斯分布。示例：假设我想预测我在某天内骑⾃⾏车的时间。给定的特征有这⼀天的类型，天⽓情况等等。如果我使⽤线性模型，它可以预测负的分钟，因为它假设⾼斯分布，不会在0 分钟停⽌。同样如果我想⽤线性模型预测概率，我可以得到负的或⼤于1 的值。解决方案：⼴义线性模型(Generalized Linear Models，GLMs)。2. 问题：特征交互。示例：⼀般来说，⼩⾬对我骑车的欲望有轻微的负⾯影响。但是在夏天，在交通⾼峰期，我还是希望下⾬，因为那时所有晴天时的⾃⾏车骑⼿都呆在家⾥，我就可以有⾃⼰的⾃⾏车道！这是⼀种时间和天⽓之间的交互，⽆法由纯加性模型获得。解决方案：⼿动添加交互。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 56, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型503. 问题：特征和y 之间的真实关系不是线性的。示例：在0 到25 摄⽒度之间，温度对我骑车欲望的影响可能是线性的，这意味着从0 到1 摄⽒度的增加导致骑车欲望的增加与从20 到21 摄⽒度的增加相同。但在较⾼的温度下，我骑车的动机会减弱甚⾄降低——我不喜欢在太热的时候骑车。解决方案：⼴义加性模型(Generalized Additive Models，GAMs)；特征转换下⾯会着重介绍这三个问题的解决⽅案。线性模型的许多进⼀步扩展这边被省略了。如果我试图覆盖这⾥的所有内容，那么本书很快就会变成关于这个主题的⼀本书，⽽该主题已经在许多其他书籍中涉及。但既然你已经看到这⾥了，所以在线性模型扩展⽅⾯会遇到的⼀些问题以及解决⽅案概述可以在本节末中进⼀步描述。4.3.1非高斯结果输出- GLM线性回归模型假定给定输⼊特征的结果遵循⾼斯分布。这⼀假设排除了许多情况：结果也可以是⼀个类别(癌症对健康)、⼀个计数(⼉童数量)、事件发⽣的时间(机器故障的时间) 或只有少数⾮常⾼的值的扭曲的结果(家庭收⼊)。线性回归模型可以扩展为对所有这些类型的结果建模，这个扩展称为广义线性模型，简称GLM。在本节中，我们将对通⽤框架和该框架中的特定模型使⽤GLM这个名称。任何GLM 的核⼼概念都是：保留特征的加权和，但允许⾮⾼斯结果分布，并通过可能的⾮线性函数连接该分布的期望均值与加权和。例如，逻辑回归模型假设结果为伯努利分布，并使⽤Logit 函数连接期望均值与加权和。GLM 数学上使⽤连接函数g 将特征的加权和与假定分布的均值连接起来，其中连接函数可以根据结果的类型可以⾃由选择：g(EY (y|x)) = β0 + β1x1 + . . . βpxpGLM 由三个部分组成：连接函数g、加权和XT β (有时称为线性预测因⼦) 以及定义EY 指数族的概率分布。指数族是⼀组可以⽤相同的(参数化的) 公式构成的分布，其中包括指数、分布的均值和⽅差以及⼀些其他参数。这边我们不会深⼊研究数学细节，因为这是⼀个⾮常⼤的探索领域。维基百科有⼀个指数族分布列表，这个列表中的分布都可以作为GLM 的选择。根据你要预测的结果类型，可以选择合适的分布。结果是对某事的计数吗？(例如⼀个家庭中孩⼦的数量) 那么，泊松分布可能是⼀个不错的选择。结果是否总是正的？(例如两次事件之间的时间) 那么，指数分布可能是⼀个不错的选择。让我们把经典线性模型看作是GLM 的⼀个特例。经典线性模型中⾼斯分布的连接函数是⼀个简单的恒等函数。⾼斯分布由均值和⽅差参数进⾏参数化。均值描述了我们的期望均值，⽽⽅差则描述了在均值上下值的变化程度。在线性模型中，连接函数会连接特征加权和和⾼斯分布的均值。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 57, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型51在GLM 框架下，这个概念推⼴到任何分布(指数族) 和任意连接函数。如果y 是某物的计数，例如某⼈某天喝的咖啡数量，我们可以⽤⼀个带有泊松分布和⾃然对数作为连接函数的GLM 来建模：ln(EY (y|x)) = xT β逻辑回归模型也是⼀个假定伯努利分布并使⽤Logit 函数作为连接函数的GLM。逻辑回归中使⽤的⼆项分布的均值是y 为1 的概率。xT β = ln(EY (y|x)1 −EY (y|x))= ln(P(y = 1|x)1 −P(y = 1|x))如果我们把这个⽅程解P(y = 1) 放在⼀边，我们得到逻辑回归公式：P(y = 1) =11 + exp(−xT β)指数族的每⼀个分布都有⼀个标准的连接函数，可以从分布中数学地推导出来。GLM 框架使选择独⽴于分布的连接函数成为可能。如何选择正确的连接函数？没有完美的配⽅。你要考虑到关于⽬标分布的知识，同时也要考虑到理论上的考虑，以及模型与实际数据的匹配程度。对于某些分布，标准的连接函数可能导致对该分布⽆效的值。因为你可以选择任何连接函数，所以简单的解决⽅案是选择另⼀个尊重分布域的函数。示例我们模拟了⼀个关于咖啡饮⽤⾏为的数据集，⽤于强调对GLM 的需求。假设你已经收集了关于每天喝咖啡⾏为的数据(如果你不喜欢咖啡，就假装是茶)。除了杯⼦的数量外，你还记录了⽬前的压⼒⽔平(从1 到10)，前⼀天晚上的睡眠状况(从1 到10)，以及当天是否必须⼯作。⽬的是根据压⼒、睡眠和⼯作来预测咖啡的数量。我们模拟了200 天的数据，在1 到10 之间均匀地绘制压⼒和睡眠，并且以0.5/0.5 的概率绘制“是/否” ⼯作(真是命啊！)。然后，每天从泊松分布中提取咖啡的数量，将强度λ (也是泊松分布的期望值) 建模为睡眠、压⼒和⼯作特征的函数。估计你可以猜到这个故事的结局：“嘿，让我们⽤⼀个线性模型来建模这个数据……哦，它不起作⽤……让我们⽤泊松分布的GLM……惊喜！现在可以了！“。让我们看看⽬标变量的分布，即给定⽇期的咖啡数量：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 58, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型52图4.9. 200 天的每⽇咖啡数量的模拟分布。在200 天中的76 天根本没有喝咖啡，在最极端的⼀天喝了7 杯。让我们先使⽤⼀个线性模型来预测咖啡的数量，使⽤睡眠⽔平、压⼒⽔平和是/否⼯作作为特征。当我们错误地假设⾼斯分布时，会发⽣什么错误？错误的假设可能会使估计⽆效，尤其是权重的置信区间。更加明显的问题是，预测与真实结果的“允许” 范围不匹配，如下图所⽰。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 59, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型53图4.10. 预测咖啡的数量取决于压⼒，睡眠和⼯作。线性模型预测负值。线性模型没有意义，因为它预测咖啡的数量为负。这个问题可以⽤⼴义线性模型GLM 来解决。我们可以改变连接函数和假定的分布。⼀种可能是保持⾼斯分布，并使⽤⼀个总是导致正预测的连接函数，例如对数连接(逆函数是exp 函数) ⽽不是恒等函数。更好的⽅案是：我们选择⼀个与数据⽣成过程相对应的分布和⼀个适当的连接函数。由于结果是⼀个计数，泊松分布是⼀个⾃然选择，同时对数作为连接函数。在这种情况下，数据甚⾄是由泊松分布⽣成的，因此泊松GLM 是最佳选择。拟合的泊松GLM 导致预测值的以下分布：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 60, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型54图4.11. 预测的咖啡数量取决于压⼒，睡眠和⼯作。具有泊松假设和对数连接的GLM 是适⽤于此数据集的模型。没有负数量的咖啡，现在看起来好多了。对GLM 权重的解释假设分布和连接函数共同决定了如何解释估计的特征权重。在咖啡计数的⽰例中，我们使⽤了⼀个带有泊松分布和对数连接的GLM，这意味着特征和期望结果之间存在以下关系。ln(E(coffees|stress, sleep, work)) = β0 + βstressxstress + βsleepxsleep + βworkxwork为了解释权重，我们对连接函数取逆，这样我们就可以解释特征对期望结果的影响，⽽不是对期望结果的对数的影响。E(coffees|stress, sleep, work) = exp(β0 + βstressxstress + βsleepxsleep + βworkxwork)由于所有权重都在指数函数中，因此特征影响的解释不是加性的，⽽是乘性的，因为exp(a + b) =exp(a) exp(b)。解释的最后⼀个成分是虚构样本的实际权重。下表列出了估计的权重和exp(权重)以及95% 的置信区间：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 61, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型55weightexp(weight) [2.5%, 97.5%](Intercept)-0.160.85 [0.54, 1.32]stress0.121.12 [1.07, 1.18]sleep-0.150.86 [0.82, 0.90]workYES0.802.23 [1.72, 2.93]压⼒⽔平提⾼⼀点，咖啡的预期数量乘以系数1.12。将睡眠质量提⾼⼀点，咖啡的预期数量乘以系数0.86。⼯作⽇的预测咖啡数量平均是休息⽇咖啡数量的2.23 倍。总之，压⼒越⼤，睡眠越少，⼯作越多，咖啡消耗越多。在本⼩节中，你了解了⼀些关于当⽬标不遵循⾼斯分布时有⽤的⼴义线性模型的知识。接下来，我们将研究如何将两个特征之间的交互集成到线性回归模型中。4.3.2交互线性回归模型假设⼀个特征的效应是保持相同的，与其他特征的值⽆关(= 没有交互)。但数据中经常有交互作⽤，为了预测⾃⾏车租赁数量，“温度” 和“是否⼯作⽇” 之间可能存在交互作⽤。也许，当⼈们不得不⼯作时，温度对租⽤⾃⾏车数量的影响不⼤，因为不管发⽣什么，⼈们都会骑着租⽤的⾃⾏车去⼯作。在休息⽇，许多⼈为了享乐⽽骑车，但前提天⽓⾜够暖和。当涉及到⾃⾏车租赁时，我们可能希望温度和⼯作⽇之间的交互作⽤。我们如何才能得到包含交互作⽤的线性模型？在拟合线性模型之前，要在特征矩阵中添加⼀列，表⽰特征之间的交互，并像往常⼀样拟合模型。该解决⽅案在某种程度上很优雅，因为它不需要对线性模型进⾏任何更改，只需要数据中的附加的列。在⼯作⽇和温度⽰例中，我们将添加⼀个新特征，该特征在⾮⼯作⽇为零，否则它具有温度特征的值(假定⼯作⽇为参照类别)。假设我们的数据如下：worktempY25N12N30Y5线性模型使⽤的数据矩阵看起来略有不同。下表显⽰了如果我们不指定任何交互，为模型准备的数据是什么样⼦的。通常，这种转换是由软件⾃动执⾏的。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 62, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型56InterceptworkYtemp112510121030115第⼀列是截距项，第⼆列对分类特征进⾏编码(参照类别为0，其他类别为1)，第三列为温度特征。如果我们希望线性模型考虑温度和⼯作⽇特征之间的交互作⽤，我们必须为交互作⽤添加⼀列：InterceptworkYtempworkY.temp11252510120103001155新的列“workY.temp” 捕获了特征⼯作⽇和温度之间的交互。对于⼀个实例，如果⼯作特征处于参照类别(“N” 表⽰⾮⼯作⽇)，则此新特征列为0，否则为实例的温度特征的值。使⽤这种编码⽅式，线性模型可以学习两种⼯作⽇下的温度的不同线性效应。这是两个特征之间的交互作⽤。在没有交互项的情况下，分类特征和数值特征的组合效应可以描述成⼀条针对不同类别垂直移动的线。如果我们将交互包括在内，我们就允许数值特征(斜率) 的影响在每个类别中具有不同的值。两个分类特征的交互作⽤也有类似的效果。我们创建了表⽰类别组合的附加特征。以下是⼀些包含⼯作⽇(work) 和分类天⽓特征(wthr) 的⼈⼯数据：workwthrY2N0N1Y2接下来，我们将包括交互项：InterceptworkYwthr1wthr2workY.wthr1workY.wthr2110101', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 63, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型57InterceptworkYwthr1wthr2workY.wthr1workY.wthr2100000101000110101第⼀列⽤于估计截距；第⼆列是编码了⼯作⽇特征；第三列和第四列表⽰天⽓特征，这需要两列，因为需要两个权重来捕获三个类别的效果(其中⼀个类别是参照类别)；剩余的两列捕获交互。对于这两个特征的每个类别(参照类别除外)，如果两个特征都有⼀个特定的类别，我们将创建⼀个新的特征列(即1)，否则为0。对于两个数值特征，交互列更容易构造：我们简单地将两个数值特征相乘。有⼀些⽅法可以⾃动检测和添加交互项，其中之⼀可以参考RuleFit。RuleFit 算法⾸先挖掘交互项，然后估计包含交互的线性回归模型。示例让我们回到在线性模型⼀节中已经建模的⾃⾏车租赁预测任务。这⼀次，我们还考虑了温度和⼯作⽇特征之间的交互作⽤，这导致以下估计的权重和置信区间。WeightStd. Error2.5%97.5%(Intercept)2185.8250.21694.62677.1seasonSUMMER893.8121.8654.71132.9seasonFALL137.1161.0-179.0453.2seasonWINTER426.5110.3209.9643.2holidayHOLIDAY-674.4202.5-1071.9-276.9workingdayWORKING DAY451.9141.7173.7730.1weathersitMISTY-382.187.2-553.3-211.0weathersitRAIN/…-1898.2222.7-2335.4-1461.0temp125.48.9108.0142.9hum-17.53.2-23.7-11.3windspeed-42.16.9-55.5-28.6days_since_20114.90.24.65.3workingdayWORKING DAY:temp-21.88.1-37.7-5.9额外的交互效应是负的(-21.8)，与零有显著差异，如95% 置信区间所⽰，其中不包括零。顺便说⼀句，数据不是独⽴同分布的，因为彼此接近的⽇⼦并不是彼此独⽴的。就拿它来说，置信区间可能会产⽣', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 64, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型58误导。交互项改变了所涉及特征的权重解释。如果是⼯作⽇，温度会有负效应吗？答案是否定的，即使表格建议未经培训的⽤户使⽤也可以。我们不能孤⽴地解释“workingdayWORKING DAY:tem”的交互权重，因为这样的解释是：“在保持所有其他特征值不变的情况下，增加⼯作⽇温度的交互作⽤会减少⾃⾏车的预测数量。” 但是，交互项的效应只是添加到温度的主要效应中。假设这是⼀个⼯作⽇，我们想知道如果今天的温度升⾼1 度会发⽣什么。那么我们需要将“temp” 和“workingdayWORKING DAY:tem” 的权重相加，以确定估计值增加了多少。从视觉上更容易理解交互。通过引⼊分类特征和数值特征之间的交互项，我们得到了温度的两个斜率，⽽不是⼀个。⾮⼯作⽇(“NO WORKING DAY”) 的温度斜率可直接从表中读取(125.4)。⼯作⽇(“WORKING DAY”) 的温度斜率是两个温度权重的总和(125.4-21.8=103.6)。在温度等于0 时，“NO WORKING DAY” 线的截距由线性模型(2185.8) 的截距项确定。在温度等于0 时，“WORKING DAY” 线的截距由截距项+ ⼯作⽇的效应(2185.8+451.9=2637.7) 确定。图4.12.温度和⼯作⽇对线性模型预测的⾃⾏车数量的影响(包括交互作⽤)。实际上，我们得到了两个温度斜率，每个⼯作⽇特征对应⼀个类别。4.3.3非线性效应- GAM世界不是线性的。线性模型中的线性意味着，⽆论⼀个实例在某个特定特征中具有什么值，将该值增加1 个单位对预测结果总是有相同的效应。温度在10 摄⽒度时升⾼1 度，对租⽤⾃⾏车的数量', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 65, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型59产⽣的效应是否与温度在40 摄⽒度时升⾼相同？直观地说，⼈们期望把温度从10 摄⽒度提⾼到11 摄⽒度会对⾃⾏车租赁产⽣正效应，从40 摄⽒度提⾼到41 摄⽒度会产⽣负效应，正如整本书中许多⽰例中所看到的那样。温度特征对租⽤⾃⾏车的数量有线性的、正的效应，但在某些时候它会变平，甚⾄在⾼温下也会产⽣负效应。线性模型并不关⼼，它会尽⼒地找到最佳的线性平⾯(通过最⼩化欧⼏⾥得距离)。你可以使⽤以下⽅法对⾮线性关系建模：• 特征的简单转换(例如对数)• 特征分类• ⼴义加性模型(GAM)在介绍每种⽅法的细节之前，让我们先从⼀个⽰例开始，说明这三种⽅法。我们使⽤并训练了⼀个仅具有温度特征的线性模型来预测⾃⾏车租赁的数量。下图显⽰了估计的斜率：标准线性模型、对数(转换) 温度的线性模型、将温度视为分类特征的线性模型以及使⽤回归样条(GAM) 的线性模型。图4.13.仅使⽤温度特征预测租⽤⾃⾏车的数量。线性模型(左上) ⽆法很好地拟合数据。⼀种解决⽅案(右上) 是对特征进⾏转换(例如使⽤对数)，对其进⾏分类(左下) (这通常是⼀个错误的决定)，或者使⽤可以⾃动拟合温度的平滑曲线的GAM (右下)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 66, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型60特征转换通常将特征的对数⽤作转换。使⽤对数表⽰，温度每升⾼10 倍，对⾃⾏车数量有相同的线性效应，因此从1 摄⽒度到10 摄⽒度的变化与从0.1 到1 的变化具有相同的效果(听起来错误)。特征变换的其他例⼦有平⽅根、平⽅函数和指数函数。使⽤特征转换意味着你⽤特征的函数(如对数) 替换数据中该特征的列，并照常拟合线性模型。⼀些统计程序还允许你在线性模型调⽤中指定转换。因⽽在转换特征时，可以发挥创造⼒。特征的解释会根据所选转换⽽变化。如果使⽤对数转换，线性模型中的解释将变为：“如果特征的对数增加1，则预测将增加相应的权重。” 当使⽤不是恒等函数作为连接函数的GLM 时，则解释将更复杂，因为你必须将这两种转换合并到解释中(除⾮它们相互抵消，如log 和exp，那么解释会容易)。特征分类实现⾮线性效应的另⼀种可能⽅法是离散化特征，将其转化为分类特征。例如，你可以将温度特征切割为20 个间隔，级别分别为[-10, -5)，[-5, 0)…等等。当你使⽤分类温度⽽不是连续温度时，线性模型将估计阶跃函数，因为每个级别都有⾃⼰的估计。这种⽅法的问题在于，它需要更多的数据，更容易过拟合，⽽且不清楚如何有意义地离散化特征(等距间隔或分位数？有多少个间隔？) 只有在有很强理由的情况下，我们才会使⽤离散化。例如，使模型与另⼀项研究相⽐较。广义加性模型为什么不“简单” 地允许(⼴义) 线性模型学习⾮线性关系呢？这就是GAM 背后的动机。GAM 放宽了这种限制，即关系必须是⼀个简单的加权和，⽽是假定可以由每个特征的任意函数的和建模结果。在数学上，GAM 中的关系如下：g(EY (y|x)) = β0 + f1(x1) + f2(x2) + . . . + fp(xp)该公式与GLM 公式类似，不同之处在于线性项βjxj 被更灵活的函数fj(xj) 取代。GAM 的核⼼仍然是特征效应的总和，但是你可以选择允许某些特征和输出之间存在⾮线性关系。线性效应也包含在框架中，因为对于要线性处理的特征，我们可以将它们的形式fj(xj) 限制为仅采⽤βjxj。最⼤的问题是如何学习⾮线性函数。答案称为样条函数(“splines” 或“spline functions”)。样条曲线是可以组合以近似任意函数的函数，有点像堆叠乐⾼积⽊来构建更复杂的东西。定义这些样条函数的⽅法多种多样。如果你有兴趣了解有关定义样条曲线的所有⽅法的更多信息，祝你旅途愉快。我们不想在这⾥讨论细节，只想建⽴⼀个直觉。对于理解样条曲线，最⼤的帮助⽅法是可视化各个样条函数，并研究是如何修改数据矩阵的。例如，为了⽤样条曲线对温度进⾏建模，我们从数据中删除了温度特征，并将其替换为4 列，每列表⽰⼀个样条函数。通常你会有更多的样条函数，我们减少数量只是为了说明⽬的。这些新样条特征的每个实例的值取决于实例的温度值。连同所有的线性效应，GAM 还将估计这些样条权重。GAM 还为权重引⼊了⼀个惩罚项，以使它们接近零。这有效地降低了样条的灵活性，并减少了过拟合。然后通过交叉验证来调整通常⽤于控制曲线灵活性的平滑度参数。忽略惩罚项，⽤样条做⾮线性建模是⼀个很棒的特征⼯程。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 67, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型61在⽰例中，我们仅使⽤温度的GAM 预测⾃⾏车租赁数量，模型特征矩阵如下所⽰：ts(temp).1s(temp).2s(temp).3s(temp).410.93-0.140.21-0.8310.83-0.270.27-0.7211.320.71-0.39-1.6311.320.70-0.38-1.6111.290.58-0.26-1.4711.320.68-0.36-1.59每⾏表⽰数据中的单个实例(⼀天)。每个样条列包含特定温度值下样条函数的值。下图显⽰了这些样条函数的外观：图4.14.为了平滑地模拟温度特征效应，我们使⽤了4 个样条函数。每个温度值都映射到(此处)4 个样条线值。如果实例的温度为30 摄⽒度，则第⼀个样条曲线特征的值为-1，第⼆个样条曲线的值为0.7，第三个样条曲线的值为-0.8，第四个样条曲线的值为1.7。GAM 为每个温度样条特征分配权重：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 68, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型62weight(Intercept)4504.35s(temp).1-989.34s(temp).2740.08s(temp).32309.84s(temp).4558.27⽽实际曲线是由⽤估计权重加权的样条函数之和得到的，如下所⽰：图4.15. 预测所租⾃⾏车的数量(温度是唯⼀特征)，温度的GAM 特征效应。平滑效果的解释需要对拟合曲线进⾏⽬视检查。样条曲线通常以均值预测为中⼼，因此曲线上的⼀个点与均值预测是不同的。例如，在0 摄⽒度时，预测的⾃⾏车数量⽐平均预测值低3,000。4.3.4优点线性模型的所有这些扩展本⾝就是⼀个很⼤的探索空间。⽆论线性模型遇到的问题是什么，你都可能会找到可解决该问题的扩展。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 69, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型63⼤多数⽅法已经使⽤了数⼗年。例如，GAM 已经有将近30 年的历史了。许多来⾃⼯业界的研究⼈员和实践者对线性模型⾮常有经验，并且这些方法在许多社区被认为是建模的现状。除了进⾏预测之外，你还可以使⽤模型进行推断，得出有关数据的结论——只要不违反模型假设即可。你还可以得到权重的置信区间、显著性检验、预测区间等等。⼀些统计软件通常有很好的接⼜来适应GLM、GAM 和更特殊的线性模型。许多机器学习模型的不透明性源于1). 缺乏稀疏性，这意味着使⽤了许多特征；2). 以⾮线性⽅式处理的特征，这意味着需要多个权重来描述特征效应；3). 特征之间交互作⽤的建模。线性模型具有很⾼的可解释性，但通常不适合实际，本节所述的扩展提供了⼀种很好的⽅式，可以在平稳地过渡到更灵活的模型的同时，保留⼀些可解释性。4.3.5缺点作为优势，我们说过线性模型是⼀个⾮常⼤的探索空间，扩展简单线性模型的方法太多了。事实上，很多领域的⼈都做了这些研究，问题是许多研究者和实践者在他们领域内都⽤他们⾃⼰的名字来命名那些可能做着相同事情的⽅法，这会⾮常令⼈困惑。线性模型的⼤多数修改都会使模型的解释性变差。不是恒等函数的任何连接函数(在GAM 中) 都会使解释复杂化；交互也会使解释复杂化；⾮线性特征效应要么不太直观(如对数转换)，要么不能再由单个数字(如样条函数) 来概括。GLM、GAM 等依赖于有关数据生成过程的假设。如果违反了这些假设，则对权重的解释将不再有效。在许多情况下，随机森林或梯度提升树等基于树的集成⽅法的性能⽐最复杂的线性模型要好。这部分是经验之谈，也部分是来⾃kaggle.com 等平台上的获奖模型的观察结果。4.3.6软件本节中的所有⽰例都是使⽤R 语⾔创建的。对于GAM，使⽤了gam 软件包，但还有许多其他软件包。R 有数量惊⼈的软件包来扩展线性回归模型。R 语⾔在线性回归模型的扩展⽅⾯，是其他分析语⾔所⽆法⽐拟的。你会在Python 中找到GAM 的实现(例如pyGAM)，但是这些实现还不那么成熟。4.3.7进一步扩展如前所述，这⾥列出了线性模型可能遇到的问题，以及问题的解决⽅案。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 70, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型641. 问题：数据违反了独⽴同分布(IID) 的假设。示例：例如，对同⼀患者重复测量。解决方案：混合模型(Mixed Models) 或⼴义估计⽅程(Generalized Estimating Equations)。2. 问题：模型有异⽅差性误差(Heteroscedastic Errors)。示例：例如，在预测房屋价格时，昂贵房屋的模型误差通常较⾼，这违背了线性模型的同⽅差性。解决方案：稳健回归(Robust Regression)。3. 问题：有异常点会强烈影响模型。解决方案：稳健回归。4. 问题：想预测事件发⽣前的时间。示例：到达事件的时间数据通常带有经过审查的测量值，这意味着在某些情况下没有⾜够的时间来观察事件。例如，⼀家公司想预测其制冰机的故障，但只有两年的数据。⼀些机器两年后仍然完好⽆损，但可能会在以后会故障。解决方案：参数⽣存模型(Parametric Survival Models)，Cox 回归(Cox Regression)，⽣存分析(Survival Analysis)。5. 问题：要预测的结果是⼀个类别。解决方案：如果结果有两个类别，则使⽤逻辑回归模型(Logistic Regression Model)，该模型对类别的概率进⾏建模。如果你有更多类别，可以考虑多分类回归(Multinomial Regression)。逻辑回归模型和多分类回归都是GLM。6. 问题：预测有序的类别。示例：例如，学校成绩。解决方案：⽐例优势模型(Proportional Odds Model)。7. 问题：结果是⼀个计数(就像⼀个家庭中孩⼦的数量)。解决方案：寻找泊松回归(Poisson Regression)。泊松模型也是⼀个GLM。8. 问题：在上⼀个问题中，0 的计数值⾮常频繁。解决方案：寻找零膨胀泊松回归(Zero-inflatedPoissonRegression)，栅栏模型(HurdleModel)。9. 问题：不确定模型中需要包含哪些特征才能得出正确的因果结论。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 71, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型65示例：例如，想知道药物对⾎压的影响。这种药物对某些⾎液有直接影响，⽽该⾎液会影响结果。应该将⾎液纳⼊回归模型吗？解决方案：寻找因果推理(Causal Inference)，中介分析(Mediation Analysis)。10. 问题：缺少数据。解决方案：多重填补(Multiple Imputation)。11. 问题：把先前的知识整合到模型中。解决方案：贝叶斯推断(Bayesian Inference)。4.4决策树线性回归和逻辑回归在特征与结果之间的关系为⾮线性或特征交互的情况下会失败。是时候“点亮” 决策树(Decision Tree) 了！基于树的模型根据特征中的某些截断值多次分割(Split，或称分裂、拆分) 数据。通过分割，可以创建数据集的不同⼦集，每个实例都属于⼀个⼦集。最后的⼦集称为终端(Terminal) 或叶节点(Leaf Nodes)，中间的⼦集称为内部节点(Internal Nodes) 或分裂节点(Split Nodes)。为了预测每个叶节点的结果，使⽤该节点中训练数据的平均结果。树模型可⽤于分类和回归。有多种算法可以⽣成⼀棵树，它们在树的可能结构(例如每个节点的分割数量)、如何分割的标准、何时停⽌分割以及如何估计叶节点内的简单模型等⽅⾯存在差异。CART 算法可能是最流⾏的树归纳算法。我们将专注于CART，但是对于⼤多数其他树类型，其解释是相似的。我们推荐《TheElements of Statistical Learning》⼀书来更详细地介绍CART。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 72, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型66图4.16. 具有⼈⼯数据的决策树。特征x1 的值⼤于3 的实例最终出现在节点5 中。所有其他实例都分配给节点3 或节点4，具体取决于特征x2 的值是否超过1。下⾯的公式描述了结果y 和特征x 之间的关系：ˆy = ˆf(x) =M∑m=1cmI{x ∈Rm}每个实例正好属于⼀个叶节点(= ⼦集Rm)。I{x∈Rm} 是指⽰函数，即如果x 在⼦集Rm 中则为1，否则为0。如果⼀个实例属于叶节点Rl，则预测结果为ˆy = cl，其中cl 是叶节点Rl 中所有训练实例的平均值。但是⼦集是从哪⾥来的呢？这很简单：CART 采⽤⼀个特征，并确定哪个截断值将回归任务的y 的⽅差最⼩化，或者分类任务的y 的类别分布的基尼指数最⼩化。⽅差告诉我们节点中的y 值围绕其平均值分散的程度。基尼指数告诉我们节点的“不纯” 程度，例如，如果所有的类别都有相同的频率，那么这个节点就是不纯的；如果只有⼀个类别存在，那么它就是最⼤的纯度。当节点中的数据点具有⾮常相似的y 值时，⽅差和基尼指数最⼩化。因此，最佳截断值是使两个结果⼦集在⽬标结果⽅⾯尽可能不同。对于分类特征，该算法通过尝试不同类别分组来创建⼦集。在确定每个特征的最佳截断值后，算法选择要进⾏分割的特征，这是考虑⽅差或基尼指数下的最佳分割，并将此分割添加到树中。该算法继续此搜索并在新节点中递归分割，直到达到停⽌条件。可能的条件是：分', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 73, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型67割前必须在节点中的最⼩实例数，或必须在终端节点中的最⼩实例数。(CART 的理解与代码实现可以参考这份⼯作的第五章机器学习部分。)4.4.1解释解释很简单：从根节点开始，转到下⼀个节点，⽽边表明要查看的⼦集。⼀旦到达叶节点，该节点将表明预测的结果。所有边通过“AND” 连接。模板：“如果特征x ⽐阈值c [⼩/⼤] AND …，那么预测结果就是节点y 中实例的平均值。”特征重要性在决策树中，⼀个特征的总体重要性可以⽤以下⽅法计算：遍历使⽤该特征的所有分割，并测量它相对于⽗节点减少了多少⽅差或基尼指数。所有重要性的总和被缩放为100，这意味着每个重要性可以解释为总体模型重要性的⼀部分。树分解通过将决策路径分解为每个特征的组成，可以解释决策树的单个预测。我们可以通过树跟踪决策，并通过在每个决策节点上添加的贡献来解释预测。决策树中的根节点是我们的起点。如果我们使⽤根节点进⾏预测，它将预测训练数据结果的均值。在下⼀个分割中，我们根据路径中的下⼀个节点减去或者添加⼀项。为了得到最终的预测，我们必须遵循要解释的数据实例的路径，并不断地添加到公式中。ˆf(x) = ¯y +D∑d=1split.contrib(d, x) = ¯y +p∑j=1feat.contrib(j, x)单个实例的预测是⽬标结果的均值加上在根节点和实例结束的终端节点之间发⽣的D 分割的所有贡献的总和。不过，我们对分割的贡献不感兴趣，⽽是对特征的贡献感兴趣。⼀个特征可能⽤于多次分割，或者根本不⽤于分割。我们可以为p 个特征的每⼀个添加贡献，并获得每个特征对预测贡献多少的解释。4.4.2示例让我们再看⼀下⾃⾏车租赁数据，我们希望通过决策树来预测某⼀天的⾃⾏车租⽤数量。学到的树如下所⽰：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 74, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型68图4.17. 拟合在⾃⾏车租赁数据上的回归树。树的最⼤允许深度设置为2。已为分割选择天数特征和温度特征。箱线图显⽰了终端节点中⾃⾏车数量的分布。第⼀次分割和第⼆次分割中的第⼀个是使⽤天数特征执⾏的，该特征统计数据收集开始后的天数，并涵盖了随着时间的推移⾃⾏车租赁服务变得越来越流⾏的趋势。在第105 天之前的天数中，⾃⾏车的预测数量⼤约是1800；在第106 天和第430 天之间，⼤约是3900。对于第430 天之后的天数，预测值为4600 (如果温度低于12 摄⽒度) 或6600 (如果温度⾼于12 摄⽒度)。特征的重要性告诉我们特征在多⼤程度上有助于提⾼所有节点的纯度。这⾥使⽤了⽅差，因为预测⾃⾏车租赁是⼀个回归任务。可视化树表明温度特征和天数特征都被⽤于分割，但没有量化哪个特征更重要。特征重要性表明天数特征远⽐温度特征重要。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 75, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型69图4.18. 通过平均提⾼的节点纯度可以衡量特征的重要性。4.4.3优点树结构⾮常适合捕获数据中特征之间的交互。这些数据最终分成不同的组，通常⽐线性回归中的多维超平⾯上的点更容易理解。可以说，解释相当简单。树结构也有⼀个自然的可视化，包括它的节点和边。树可以按照“2.6 ⼈性化的解释” ⼀节中的定义提供良好的解释。树结构⾃动将单个实例的预测值视为反事实：“如果某个特征⼤于/⼩于分割点，则预测值将是y1 ⽽不是y2”。树的解释具有对⽐性，因为你总是可以将实例的预测与只是树的其他叶节点的相关的“假设” 场景(由树定义) 进⾏⽐较。如果这棵树很短，像1 到3 个分割的深度，那么所得到的解释是有选择性的。深度为3 的树最多需要3 个特征和分割点来为单个实例的预测创建解释。预测的真实性取决于树的预测性能。对于短树的解释⾮常简单和普遍性，因为对于每⼀个分割，实例都属于⼀个或另⼀个叶⼦，并且⼆进制决策很容易理解。不需要转换特征。在线性模型中，有时需要取特征的对数。决策树同样适⽤于特征的任何单调变换。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 76, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型704.4.4缺点树不能处理线性关系。输⼊特征和结果之间的任何线性关系都必须通过分割来近似，从⽽创建⼀个阶跃函数。这不是有效的。这会缺乏平滑度。输⼊特征的微⼩变化会对预测结果产⽣很⼤影响，这通常是不可取的。想象⼀棵树预测房屋的价格，⽽这棵树使⽤房屋的⾯积⼤⼩作为分割特征之⼀。分割发⽣在100.5 平⽅⽶。可以想象⼀下，使⽤你的决策树模型进⾏房屋价格估计的⽤户：他们测量他们的房⼦，得出房⼦有99 平⽅⽶，然后放⼊模型中计算价格，得到20 万欧元的预测。⽤户注意到他们忘记了测量⼀个2平⽅⽶的⼩储藏室。储藏室有⼀道倾斜的墙，因此他们不确定是可以计数全部⾯积还是仅占⼀半。所以他们决定尝试100.0 和101.0 平⽅⽶。结果：模型输出了20 万欧元和20.5 万欧元，这是相当不合理的，因为这表明从99 平⽅⽶到100 平⽅⽶没有变化。树也相当不稳定。训练数据集中的⼀些更改可以创建完全不同的树。这是因为每个分割都依赖于⽗分割。如果选择其他特征作为第⼀个分割特征，则整个树结构将发⽣更改。如果结构如此容易地改变，则不会对模型产⽣信⼼。决策树很容易理解——只要它们很短。终端节点的数量随着深度的增加而迅速增加。终端节点越多，树越深，就越难理解树的决策规则。深度为1 表⽰2 个终端节点，深度2 表⽰最多4 个节点，深度3 表⽰最多8 个节点。⼀棵树中终端节点的最⼤数量是深度的2 的指数倍。4.4.5软件对于本节中的⽰例，我使⽤了R rpart 包实现CART。CART 以多种编程语⾔(包括Python) 实现。可以说，CART 是⼀个⾮常古⽼且有些过时的算法，并且有⼀些有趣的新算法可以拟合树。你可以在机器学习和统计学习CRAN 任务视图中的关键字“递归分区” 下找到⼀些决策树R 包的概述。4.5决策规则决策规则(Decision Rules) 是⼀个简单的IF-THEN 语句，由条件(也称为先⾏条件) 和预测组成。例如：“IF 今天下⾬AND 现在是四⽉(条件)，THEN 明天下⾬(预测)”。可以使⽤单个决策规则或多个规则的组合进⾏预测。决策规则遵循⼀个⼀般结构：IF 条件满⾜，THEN 做⼀个特定的预测。决策规则可能是最容易解释的预测模型。它们的IF-THEN 结构在语义上类似于⾃然语⾔和我们的思维⽅式，前提是条件是由可理解的特征构建的，条件的长度很短(⽤AND 连接的feature=value 组合较少)，并且没有太多规则。在编程中，编写IF-THEN 规则是很⾃然的。机器学习中的新功能是通过算法学习决策规则。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 77, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型71想象⼀下，使⽤⼀种算法来学习预测房屋价格的决策规则(价格low，medium 或high)。这个模型学习到的⼀个决策规则可能是：如果⼀个房屋超过100 平⽅⽶，并且有⼀个花园，那么它的价格就很⾼。更正式⼀点：IF size>100 AND garden=1 THEN value=high。让我们分解⼀下决策规则：• size>100 是IF 部分的第⼀个条件。• garden=1 是IF 部分的第⼆个条件。• 这两个条件通过“AND” 连接以创建新条件。两者都必须为真，才能应⽤该规则。• 预测结果(THEN 部分) 为value=high。决策规则在条件中⾄少使⽤⼀个feature=value 语句，对于“AND” 可以添加的数量没有上限。⼀个例外是没有显式IF 部分的默认规则，当没有其他规则应⽤时该规则适⽤，稍后将对此进⾏详细说明。决策规则的有⽤性通常概括为两个数字：⽀持度(Support) 和准确性(Accuracy)。• 规则的支持度或覆盖率(Coverage)：规则条件适⽤的实例所占的百分⽐称为⽀持度。例如，IF size=big AND location=good THEN value=high ⽤于预测房屋价格。假设1000 个房屋中有100 个⼤的⽽且位置好的，那么规则的⽀持度就是10%。预测(THEN 部分) 对于⽀持度的计算并不重要。• 规则的准确性或置信度(Confidence)：规则的准确性是衡量规则在规则条件适⽤的实例预测正确类别时的准确性的指标。例如：假设规则IFsize=bigANDlocation=goodTHEN value=high 适⽤的100 个房屋，其中，85 个房屋中的value=high，14 个房屋中的value=medium，1 个房屋中的value=low，则规则的准确性为85%。通常在准确性和⽀持度之间有⼀个权衡：通过在条件中添加更多的特征，我们可以获得更⾼的准确性，但会失去⽀持度。为了创建⼀个好的分类器来预测房屋的价格，我们可能不仅需要学习⼀条规则，⽽且需要学习10或20 条规则。然后事情会变得更加复杂，并且可能会遇到以下问题之⼀：• 规则可能会重叠：如果我想预测房屋的价格并且应⽤两个或多个规则，并且它们给我带来⽭盾的预测，该怎么办？• 没有适⽤的规则：如果我想预测房屋的价格，却没有适⽤的规则怎么办？组合多个规则有两种主要策略：决策列表(Decision List) (有序) 和决策集(Decision Set) (⽆序)。这两种策略都暗⽰了对重叠规则问题的不同解决⽅案。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 78, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型72• 决策列表向决策规则引⼊有序。如果第⼀条规则对实例的条件为真，则使⽤第⼀条规则的预测。如果没有，我们将转到下⼀个规则，检查它是否适⽤，依此类推。决策列表通过只返回适⽤列表中第⼀个规则的预测来解决重叠规则的问题。• 决策集类似于规则的民主，只是有些规则可能具有更⾼的投票权。在⼀套规则中，这些规则要么相互排斥，要么有解决冲突的策略，例如多数投票，这些策略可以通过个别规则的准确性或其他质量措施进⾏加权。当⼀些规则适⽤时，解释性可能会受到影响。决策列表和决策集都可能⾯临没有规则适⽤于实例的问题。这可以通过引⼊默认规则来解决。默认规则是在没有其他规则适⽤时应⽤的规则。默认规则的预测通常是数据点中最常见的⼀类，⽽其他规则没有涵盖这些数据点。如果⼀组规则或规则列表覆盖了整个特征空间，我们称之为详尽的。通过添加默认规则，集合或列表将⾃动变得详尽。有很多⽅法可以从数据中学习规则，本书远没有涵盖所有的这些⽅法。本节展⽰了其中三个。选择这些算法是为了涵盖通⽤的学习规则的⼀般思想，因此这三种算法代表了截然不同的⽅法。1. OneR 从单个特征中学习规则。OneR 的特点在于其简单性、可解释性、并且可以⽤作基准。2. 顺序覆盖(Sequential covering) 是⼀种通⽤过程，可以迭代地学习规则并删除新规则覆盖的数据点。许多规则学习算法都使⽤此过程。3. 贝叶斯规则列表(Bayesian Rule List) 使⽤贝叶斯统计将预先挖掘的频繁模式组合到决策列表中。使⽤预先挖掘模式是许多规则学习算法所使⽤的常见⽅法。让我们从最简单的⽅法开始：使⽤单个最佳特征来学习规则。4.5.1从单个特征学习规则(OneR)Holte (1993)[16] 提出的OneR 算法是最简单的规则归纳算法之⼀。从所有特征中，OneR 选择⼀个包含有关感兴趣结果的最多信息的特征，并从该特征创建决策规则。尽管命名为OneR 代表“⼀个规则”，但是算法会⽣成多个规则：它实际上是所选最佳特征的每个(唯⼀的，不重复的) 特征值的⼀个规则。⼀个更好的名字应该是OneFeatureRules。算法简单快捷：1. 通过选择适当的间隔来离散化连续特征。2. 对于每个特征：• 在特征值和(分类) 结果之间创建交叉表。• 对于特征的每个值，创建⼀个规则，预测具有此特定特征值(可以从交叉表中读取) 的实例的最常见类别。• 计算特征规则的总误差。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 79, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型733. 选择总误差最⼩的特征。OneR 始终覆盖数据集的所有实例，因为它使⽤所选特征的所有级别(可能值)。缺失值可以作为附加特征值处理，也可以预先输⼊。OneR 模型是⼀个只有⼀个分割的决策树。分割不⼀定像CART 中那样是⼆进制的(⼆分叉)，⽽是取决于(唯⼀的、不重复的) 特征值的数量。让我们来看⼀个例⼦，OneR 是如何选择最佳特征的。下表显⽰了有关房屋的⼈⼯数据集，其中包含有关房屋价格、位置、⾯积⼤⼩以及是否允许养宠物的信息。我们有兴趣学习⼀个简单的模型来预测房屋的价格。locationsizepetsvaluegoodsmallyeshighgoodbignohighgoodbignohighbadmediumnomediumgoodmediumonly catsmediumgoodsmallonly catsmediumbadmediumyesmediumbadsmallyeslowbadmediumyeslowbadsmallnolowOneR 在每个特征和结果之间创建交叉表：value=lowvalue=mediumvalue=highlocation=bad320location=good023value=lowvalue=mediumvalue=highsize=big002size=medium130size=small211', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 80, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型74value=lowvalue=mediumvalue=highpets=no112pets=only cats020pets=yes211对于每⼀个特征，我们⼀⾏⼀⾏地遍历表：每个特征值都是规则的IF 部分；对于具有这个特征值的实例，最常见的类别是预测，即规则的THEN 部分。例如，级别(可能值) 为small，medium 和big 的(⾯积) ⼤⼩特征在三个规则中。对于每个特征，我们计算⽣成的规则的总错误率，即误差总和。位置特征具有可能的值bad 和good。不良地段房屋最常见的价值是low，当我们⽤low 作为预测时，我们会有两个错误，因为有两个房屋的价格是medium 的。好地段的房屋预测值high，我们又有了两个错误，因为有两个房屋的价格medium。我们使⽤位置特征的误差是4/10，对于⾯积⼤⼩特征是3/10，对于宠物特征是4/10。⾯积⼤⼩特征⽣成误差最⼩的规则，并将⽤于最终的OneR 模型：IF size=small THEN value=lowIF size=medium THEN value=mediumIF size=big THEN value=highOneR 倾向于具有许多可能级别的特征，因为这些特征可以更容易地覆盖⽬标。设想⼀个只包含噪声不包含信号的数据集，这意味着所有特征都具有随机值，并且对⽬标没有预测价值。有些特征⽐其他特征有更多的级别。具有更多级别的特征更容易过拟合。每个数据中的实例具有不同级别的特征可以完美地预测整个训练数据集。⼀种解决⽅案是将数据拆分为训练集和验证集，学习训练数据的规则，并评估在验证集上选择特征时的总误差。连结(Ties) 是另⼀个问题，即当两个特征导致相同的总误差时。OneR 通过使⽤具有误差最⼩的第⼀个特征或具有卡⽅检验的p 值最⼩的特征来解决连结。示例让我们尝试使⽤真实数据进⾏OneR。我们使⽤宫颈癌分类任务来测试OneR 算法。将输⼊的所有连续特征离散化为5 个分位数，创建以下规则：Ageprediction(12.9,27.2]Healthy(27.2,41.4]Healthy(41.4,55.6]Healthy(55.6,69.8]Healthy', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 81, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型75Ageprediction(69.8,84.1]HealthyOneR 选择年龄特征作为最佳预测特征。由于癌症是罕见的，因此对于每个规则选择多数的类别作为预测，因此预测的标签始终是Healthy，这是⽆济于事的。在这种不平衡的情况下，使⽤标签预测是没有意义的。年龄(Age) 间隔与癌症/健康状况(Cancer/Healthy) 以及患癌症的⼥性百分⽐之间的交叉表更具参考价值：# Healthy# CancerP(Cancer)Age=(12.9,27.2]477260.05Age=(27.2,41.4]290250.08Age=(41.4,55.6]3140.11Age=(55.6,69.8]100.00Age=(69.8,84.1]400.00但是在你开始解释任何东西之前：由于对每个特征和每个值的预测都是Healthy，所以所有特征的总误差率都是相同的。默认情况下，通过使⽤误差率最低的特征中的第⼀个特征(这⾥，所有特征都有55/858) 来解决总误差中的连结，这恰好是年龄特征。OneR 不⽀持回归任务。但是我们可以通过将连续结果切成间隔来将回归任务转变为分类任务。我们使⽤此技巧将OneR 的⾃⾏车数量减少到四个四分位数(0-25%，25-50%，50-75% 和75-100%)，从⽽预测使⽤OneR 的⾃⾏车租赁数量。下表显⽰了拟合OneR 模型后的选定特征：mnthpredictionJAN[22,3152]FEB[22,3152]MAR[22,3152]APR(3152,4548]MAY(5956,8714]JUN(4548,5956]JUL(5956,8714]AUG(5956,8714]SEP(5956,8714]OKT(5956,8714]NOV(3152,4548]', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 82, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型76mnthpredictionDEZ[22,3152]所选特征是⽉份(mnth)。特征⽉份有12 个特征级别，⽐⼤多数其他特征都要多，因此存在过拟合的风险。在乐观的⽅⾯：⽉份特征可以处理季节性趋势(例如，冬季租车减少)，并且预测似乎是明智的。现在，我们从简单的OneR 算法过渡到使⽤具有更复杂条件的规则(由⼏个特征组成) 的更复杂过程：顺序覆盖。4.5.2顺序覆盖顺序覆盖(Sequential Covering) 是⼀个通⽤过程，它重复学习单个规则以创建⼀个决策列表(或集合)，该决策列表(或集合) 按规则覆盖整个数据集。许多规则学习算法是顺序覆盖算法的变体。本节介绍了主要的配⽅，以及RIPPER (⼀个变体的顺序覆盖算法)。这个想法很简单：⾸先，找到⼀个适⽤于某些数据点的好规则；删除规则所涵盖的所有数据点(当条件适⽤时，不管这些点是否正确分类，都会覆盖数据点)；重复规则学习，在剩余的点删除覆盖的点，直到没有剩余的点或满⾜另⼀个停⽌条件为⽌。最后结果是⼀个决策列表。这种重复规则学习和删除覆盖数据点的⽅法称为“变治法” (separate-and-conquer)。假设我们已经有⼀个算法可以创建⼀个覆盖部分数据的规则。两个类(⼀个正类，⼀个负类) 的顺序覆盖算法的⼯作原理如下：• 从⼀个空的规则列表(rlist) 开始。• 学习⼀个规则r。• 直到规则列表低于某个质量阈值(或正⾯⽰例未被覆盖)：– 将规则r 添加到rlist。– 删除规则r 涵盖的所有数据点。– 在剩余数据中学习其他的规则。• 返回决策列表。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 83, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型77图4.19. 覆盖算法的⼯作原理是依次⽤单个规则覆盖特征空间并删除那些规则已覆盖的数据点。出于可视化⽬的，特征x1 和x2 是连续的，但是⼤多数规则学习算法都需要分类特征。例如：我们有⼀个任务和数据集，⽤于根据⾯积⼤⼩，位置以及是否允许携带宠物来预测房屋的价格。我们学习了第⼀条规则，结果是：IF size=big AND location=good THEN value=high。然后我们从数据集中删除所有位于良好位置的⼤房屋。利⽤剩下的数据，我们学习下⼀个规则。可能是：IF location=good THEN value=medium。请注意，这条规则是在没有好位置的⼤房屋的数据上学习的，只留下有好位置的中⼩型房屋。对于多类别的设置，必须修改⽅法。⾸先，通过频繁度(Prevalence) 来对类别进⾏排序。顺序覆盖算法从最不频繁类开始，为其学习规则，删除所有覆盖的实例；然后转到第⼆不频繁的类，依此类推。当前类别始终被视为正类别，⽽具有较⾼频繁度的类别都被认为负类别。最后⼀个类是默认规则。这在分类中也被称为“⼀对多” 策略。我们如何学习⼀条规则？OneR 算法在这⾥是⽆⽤的，因为它总是覆盖整个特征空间。但是还有很多其他可能。⼀种可能⽅法是通过束搜索(Beam Search) 从决策树中学习单个规则：• 学习决策树(使⽤CART 或其他树学习算法)。• 从根节点开始，递归地选择最纯的节点(例如，具有较低的错误分类率)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 84, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型78• 终端节点的多数的类别⽤作规则的预测；通向该节点的路径⽤作规则条件。下图说明了树中的束搜索：图4.20. 通过搜索决策树中的路径来学习规则。⽣成决策树以预测感兴趣的⽬标。我们从根节点开始，贪婪地和迭代地遵循局部⽣成最纯⼦集(例如，最⾼准确性) 的路径，并将所有分割值添加到规则条件中。结果是：IF size=big AND location=good THEN value=high。学习单个规则是⼀个搜索问题，其中搜索空间是所有可能规则的空间。搜索的⽬标是根据某些条件找到最佳规则。有许多不同的搜索策略：爬⼭法(Hill-climbing)，束搜索，穷尽搜索(ExhaustiveSearch)，最佳优先搜索(Best-first Search)，有序搜索(Ordered Search)，随机搜索(StochasticSearch)，⾃上⽽下搜索(Top-down Search)，⾃下⽽上搜索(Bottom-up Search) 等等。Cohen (1995) [17] 提出的RIPPER (RepeatedIncrementalPruningtoProduceErrorReduction) 是顺序覆盖算法的⼀个变体。RIPPER 有点复杂，它使⽤后处理阶段(规则修剪) 来优化决策列表(或集合)。RIPPER 可以在有序或⽆序模式下运⾏，并⽣成决策列表或决策集。示例我们将使⽤RIPPER 作为例⼦。RIPPER 算法在宫颈癌的分类任务中未找到任何规则。当我们在回归任务上使⽤RIPPER 来预测⾃⾏车数时，会发现⼀些规则。由于RIPPER 仅适⽤于分类，因此必须将⾃⾏车计数转换为分类结果。我们通过将⾃⾏车数减少到四分位数来实现的。例如(4548，5956) 是涵盖4548 和5956 之间的预测⾃⾏车计数的间隔。下表显⽰了学习规则的决策', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 85, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型79列表。rules(days_since_2011 >= 438) and (temp >= 17) and (temp <= 27) and (hum <= 67) =>cnt=(5956,8714](days_since_2011 >= 443) and (temp >= 12) and (weathersit = GOOD) and (hum >= 59)=> cnt=(5956,8714](days_since_2011 >= 441) and (windspeed <= 10) and (temp >= 13) => cnt=(5956,8714](temp >= 12) and (hum <= 68) and (days_since_2011 >= 551) => cnt=(5956,8714](days_since_2011 >= 100) and (days_since_2011 <= 434) and (hum <= 72) and(workingday = WORKING DAY) => cnt=(3152,4548](days_since_2011 >= 106) and (days_since_2011 <= 323) => cnt=(3152,4548]=> cnt=[22,3152]解释很简单：如果条件适⽤，我们将在右侧预测⾃⾏车数量的间隔。最后⼀个规则是默认规则，当其他规则均不应⽤于实例时，该规则适⽤。要预测新实例，从列表顶部开始，检查规则是否适⽤。当⼀个条件匹配时，规则的右侧就是该实例的预测。默认规则确保始终存在预测。4.5.3贝叶斯规则列表在本节中，我们将向你展⽰学习决策列表的另⼀种⽅法，它遵循以下粗略的⽅法：1. 从数据中预先挖掘(pre-mined) 频繁使⽤的模式，这些模式可⽤作决策规则的条件。2. 从预先挖掘的规则中选择决策列表。使⽤此配⽅的特定⽅法称为贝叶斯规则列表(Letham 等⼈，2015)[18] (Bayesian Rule Lists) 或简称为BRL。BRL 使⽤贝叶斯统计从频繁模式中学习决策列表，这些频繁模式已使⽤FP-tree(Borgelt，2005)[19] 算法进⾏了预先挖掘。让我们从BRL 的第⼀步慢慢开始。频繁模式的预先挖掘频繁模式是特征值的频繁同现。作为BRL 算法的预处理步骤，我们使⽤特征(在此步骤中不需要⽬标结果) 并从中提取频繁出现的模式。模式可以是单个特征值size=medium 或者特征值的组合size=medium AND location=bad。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 86, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型80模式的频率(frequency) 是通过其在数据集中的⽀持度来测量：Support(xj = A) = 1nn∑i=1I(x(i)j= A)其中A 是特征值，n 是数据集中的数据点数量，I 是指⽰函数(如果实例i 的特征xj 具有级别A，则返回1，否则为0)。在房屋价格数据集中，如果20% 的房屋没有阳台，⽽80% 的房屋有⼀个或多个阳台，那么对balcony=0 的⽀持度为20%。⽀持度还可以测量特征值组合，例如balcony=0AND pets=allowed。有许多算法可以找到这种频繁的模式，例如Apriori 或FP-Growth。使⽤哪种⽅法⽆关紧要，只有找到模式的速度不同，但⽣成的模式总是相同的。我们将给出Apriori 如何找到频繁模式的⼤致想法。实际上，Apriori 算法由两部分组成，第⼀部分查找频繁的模式，第⼆部分从中构建关联规则。对于BRL 算法，我们只对Apriori 第⼀部分中⽣成的频繁模式感兴趣。在第⼀步中，Apriori 算法从⽀持度⼤于⽤户定义的最⼩⽀持度的所有特征值开始。如果⽤户说最⼩⽀持应该是10%，⽽只有5% 的房屋拥有size=big，我们将删除该特征值并仅保留size=medium和size=small 作为模式。这并不意味着将房屋从数据中删除，它只是意味着size=big 不会作为频繁模式返回。基于具有单个特征值的频繁模式，Apriori 算法迭代地寻找越来越⾼阶的特征值组合。模式是通过将feature=value 语句与逻辑AND (例如，size=medium AND location=bad) 相结合来构造的。删除⽀持度低于最⼩⽀持度的⽣成模式。最后我们有了所有频繁模式。频繁模式的任何⼦集都会再次频繁出现，这称为Apriori 性质。直观地说，它是有意义的：通过从模式中删除条件，缩⼩后的模式只能覆盖更多或相同数量的数据点，⽽不能覆盖更少的数据点。例如，如果20% 的房屋size=medium and location=good，那么size=medium 的房屋的⽀持度为20% 或更⼤。Apriori 性质⽤于减少要检查的模式的数量。只有在频繁模式的情况下，我们才需要检查⾼阶模式。现在我们已经完成了贝叶斯规则列表算法的预挖掘条件。但在我们继续进⾏BRL 的第⼆步之前，这⾥想说⼀下另⼀种基于预先挖掘模式的规则学习⽅法。其他⽅法建议将感兴趣的结果包含到频繁的模式挖掘过程中，并构建IF-THEN 规则的Apriori 算法的第⼆部分。由于该算法是⽆监督的，因此THEN 部分还包含我们不感兴趣的特征值。但是我们可以通过仅对THEN 部分感兴趣的结果的规则进⾏过滤。这些规则已经形成了⼀个决策集，但也可以对规则进⾏排列、修剪、删除或重新组合。然⽽，在BRL ⽅法中，我们使⽤频繁模式，学习THEN 部分以及如何使⽤贝叶斯统计将模式排列到决策列表中。学习贝叶斯规则列表BRL 算法的⽬标是通过选择预先挖掘的条件来学习⼀个准确的决策列表，同时对规则较少、条件较短的列表进⾏优先级排序。BRL 通过在条件长度(最好是较短的规则) 和规则数量(最好是较短', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 87, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型81的列表) 的先验分布中定义决策列表的分布来实现此⽬标。在给定简短假设的前提下，列表的后验概率分布使得描述决策列表的可能性，以及列表对数据拟合程度成为可能。我们的⽬标是找到使后验概率最⼤化的列表。由于⽆法直接从列表的分布中找到确切的最佳列表，BRL 建议采⽤以下⽅法：1. ⽣成初始决策列表，该列表是从先验分布中随机抽取的。2. 通过添加，切换或删除规则来迭代地修改列表，确保结果列表遵循列表的后验分布。3. 根据后验分布，从概率最⾼的采样列表中选择决策列表。让我们更仔细地研究⼀下算法：该算法从使⽤FP-Growth 预先挖掘的特征值模式开始。BRL 对⽬标的分布和定义⽬标分布的参数的分布做了许多假设。(这是贝叶斯统计。) 如果你不熟悉贝叶斯统计，不要太在意下⾯的解释。重要的是要知道贝叶斯⽅法可以结合现有知识或要求(所谓的先验分布)，同时也适合于数据。在决策列表的情况下，贝叶斯⽅法是有意义的，因为先验假设会推动决策列表较短且规则简短。⽬标是从后验分布中抽取决策列表d：p(d|x, y, A, α, λ, η)|{z}posteriori∝p(y|x, d, α)|{z}likelihood· p(d|A, λ, η)|{z}priori其中，d 是决策列表，x 是特征，y 是⽬标，A 是预先挖掘的条件集，λ 决策列表的先验期望长度，η 是⼀个规则中先验期望的条件数，α 是狄利克雷分布参数(如果是考虑正类别和负类别的先验伪计数，最好固定为(1,1))。p(d|x, y, A, α, λ, η)是根据观测数据和先验假设量化决策列表的概率的。这与给定决策列表和数据下的结果y 的似然，和给定先验假设和预先挖掘的条件下列表的概率的乘积成正⽐。p(y|x, d, α)是给定决策列表和数据下观察到的y 的似然。BRL 假设y 由狄利克雷多项分布⽣成。决策列表d解释数据越多，似然越⾼。p(d|A, λ, η)是决策列表的先验分布。它将针对列表中规则数的截断泊松分布(参数λ) 与针对规则的条件中特征值数(即条件长度) 的截断泊松分布(参数η) 相乘。如果⼀个决策列表能够很好地解释结果y 并且遵循先验假设，那么它很可能具有很⾼的后验概率。贝叶斯统计中的估计总是有点棘⼿，因为我们通常不能直接计算出正确的答案，⽽是使⽤马尔可夫链蒙特卡洛⽅法绘制候选者，对其进⾏评估并更新我们的后验估计。对于决策列表，这更为棘⼿，', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 88, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型82因为我们必须从决策列表的分布中绘制。BRL 的作者建议⾸先绘制⼀个初始决策列表，然后对其进⾏迭代修改，从列表的后验分布(决策列表的马尔可夫链) 中⽣成决策列表的样本。结果可能依赖于初始决策列表，因此建议重复此过程以确保列表的多样性。软件实现中的默认值是10 次。以下配⽅告诉我们如何绘制初始决策列表：• FP-Growth 预先挖掘模式。• 从截断的泊松分布中采样列表长度参数m。• 对于默认规则：采样⽬标值的狄利克雷多项分布参数θ0 (即在没有其他的可以应⽤时应⽤的规则)。• 对于决策列表规则j = 1, …, m，执⾏：– 对规则j 采样规则长度参数lj (条件长度)。– 从预先挖掘的条件中采样长度为lj 的条件。– 对THEN 部分采样狄利克雷多项分布参数(即给定规则的⽬标结果的分布)。• 对于数据集中的每个观测：– 从决策列表中查找⾸先适⽤的规则(⾃上⽽下)。– 从适⽤规则建议的概率分布(狄利克雷分布) 中得出预测结果。下⼀步是从这个初始样本开始⽣成许多新列表，从⽽从决策列表的后验分布中获取许多样本。通过从初始列表开始，然后随机地将规则移动到列表中的其他位置，或者将从预先挖掘的条件中的规则添加到当前决策列表中，或者从决策列表中删除规则，来对新决策列表进⾏采样。随机选择切换，添加或删除规则。在每个步骤中，算法都会估计决策列表的后验概率(准确性和简短性的混合)。Metropolis Hastings 算法确保我们采样具有较⾼后验概率的决策列表。此过程为我们提供了决策列表分布中的许多样本。BRL 算法选择具有最⾼后验概率的样本的决策列表。示例理论就是这样，现在让我们看看BRL ⽅法的实际应⽤。⽰例使⽤了Yang 等⼈(2017)[20] 提出的⼀种更快的BRL 变体，称为“可伸缩贝叶斯规则列表” (SBRL)。我们使⽤SBRL 算法来预测宫颈癌的风险。为了使SBRL 算法正常⼯作，我们⾸先必须离散化所有输⼊特征。为此，我们基于分位数的值的频率对连续特征进⾏了分类。我们得到以下规则：rulesIf {STDs=1} (rule[259]) then positive probability = 0.16049383', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 89, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型83ruleselse if {Hormonal.Contraceptives..years.=[0,10)} (rule[82]) then positive probability =0.04685408else (default rule) then positive probability = 0.27777778请注意，我们得到了明智的规则，因为THEN 部分的预测不是类别结果，⽽是癌症的预测概率。这些条件是从使⽤FP-Growth 算法预先挖掘的模式中选择的。下表显⽰了SBRL 算法可以从中选择以构建决策列表的条件池。在我作为⽤户允许的条件下，特征值的最⼤数⽬是2。以下是⼗个模式的⽰例：pre-mined conditionsNum.of.pregnancies=[3.67,7.33)IUD=0,STDs=1Number.of.sexual.partners=[1,10),STDs..Time.since.last.diagnosis=[1,8)First.sexual.intercourse=[10,17.3),STDs=0Smokes=1,IUD..years.=[0,6.33)Hormonal.Contraceptives..years.=[10,20),STDs..Number.of.diagnosis=[0,1)Age=[13,36.7)Hormonal.Contraceptives=1,STDs..Number.of.diagnosis=[0,1)Number.of.sexual.partners=[1,10),STDs..number.=[0,1.33)STDs..number.=[1.33,2.67),STDs..Time.since.first.diagnosis=[1,8)接下来，我们将SBRL 算法应⽤于⾃⾏车租赁预测任务。只有把预测⾃⾏车数量的回归问题转化为⼆分类任务时，这才有效。我们通过构建标签(如果⼀天的⾃⾏车数量超过4000 辆，则为1，否则为0) 随意创建了⼀个分类任务。SBRL 学习了以下列表：rulesIf {yr=2011,temp=[-5.22,7.35)} (rule[718]) then positive probability = 0.01041667else if {yr=2012,temp=[7.35,19.9)} (rule[823]) then positive probability = 0.88125000else if {yr=2012,temp=[19.9,32.5]} (rule[816]) then positive probability = 0.99253731else if {season=SPRING} (rule[351]) then positive probability = 0.06410256else if {yr=2011,temp=[7.35,19.9)} (rule[730]) then positive probability = 0.44444444else (default rule) then positive probability = 0.79746835', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 90, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型84让我们预测⼀下，在17 摄⽒度的温度下，2012 年⼀天内⾃⾏车的数量将超过4000 辆。第⼀条规则不适⽤，因为它只适⽤于2011 年。第⼆条规则适⽤，因为时间是在2012 年，17 摄⽒度是在时间间隔[7.35,19.9]。我们对出租4000 辆⾃⾏车的概率的预测是88%。4.5.4优点本节⼀般讨论IF-THEN 规则的好处。IF-THEN 规则很容易解释。它们可能是可解释模型中最容易解释的。此声明仅适⽤于规则数量较少、规则条件较短(最多3 个) 以及规则组织在决策列表或不重叠的决策集中的情况。决策规则可以像决策树那样具有表达能力，同时更紧凑。决策树通常也会遭受重复的⼦树，也就是说，当左⼦节点和右⼦节点中的分割具有相同的结构时。使⽤IF-THEN 规则的预测很快，因为只需要检查⼏个⼆进制语句就可以确定哪些规则适⽤。决策规则对于输⼊特征的单调变换具有很强的鲁棒性，因为条件中只有阈值会发⽣变化。它们对于异常值也很健壮，因为它只在条件适⽤或不适⽤时才重要。IF-THEN 规则通常⽣成稀疏模型，这意味着不包含许多特征。它们仅为模型选择相关特征。例如，默认情况下，线性模型为每个输⼊特征指定权重。不相关的特征可以简单地被IF-THEN 规则忽略。像来⾃OneR 这样的简单规则可以⽤作更复杂算法的基线。4.5.5缺点此部分处理IF-THEN 规则的缺点。关于IF-THEN 规则的研究和⽂献主要集中在分类上，⼏乎完全忽略了回归。虽然你可以将⼀个连续的⽬标划分为间隔，并将其转化为分类问题，但你总是会丢失信息。⼀般来说，如果⽅法可以同时⽤于回归和分类，则更具吸引⼒。这些特征通常也必须是分类的。这意味着如果要使⽤数值特征，必须对它们进⾏分类。有很多⽅法可以将⼀个连续的特征分割成间隔，但这并不是⼀件微不⾜道的事情，并且会带来许多没有明确答案的问题。该特征应划分为多少个间隔？分割标准是什么：固定的间隔长度、分位数或其他什么？对连续特征进⾏分类是⼀个不容忽视的问题，但经常被忽视，⼈们只是使⽤次佳的⽅法(就像我们在⽰例中所做的那样)。许多旧的规则学习算法容易过拟合。这⾥给出的算法都⾄少有⼀些保护措施来防⽌过拟合：OneR是有限的，因为它只能使⽤⼀个特征(只有当特征具有太多的级别(可能值) 或有许多特征时才有问', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 91, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型85题，这等同于多个测试问题)，RIPPER 进⾏修剪，以及贝叶斯规则列表对决策列表施加了先验分布。在描述特征和输出之间的线性关系时，决策规则是不好的。这是他们与决策树共享的问题。决策树和规则只能产⽣阶跃预测函数，其中预测的变化总是离散的阶跃，⽽不是平滑的曲线。这与输⼊必须是分类的问题有关。在决策树中，它们通过分割来隐式分类。4.5.6软件和替代方法OneR 在R 包OneR 中实现，R 包⽤于本书中的⽰例。OneR 还可以在Weka 机器学习库中实现，并且可以在Java，R 和Python 中使⽤。RIPPER 也在Weka 中实现。对于⽰例，我在RWeka 包中使⽤了JRIP 的R 实现。SBRL 可在R 包获得(我在⽰例中使⽤)，Python 或C 实现提供。我们不会尝试列出学习决策规则集和列表的所有替代⽅法，⽽是会指出⼀些总结性⼯作。我们推荐Fuernkranz 等撰写的《Foundations of Rule Learning》(2012)[ürnkranz12]。对于那些想深⼊研究该主题的⼈来说，这是⼀项有关学习规则的⼴泛⼯作。它为思考学习规则提供了⼀个整体框架，并提出了许多规则学习算法。我们还建议Weka 规则学习器，该学习器实现了RIPPER，M5Rules，OneR，PART 等。IF-THEN 规则可⽤于线性模型中，如本书有关RuleFit 算法的⼀节中所述。4.6RuleFitFriedman 和Popescu (2008)[21] 的RuleFit 算法学习了稀疏线性模型，该模型包括以决策规则形式⾃动检测到的交互作⽤。线性回归模型不考虑特征之间的交互作⽤。具有像线性模型⼀样简单且可解释但又集成了特征交互的模型，这难道不⽅便吗？RuleFit 填补了这⼀空⽩。RuleFit 学习具有原始特征以及许多新特征(决策规则) 的稀疏线性模型。这些新特征捕获了原始特征之间的交互。RuleFit 从决策树⾃动⽣成这些特征。通过将分割的决策合并为规则，可以将通过树的每条路径转换为决策规则。节点预测被丢弃，决策规则中仅使⽤分割：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 92, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型86图4.21. 可以从具有3 个终端节点的树中⽣成4 条规则。这些决策树从何⽽来？这些树被训练来预测感兴趣的结果。这样可以确保分割对于预测任务有意义。⽣成⼤量树的任何算法都可以⽤于RuleFit，例如随机森林。每棵树都分解为决策规则，这些规则⽤作稀疏线性回归模型(Lasso) 中的附加特征。RuleFit 论⽂使⽤波⼠顿房屋数据来说明这⼀点：⽬的是预测波⼠顿社区的房价中位数。RuleFit ⽣成的规则之⼀是：IF number of rooms > 6.64 AND concentration of nitric oxide < 0.67THEN 1 ELSE 0。RuleFit 还提供了特征重要性度量，可帮助识别对于预测很重要的线性项和规则。根据回归模型的权重可以计算特征重要性。重要性度量可以针对原始特征进⾏汇总(特征⽤于其“原始” 形式，也可能⽤于决策规则中)。RuleFit 还引⼊了部分依赖图，通过改变特征来显⽰预测的平均变化。部分依赖图是⼀种与模型⽆关的⽅法，可以与任何模型⼀起使⽤，在有关部分依赖图的⼀节中进⾏了说明。4.6.1解释和示例由于RuleFit 最终会估计⼀个线性模型，因此其解释与“常规” 线性模型的解释相同。唯⼀的区别是该模型具有从决策规则派⽣的新特征。决策规则是⼆进制特征：值为1 表⽰满⾜规则的所有条件，否则值为0。对于RuleFit 中的线性项，其解释与线性回归模型中的解释相同：如果特征增加⼀个单位，预测结果会随着相应的特征权重⽽变化。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 93, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型87在此⽰例中，我们使⽤RuleFit 预测给定⽇期的⾃⾏车出租数量。该表显⽰了RuleFit ⽣成的五个规则，以及它们的Lasso 权重和重要性。该计算将在本节后⾯解释。DescriptionWeightImportancedays_since_2011 > 111 & weathersit in (“GOOD”, “MISTY”)79330337.25 <= hum <= 90-20272temp > 13 & days_since_2011 > 5546762394 <= windspeed <= 24-41202days_since_2011 > 428 & temp > 5366179最重要的规则是：“days_since_2011> 111 ＆weathersit in (“ GOOD”，“ MISTY”)”，相应的权重为793。解释是：如果days_since_2011> 111 ＆weathersit in (“ GOOD”，“ MISTY”)，那么当所有其他特征值保持固定时，预测的⾃⾏车数量将增加793。从最初的8 个特征中总共创建了278个这样的规则。⾮常多！但是多亏了Lasso，278 个中只有58 个的权重⾮0。计算全局特征重要性表明，温度特征和天数特征是最重要的特征：图4.22. 预测⾃⾏车数量的RuleFit 模型的特征重要性度量。预测的最重要特征是温度特征和天数特征。特征重要性度量包括原始特征项以及特征出现的所有决策规则的重要性。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 94, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型88解释模板这种解释类似于线性模型：如果特征xj 改变⼀个单位，则预测结果会改变βj，前提是所有其他特征保持不变。决策规则的权重解释是⼀种特殊情况：如果⼀个决策规则rk 所有条件都适⽤，那么预测结果变化αk (在线性模型中规则rk 学到的权重)。对于分类(使⽤逻辑回归⽽不是线性回归)：如果决策规则所有条件均适⽤，则事件与⽆事件的⼏率会改变αk。4.6.2理论让我们更深⼊地研究RuleFit 算法的技术细节。RuleFit 由两个部分组成：第⼀个部分从决策树创建“规则”，第⼆个部分以原始特征和新规则作为输⼊⽤线性模型拟合(因此名称为“RuleFit”)。步骤1：规则生成规则是什么样的？算法⽣成的规则具有简单的形式。例如：IF x2 < 3 AND x5 < 7 THEN 1 ELSE 0。通过分解决策树来构造规则：到树中节点的任何路径都可以转换为决策规则。⽤于规则的树被拟合来预测⽬标结果。因此，分割和⽣成规则经过优化可以预测你感兴趣的结果。你只需使⽤“AND” 将导致特定节点的⼆进制决策连接起来，然后你就有了⼀条规则。我们希望⽣成许多多样且有意义的规则。梯度提升⽤于通过⽤原始特征X 对y 进⾏回归或分类来拟合决策树的集成。每个⽣成的树都转换为多个规则。不仅提升树，任何树集成算法都可以⽤作RuleFit ⽣成树。可以⽤以下通式描述树集成：f(x) = a0 +M∑m=1amfm(X)M 是树的数量，fm(x) 是第m 个树的预测函数，α 是权重。Bagging，随机森林，AdaBoost 和MART 产⽣树集成，可⽤于RuleFit。我们从集成的所有树中创建规则。每个规则rm 的形式为：rm(x) =∏j∈TmI(xj ∈sjm)其中Tm 是第m 个树中使⽤的⼀组特征。I 是指⽰函数，当特征xj 在第j 个特征的值s 的指定⼦集中(由树分割指定) 时为1，否则为0。对于数值特征，sjm 是特征值范围内的间隔。间隔看起来像两种情况之⼀：xsjm,lower < xjxj < xsjm,upper该特征的进⼀步分割可能会导致更复杂的间隔。对于分类特征，⼦集s 包含特征的某些特定类别。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 95, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型89⾃⾏车租赁数据集的⽰例：r17(x) = I(xtemp < 15) · I(xweather ∈{good, cloudy}) · I(10 ≤xwindspeed < 20)如果满⾜所有三个条件，则此规则返回1，否则返回0。RuleFit 从树中提取所有可能的规则，⽽不仅仅是从叶节点中。因此，将创建的另⼀个规则是：r18(x) = I(xtemp < 15) · I(xweather ∈{good, cloudy})总共，从具有tm 个终端节点的M 个树的集合中创建的规则数是：K =M∑m=12(tm −1)RuleFit 作者介绍的⼀个技巧是学习具有随机深度的树，以便⽣成许多具有不同长度的不同规则。请注意，我们丢弃每个节点中的预测值，仅保留将我们引导到节点的条件，然后从中创建规则。决策规则的加权在RuleFit 的步骤2 中完成。另⼀种⽅式看步骤1：RuleFit 从你的原始特征⽣成⼀组新特征。这些特征是⼆进制的，可以表⽰原始特征的⾮常复杂的交互。选择规则以最⼤化预测任务。规则是从协变量矩阵X ⾃动⽣成的。你可以简单地将规则视为基于原始特征的新特征。步骤2：稀疏线性模型你在步骤1 中获得了许多规则。由于第⼀步可以看作仅是特征变换，因此你仍然⽆法完成模型拟合。另外，你希望减少规则数量。除了这些规则之外，原始数据集中的所有“原始” 特征也将⽤于稀疏线性模型中。每个规则和每个原始特征都将成为线性模型中的特征并获得权重估计。添加原始特征是因为树⽆法表⽰y 和x 之间的简单线性关系。在训练稀疏线性模型之前，我们先对原始特征进⾏调整，以使它们对异常值更加健壮：l∗j(xj) = min(δ+j , max(δ−j , xj))其中δ−j 和δ+j 是特征xj 的数据分布的δ 分位数。δ 选择为0.05 意味着在5% 最⼩值或5% 最⼤值中的任何特征值xj 将分别设置为5% 或95% 的分位数。根据经验，你可以选择δ=0.025。另外，必须对线性项进⾏归⼀化，以便它们与典型决策规则具有相同的先验重要性：lj(xj) = 0.4 · l∗j(xj)/std(l∗j(xj))0.4 是均匀⽀持度分布sk ∼U(0, 1) 的规则的平均标准差。我们结合两种类型的特征以⽣成新的特征矩阵，并使⽤具有以下结构的Lasso 训练稀疏线性模型：ˆf(x) = ˆβ0 +K∑k=1ˆαkrk(x) +p∑j=1ˆβjlj(xj)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 96, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型90其中ˆα 是规则特征的估计权重向量，ˆβ 是原始特征的权重向量。由于RuleFit 使⽤Lasso，损失函数会获得附加约束，该约束会强制某些权重获得零估计：({ˆα}K1 , {ˆβ}p0) = argmin{ˆα}K1 ,{ˆβ}p0n∑i=1L(y(i), f(x(i))) + λ ·\\uf8eb\\uf8edK∑k=1|αk| +p∑j=1|bj|\\uf8f6\\uf8f8结果是⼀个线性模型，该模型对所有原始特征和规则都具有线性效应。解释与线性模型相同，唯⼀的区别是某些特征现在是⼆进制规则。步骤3 (可选)：特征重要性对于原始特征的线性项，使⽤标准预测变量来测量特征重要性：Ij = |ˆβj| · std(lj(xj))其中βj 是来⾃Lasso 模型的权重，是std(lj(xj)) 在数据上线性项的标准差。对于决策规则⽽⾔，重要性的计算公式如下：Ik = |ˆαk| ·√sk(1 −sk)其中ˆαk 是决策规则的关联Lasso 权重，sk 是数据中特征的⽀持度，即决策规则适⽤的数据点的百分⽐(其中rk(x) = 0)：sk = 1nn∑i=1rk(x(i))特征是线性项，也可能在许多决策规则中。我们如何衡量特征的总体重要性？可以为每个单独的预测测量特征的重要性Jj(x)：Jj(x) = Il(x) +∑xl∈rkIk(x)/mk其中Il 是(出现xj 的) 线性项的重要性，Ik 是(出现xj 的) 决策规则的重要性，mk 是是构成规则rk 的特征数。通过从所有实例中添加特征重要性，可以为我们提供全局特征重要性：Jj(X) =n∑i=1Jj(x(i))可以选择实例的⼦集并计算该组的特征重要性。4.6.3优点RuleFit ⾃动将特征交互添加到线性模型。因此，它解决了必须⼿动添加交互作⽤项的线性模型问题，并且对建模⾮线性关系的问题有所帮助。RuleFit 可以处理分类和回归任务。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 97, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型91创建的规则易于解释，因为它们是⼆进制决策规则。规则是否适⽤于实例。仅当规则中条件的数量不太⼤时，才能保证良好的可解释性。对于我来说，有1 到3 个条件的规则似乎很合理。这意味着树集成中树的最⼤深度为3。即使模型中有很多规则，它们也不适⽤于每个实例。对于单个实例，只有少数规则适⽤(= 具有⾮零的权重)。这提⾼了局部可解释性。RuleFit 提出了许多有⽤的诊断⼯具。这些⼯具与模型⽆关，因此你可以在本书的模型⽆关部分中找到它们：特征重要性，部分依赖图和特征交互。4.6.4缺点有时RuleFit 创建许多规则，这些规则在Lasso 模型中的权重⾮零。可解释性随着模型中特征数量的增加⽽降低。⼀种有希望的解决⽅案是强制将特征效应设为单调，这意味着特征的增加必须导致预测的增加。⼀个奇闻轶事的缺点：论⽂声称RuleFit 的性能很好——通常接近随机森林的预测性能！但是在我亲⾃尝试过的少数案例下，性能令⼈失望。只需尝试解决你的问题，然后查看其性能。RuleFit 过程的最终产物是⼀个具有额外的花哨特征(决策规则) 的线性模型。但是，由于它是线性模型，因此权重解释仍然⾮确定性的。它具有与通常的线性回归模型相同的“脚注”：“ …假设所有其他特征均已固定。” 当你有规则重叠时，它会变得有些棘⼿。例如，⽤于⾃⾏车预测的⼀个决策规则(特征) 可以是：“ temp > 10”，⽽另⼀个规则可以是“ temp > 15 ＆weather=‘GOOD’ ”。如果天⽓晴朗且温度⾼于15 摄⽒度，则温度会⾃动⾼于10 摄⽒度。在适⽤第⼆条规则的情况下，也适⽤第⼀条规则。对于第⼆条规则，估计权重的解释是：“假设所有其他特征保持不变，当天⽓好且温度在15 度以上，预测的⾃⾏车数量将增加β2。”。但是，现在⾮常清楚的是，“所有其他固定的特征” 是有问题的，因为如果应⽤规则2，那么规则1 也适⽤，解释是⽆意义的。4.6.5软件和替代方法Fokkema 和Christoffersen (2017)[22] 在R 中实现了RuleFit 算法，你可以在Github 上找到Python版本。⼀个⾮常相似的框架是skope-rules，这是⼀个Python 模块，它也从集成中提取规则。它学习最终规则的⽅式有所不同：⾸先，skope-rules 会基于召回率和精度阈值删除性能不佳的规则。然后，基于规则的逻辑项(变量+ 较⼤/较⼩的运算符) 的多样性和性能(F1 值) 进⾏选择，删除重复的规则和相似的规则。最后⼀步不依赖于使⽤Lasso，⽽是仅考虑袋外F1 值和构成规则的逻辑项。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 98, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第四章可解释的模型924.7其他可解释模型可解释模型的列表在不断增长且规模未知。它包括简单的模型，例如线性模型，决策树和朴素的贝叶斯模型，还包括更复杂的模型(这些模型结合或修改了不可解释的机器学习模型，以使其更具可解释性)。特别是后⼀种类型的模型的出版物⽬前正以⾼频率出版，很难跟上发展的步伐。该书在本节中仅介绍朴素贝叶斯分类器和k-最近邻。4.7.1朴素贝叶斯分类器朴素贝叶斯分类器使⽤条件概率的贝叶斯定理。对于每个特征，它根据特征值计算类的概率。朴素贝叶斯分类器独⽴地计算每个特征的类概率，这相当于特征独⽴性的强假设。朴素贝叶斯是⼀个条件概率模型，它对Ck 的概率建模如下：P(Ck|x) = 1Z P(Ck)n∏i=1P(xi|Ck)Z 是⼀个缩放参数，可确保所有类别的概率之和为1 (否则它们将不是概率)。类的条件概率是类概率乘以给定类每个特征的概率，并通过Z 进⾏归⼀化。可以使⽤贝叶斯定理来推导该公式。由于独⽴性假设，朴素贝叶斯是⼀个可解释的模型。可以在模块级别上解释它。由于我们可以解释条件概率，因此对于每个特征，它对特定类别预测的贡献是⾮常明显的。4.7.2k-最近邻k-最近邻⽅法可⽤于回归和分类，并将数据点的最近邻⽤于预测。对于分类，k-最近邻⽅法分配实例最近邻的最常见类。为了进⾏回归，它采⽤了邻居结果的平均值。棘⼿的部分是找到合适的k 并决定如何测量实例之间的距离，从⽽最终确定邻域。k-最近邻模型不同于本书中介绍的其他可解释模型，因为它是基于实例的学习算法。如何解读k-最近邻？⾸先，没有要学习的参数，因此在模块级别没有可解释性。此外，由于模型本质上是局部的并且没有明确学习的全局权重或结构，因此缺乏全局模型的可解释性。也许在局部可以解释？为了解释预测，你始终可以检索⽤于预测最近的k 个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 99, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法将解释与机器学习模型(= 与模型⽆关的解释⽅法) 分离具有⼀些优势(Ribeiro，Singh 和Guestrin，2016[23])。与模型⽆关(也称模型不可知) 的解释⽅法相对于模型特定的解释⽅法的最⼤优势是它们的灵活性。当解释⽅法可以应⽤于任何模型时，机器学习开发⼈员可以⾃由使⽤他们喜欢的任何机器学习模型。任何建⽴在对机器学习模型的解释之上的东西(例如图形或⽤户界⾯)，也将变得独⽴于底层机器学习模型。通常，不仅要评估⼀种机器学习模型，还要评估多种类型的机器学习模型来解决任务，并且在可解释性⽅⾯⽐较模型时，使⽤与模型⽆关的解释会更容易，因为相同的⽅法可以⽤于任何类型的模型。模型⽆关的解释⽅法的替代⽅法是仅使⽤可解释模型，这通常具有⼀个很⼤的缺点，即与其他机器学习模型相⽐，预测性能会丢失，并且你只能使⽤⼀种模型。另⼀种选择是使⽤特定于模型的解释⽅法。这样做的缺点是，它还会将你绑定到⼀种模型类型，并且将很难切换到其他模型类型。模型不可知的解释系统的理想⽅⾯是(Ribeiro，Singh 和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例如随机森林和深度神经⽹络。• 解释的灵活性：你不限于某种形式的解释。在某些情况下，线性公式可能会有⽤，⽽在其他情况下，特征重要性的图形可能会有⽤。• 表示方式的灵活性：解释系统应该能够使⽤与所解释模型不同的特征表⽰⽅式。对于使⽤抽象词嵌⼊向量的⽂本分类器，可能更希望使⽤单个词的存在进⾏解释。更大的图景让我们对模型⽆关的可解释性进⾏⾼层次的研究。我们通过收集数据来捕获世界，然后通过学习使⽤机器学习模型预测(针对任务的) 数据来进⼀步抽象世界。可解释性只是帮助⼈们理解的最上⼀层。93', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 100, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法94图5.1. 可解释的机器学习的图景。现实世界在以解释的形式到达⼈类之前经历了许多层次。最低层是世界层(World)。从字⾯上看，这可能是⾃然本⾝，例如⼈体的⽣物学及其对药物的反应⽅式，还可以是房地产市场等抽象的事物。世界层包含可以观察到的所有有趣的事物。最终，我们想了解⼀些有关世界的知识并与之互动。第⼆层是数据层(Data)。我们必须对世界进⾏数字化处理，以使其可⽤于计算机并存储信息。数据层包含图像、⽂本、表格数据等中的任何内容。通过基于数据层拟合机器学习模型，我们得到了黑盒模型层(Black Box Model)。机器学习算法从现实世界中学习数据，以做出预测或找到结构。在⿊盒模型层上⽅是可解释性方法层(Interpretability Methods)，该层有助于我们处理机器学', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 101, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法95习模型的不透明性。特定诊断的最重要特征是什么？为什么将⾦融交易分类为欺诈？最后⼀层被人类(Human) 占据。看！之所以向你招⼿，是因为你正在阅读本书，并为⿊盒模型提供了更好的解释！⼈类最终是这些解释的消费者。这种多层抽象还有助于理解统计学家和机器学习专家在⽅法上的差异。统计⼈员处理数据层，例如计划临床试验或设计调查。他们跳过⿊盒模型层，然后转到可解释性⽅法层。机器学习专家处理数据层，例如收集带标签的⽪肤癌图像样本或爬取维基百科。然后他们训练了⿊盒机器学习模型。跳过了可解释性⽅法层，⼈类直接处理了⿊盒模型的预测。可解释的机器学习很好地融合了统计学家和机器学习专家的⼯作，这⾮常棒。当然，这张图并不能捕捉到⼀切：数据可能来⾃模拟。⿊盒模型还会输出可能甚⾄⽆法传给⼈类的预测，⽽只会提供给其他机器，依此类推。但是总的来说，了解可解释性如何成为机器学习模型之上的这⼀新层是⼀个有⽤的抽象。5.1部分依赖图部分依赖图(Partial Dependence Plot，简称PDP 或PD 图) 显⽰了⼀个或两个特征对机器学习模型的预测结果的边际效应(JH Friedman，2001[1])。部分依赖图可以显⽰⽬标和特征之间的关系是线性的、单调的或更复杂的。例如，当应⽤于线性回归模型时，部分依赖图始终显⽰线性关系。⽤于回归的部分依赖函数定义为：ˆfxS(xS) = ExC[ˆf(xS, xC)]=∫ˆf(xS, xC)dP(xC)xS 是其部分依赖函数应被绘制的特征，xC 是在机器学习模型ˆf 中使⽤的其他特征。通常，集合S中只有⼀个或两个特征。S 中的特征是我们想要了解其对预测的影响的那些。特征向量xS 和xC合并组成总特征空间x。部分依赖性通过在集合C 中的特征分布上边缘化机器学习模型输出⽽起作⽤，因此该函数显⽰了我们感兴趣的集合S 中的特征与预测结果之间的关系。通过边缘化其他特征，我们得到了仅依赖于S 中的特征以及与其他特征的交互作⽤的函数。部分依赖函数ˆfxS 是通过计算训练数据中的平均值来估算的，也称为蒙特卡洛⽅法：ˆfxS(xS) = 1nn∑i=1ˆf(xS, x(i)C )部分依赖函数告诉我们特征S 的给定值对预测的平均边际效应是多少。在此公式中，xC 是数据集中我们不感兴趣的特征的实际特征值，n 是数据集中的实例数。PDP 的⼀个假设是C 中的特征与S 中的特征不相关。如果违反此假设，则为部分依赖图计算的平均值将包含极不可能或甚⾄不可能的数据点(请参见缺点)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 102, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法96对于机器学习模型输出概率的分类，部分依赖函数显⽰给定S 中不同的特征值下特定类别的概率。处理多个类别的⼀种简单⽅法是为每个类别绘制⼀条线或图。部分依赖图是⼀种全局⽅法：该⽅法考虑所有实例，并给出有关特征与预测结果的全局关系的说明。分类特征到⽬前为⽌，我们仅考虑了数值特征。对于分类特征，部分依赖很容易计算。对于(特征) 每个类别值，我们通过强制所有数据实例具有相同类别值来获得PDP 估计。例如，如果我们查看⾃⾏车租赁数据集，并对季节的部分依赖图感兴趣，则会得到4 个数字，每个季节⼀个。为了计算“夏季”的值，我们将所有数据实例的季节替换为“夏季”，然后对预测取平均值。5.1.1示例实际上，特征集S 通常仅包含⼀个特征或最多包含两个特征，因为⼀个特征产⽣2D 图，⽽两个特征产⽣3D 图。除此之外的⼀切都⾮常棘⼿。甚⾄在2D 纸张或显⽰器上显⽰3D 都已经具有挑战性。让我们回到回归⽰例，在该⽰例中，我们可以预测给定⼀天要租⽤的⾃⾏车数量。⾸先，我们拟合机器学习模型，然后分析部分依赖关系。在这种情况下，我们拟合了⼀个随机森林来预测⾃⾏车的数量，并使⽤部分依赖图来可视化模型学习到的关系。下图显⽰了天⽓特征对预测⾃⾏车数的影响。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 103, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法97图5.2. ⾃⾏车数量预测模型以及温度、湿度和风速的PDP。在温度上可以看到最⼤的差异。温度越⾼，⾃⾏车出租越多。这种趋势上升到20 摄⽒度，然后在30 摄⽒度处变平并略有下降。x 轴上的标记表⽰数据分布。对于温暖但不太热的天⽓，该模型预测平均会租⽤⼤量⾃⾏车。当湿度超过60 时，潜在的骑⾃⾏车的⼈越来越不愿意租⽤⾃⾏车。另外，风越⼤，骑⾃⾏车的⼈就越少，这是有道理的。有趣的是，当风速从25 km/h 增加到35 km/h 时，⾃⾏车租赁的预测数量不会减少，但是没有太多的训练数据，因此机器学习模型可能⽆法学习该范围的有意义的预测。⾄少从直觉上讲，我预计⾃⾏车的数量会随着风速的增加⽽减少，特别是在风速很⾼时。为了说明分类特征的部分依赖图，我们研究了季节特征对预测⾃⾏车租⽤的影响。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 104, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法98图5.3. ⾃⾏车数量预测模型和季节的PDP。出乎意料的是，所有季节都对模型预测产⽣相似的影响，仅在春季，模型预测的⾃⾏车租量会减少。我们还计算宫颈癌分类的部分依赖性。这次，我们拟合了⼀个随机森林以根据风险因素预测⼥性是否会患上宫颈癌。我们为随机森林计算并可视化癌症概率对不同特征的部分依赖性：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 105, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法99图5.4. 基于年龄和服⽤激素避孕药的时间的癌症概率的PDP。对于年龄，PDP 显⽰直到40 岁的概率都很低，之后才增加。服⽤激素避孕药的时间越长，预计的癌症风险就越⾼，尤其是10 年后。对于这两个特征，没有很多具有⼤数值的数据点可⽤，因此在这些区域中，PD 估计不太可靠。我们还可以同时可视化两个特征的部分依赖关系：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 106, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法100图5.5.患癌概率的PDP 以及年龄和怀孕次数的交互作⽤。该图显⽰了45 岁时患癌的概率增加。对于25 岁以下的⼈，怀孕1 或2 次的⼥性的预测癌症风险要低于怀孕0 次或2 次以上的⼥性。但是得出结论时要⼩⼼：这可能只是相关性，⽽不是因果关系！5.1.2优点部分依赖图的计算很直观：如果我们强制所有数据点都假定该特征值，则特定特征值处的部分依赖函数表⽰平均预测。根据我的经验，⾮专业⼈⼠通常会很快理解PDP 的概念。如果你为其计算PDP 的特征与其他特征不相关，则PDP 可以完美地表⽰该特征如何平均影响预测。在不相关的情况下，解释很清楚：部分依赖图显⽰了第j 个特征更改时数据集中的平均预测如何变化。当特征相关时会更加复杂，另请参见缺点。部分依赖图很容易实现。部分依赖图的计算具有因果关系。我们⼲预⼀项特征并测量预测的变化。在此过程中，我们分析了特征与预测之间的因果关系。[24] 这种关系对于模型是因果关系的——因为我们明确地将结果建模为特征的函数——但不⼀定对现实世界有效！', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 107, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1015.1.3缺点部分依赖函数中实际的最大特征数量为2。这不是PDP 的错，⽽是⼆维表⽰(纸或屏幕) 的错，也是我们⽆法想象3 个以上维度的错。⼀些PD 图未显⽰特征分布。忽略分布可能会产⽣误导，因为你可能会过度解释⼏乎没有数据的区域。通过显⽰RUG (x 轴上的数据点指⽰器) 或直⽅图可以轻松解决此问题。独立性的假设是PD 图最⼤的问题。假定针对其计算了部分依赖性的特征与其他特征不相关。例如，假设你要根据⼀个⼈的体重和⾝⾼来预测⼀个⼈⾛多快。对于其中⼀个特征(例如⾝⾼) 的部分依赖性，我们假设其他特征(体重) 与⾝⾼不相关，这显然是错误的假设。对于某个⾝⾼(例如200 厘⽶) 的PDP 的计算，我们对体重的边际分布求平均值，其中可能包括50 公⽄以下的体重，这对于2 ⽶⾼的⼈来说是不现实的。换句话说：当特征关联时，我们会在特征分布区域中创建实际概率⾮常低的新数据点(例如，某⼈⾝⾼2 ⽶但体重不⾜50 公⽄的概率不⼤)。解决这个问题的⼀种⽅法是适⽤于条件分布⽽⾮边际分布的累积局部效应图或简称ALE 图。异质效应可能被隐藏，因为PD 曲线仅显⽰平均边际效应。假设对于⼀个特征，你的数据点中的⼀半与预测具有正相关关系——特征值越⼤，预测值越⼤——另⼀半有负相关性——特征值越⼩，预测值越⼤。PD 曲线可能是⼀条⽔平线，因为数据集的两半的效果可能会相互抵消。然后，你可以得出结论，该特征对预测没有影响。通过绘制个体条件期望曲线⽽不是聚合线，我们可以发现异构效应。5.1.4软件和替代方法有许多实现PDP 的R 包。我将iml 包⽤作⽰例，但也有pdp 或DALEX。在Python 中，scikit-learn内置了部分依赖图，你可以使⽤PDPBox。本书介绍的PDP 替代⽅法是ALE 图和ICE 曲线。5.2个体条件期望个体条件期望(Individual Conditional Expectation，简称ICE) 图为每个实例显⽰⼀条线，该线显⽰了特征更改时实例的预测如何改变。特征平均效应的部分依赖图是⼀种全局⽅法，因为它不关注特定实例，⽽是关注整体平均。等价于单个数据实例的PDP 称为个体条件期望(ICE) 图(Goldstein 等⼈，2017[25])。ICE 图将实例对每个特征的预测依赖关系可视化，每个实例分别产⽣⼀条线，⽽部分依赖图中整体则只有⼀条线。PDP 是ICE 图的线的平均值。可以通过以下⽅式计算线(和⼀个实例) 的值：保持所有其他特征相', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 108, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法102同，通过⽤⽹格中的值替换特征的值创建该实例的变体并使⽤⿊盒模型对这些新创建的实例进⾏预测。结果是⼀组具有来⾃⽹格的特征值和相应预测的点。看个体期望⽽不是部分依赖有什么意义？部分依赖图可能会掩盖由交互作⽤创建的异构关系。PDP可以向你显⽰特征与预测之间的平均关系。仅当要为其计算PDP 的特征与其他特征之间的交互作⽤较弱时，这才有效。在交互的情况下，ICE 图将提供更多的见解。更正式的定义：在ICE 图，对{(x(i)S , x(i)C )}Ni=1 中每个实例，曲线ˆf(i)S是关于x(i)S 的，此时x(i)C 固定不变。5.2.1示例让我们返回宫颈癌数据集，看看每个实例的预测如何与特征“年龄” 相关联。我们将分析随机森林，随机森林⽤于预测给定风险因素的情况下⼥性患癌的概率。在部分依赖图中，我们看患癌症的概率在50 岁左右时增加，但这是否适⽤于数据集中的每个⼥性？ICE 图显⽰，对于⼤多数⼥性⽽⾔，年龄效应遵循50 岁时平均增加的趋势，但也有⼀些例外：对于少数⼏位在年轻时具有较⾼预测概率的⼥性，预测的癌症概率不会随年龄变化太⼤。图5.6. 按年龄划分的宫颈癌概率ICE 图。每条线代表⼀位⼥性。对于⼤多数⼥性来说，随着年龄的增长，预测癌症概率会增加。对于某些癌症预测概率⾼于0.4 的⼥性，该预测在较⾼年龄时变化不⼤。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 109, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法103下图显⽰了预测⾃⾏车租赁的ICE 图。基本的预测模型是随机森林。图5.7. 按天⽓状况预测的⾃⾏车租赁的ICE 图。可以观察到与部分依赖图相同的效果。所有曲线似乎都遵循相同的路线，因此没有明显的相互作⽤。这意味着PDP 已经很好地概括了显⽰的特征与预测的⾃⾏车数量之间的关系。中心化个体条件期望图(Centered ICE Plot)ICE 图存在⼀个问题：有时很难判断ICE 图在个体之间是否不同，因为它们从不同的预测开始。⼀种简单的解决⽅案是将曲线中⼼化于特征中的某个点，并仅显⽰到该点的预测差异。结果图称为中⼼化ICE 图(c-ICE)。将曲线锚定在特征的下端是⼀个不错的选择。新曲线定义为：ˆf(i)cent = ˆf(i) −1 ˆf(xa, x(i)C )其中1 是具有适当数量尺⼨(通常为⼀个或两个) 的1 的⽮量，ˆf 是拟合模型，xa 是锚定点。示例例如，绘制宫颈癌年龄ICE 图，并以观察到的最⼩年龄为中⼼：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 110, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法104图5.8. 按年龄预测癌症概率的中⼼化ICE 图。在14 岁时，线固定为0。与14 岁相⽐，⼤多数⼥性的预测⼀直保持不变，直到45 岁时预测的概率增加。中⼼化ICE 图使⽐较单个实例的曲线变得更加容易。如果我们不希望看到预测值的绝对变化，⽽是希望预测与特征范围的固定点相⽐的差异，这将很有⽤。让我们看⼀下⽤于⾃⾏车租赁预测的中⼼化ICE 图：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 111, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法105图5.9. 根据天⽓状况预测的⾃⾏车数量的中⼼化ICE 图。这些线表⽰与各个特征值处于其观测最⼩值时的预测值之间的差异。导数个体条件期望图(Derivative ICE Plot)另⼀种使从视觉上更容易发现异质性的⽅法是观察预测函数相对于特征的导数。⽣成的图称为导数ICE 图(d-ICE)。函数(或曲线) 的导数告诉你是否发⽣了变化以及它们发⽣的⽅向。利⽤导数ICE图，很容易发现特征值的范围，在该范围内，⿊盒模型预测对实例会发⽣变化(⾄少某些)。如果分析的特征xS 和其他特征xC 之间没有交互作⽤，那么预测函数可以表⽰为：ˆf(x) = ˆf(xS, xC) = g(xS) + h(xC),withδ ˆf(x)δxS= g′(xS)如果没有交互作⽤，则所有情况下的各个偏导数都应相同。如果它们不同，则是由于交互作⽤，并且在d-ICE 图中可见。除了显⽰预测函数的导数相对于S 中特征的的各个曲线外，显⽰导数的标准差还有助于突出显⽰S 中特征中具有估计异质性的区域。d-ICE 图需要很长时间才能计算出来，这是不切实际的。5.2.2优点与部分依赖图相⽐，个体条件期望曲线更直观。如果我们改变感兴趣的特征，则⼀条线代表⼀个实例的预测。与部分依赖图不同，ICE 曲线可以揭示异质关系。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 112, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1065.2.3缺点ICE 曲线只能有意义地显示一个特征，因为两个特征将需要绘制多个重叠曲⾯，你将在图中看不到任何内容。ICE 曲线与PDP ⾯临相同的问题：如果感兴趣的特征与其他特征相关联，则根据联合特征分布，线中的某些点可能是无效的数据点。如果绘制了许多ICE 曲线，该图可能会过于拥挤，你将看不到任何东西。解决⽅案：要么为线条添加⼀些透明度，要么仅绘制线条的样例。在ICE 绘图中，很难看到平均值。这有⼀个简单的解决⽅案：将个体条件期望曲线与部分依赖图结合起来。5.2.4软件和替代方法ICE 图在R iml 包(⽤于这些⽰例)，ICEbox[26] 和pdp 中实现。另⼀个做类似于ICE 的R 包是condvis。5.3累积局部效应图累积局部效应[27] (Accumulated Local Effects Plot) 描述了特征平均如何影响机器学习模型的预测。ALE 图是部分依赖图(PDP) 的更快、更⽆偏的替代⽅法。我建议先阅读有关部分依赖图的⼩节，因为它们更易于理解，并且两种⽅法都有相同的⽬标：两者都描述了特征如何平均影响预测。在下⾯的内容中，我想让你相信，当特征相关时，部分依赖图存在严重问题。5.3.1动机和直觉如果机器学习模型的特征相关，则部分依赖图将不可信。计算与其他特征强相关的特征的部分依赖图涉及对在实际中不太可能出现的⼈⼯数据实例的平均预测。这会极⼤地影响估计的特征效应。想象⼀下，为机器学习模型计算部分依赖图，该模型根据房间数量和居住⾯积来预测房屋的价格。我们对居住⾯积对预测值的影响感兴趣。提醒⼀下，部分依赖图的配⽅如下：1) 选择特征。2) 定义⽹格。3) 每个⽹格值：a) ⽤⽹格值替换特征，b) 平均预测。4) 绘制曲线。为了计算PDP 的第⼀个⽹格值——例如30 平⽅⽶——我们将所有实例的居住⾯积替换为30 平⽅⽶，即使是拥有10 个房间的房屋也是如此。在我看来，这样⽣成的房屋就很不寻常。部分依赖图将这些不切实际的房屋包', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 113, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法107括在特征效应估计中，并假装⼀切都很好。下图说明了两个相关特征以及部分依赖图⽅法对不太可能发⽣的实例的预测取平均的结果。图5.10.⾼度相关的特征x1 和x2。为了计算x1 在0.75 时的特征效应，PDP 错误地假设x2 在x1 = 0.75 处的分布与x2 的边际分布相同(垂直线)，则⽤0.75 替换所有实例的x1。这导致不太可能的x1 和x2 的组合(例如x1 = 0.75 时x2 = 0.2)，PDP 将其⽤于计算平均效应。(注：在本书中特征效应与特征影响描述相同概念)我们如何做才能得到尊重特征相关性的特征效应估计？我们可以对特征的条件分布求平均值，即在x1 的⽹格值处，对x1 值类似的实例的预测求平均值。使⽤条件分布来计算特征效应的解决⽅案称为边际图(Marginal Plot) 或M 图(名称易混淆，因为它们基于条件⽽不是边际分布)。等⼀下，我不是说谈论ALE 图的吗？M 图不是我们正在寻找的解决⽅案。为什么M 图⽆法解决我们的问题？如果我们平均30 平⽅⽶左右的所有房屋的预测，我们会估计居住⾯积和房间数量的联合效应，因为它们的相关性。假设居住⾯积对房屋的预测值没有影响，⽽只有房间数才有影响。M 图将仍然显⽰出居住⾯积会增加预测值，因为房间数会随居住⾯积的增加⽽增加。下图显⽰了两个相关特征，M 图如何⼯作。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 114, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法108图5.11. ⾼度相关的特征x1 和x2。M 图在条件分布上取平均值。此处x2 在x1 = 0.75 处的条件分布。平均局部预测会导致混合两种特征的特征效应。M 图避免了对不太可能出现的数据实例的平均预测，但是它们将特征的效应与所有相关特征的效应混合在⼀起。ALE 图通过基于特征的条件分布来计算预测差异⽽不是平均值，从⽽解决了这⼀问题。对于30 平⽅⽶的居住⾯积的效应(或称影响)，ALE ⽅法使⽤所有⼤约30 平⽅⽶的房屋，得到(假装) 这些房屋为31 平⽅⽶的预测减去(假装) 为29 平⽅⽶的预测。这给了我们居住区域的纯粹效应，并且没有将效应与相关特征的效应混合在⼀起。差异的使⽤会阻⽌其他特征的影响。下图直观地说明了如何计算ALE 图。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 115, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法109图5.12. 与x2 相关的特征x1 的ALE 计算。⾸先，我们将特征划分为间隔(垂直线)。对于间隔中的数据实例(点)，当我们⽤间隔的上限和下限(⽔平线) 替换特征时，我们计算预测值的差异。这些差异随后被累积并中⼼化，从⽽形成ALE 曲线。总结每种类型的图(PDP，M，ALE) 如何在某个⽹格值v 下计算xj 的特征效应：部分相关图：“让我展⽰对于特征xj 当每个数据实例具有值v 时模型平均预测的结果。我忽略了值v 是否对所有数据实例都有意义。”M 图：“让我告诉你模型对于特征xj 的值接近v 的数据实例平均预测什么。效应可能是由于该特征，也由于相关的特征。”ALE 图：“让我向你展⽰该窗⼜中数据实例的模型预测如何在围绕v 的特征xj 的⼀个⼩的“窗⼜”中变化。”5.3.2理论PD，M 和ALE 图在数学上有何不同？这三种⽅法的共同点是将复杂的预测函数f 简化为仅依赖⼀个(或两个) 特征的函数。这三种⽅法都通过平均其他特征的效应来简化特征，但是在计算预测平均值或预测差异以及是否对边际或条件分布进⾏平均⽅⾯有所不同。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 116, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法110部分依赖图对边际分布的预测进⾏平均。ˆfxS,PDP (xS) = EXC[ˆf(xS, XC)]=∫xCˆf(xS, xC)P(xC)dxC这是预测函数f 的值，在特征值xS 处，对xC 中的所有特征取平均值。平均是指计算集合C 中特征的边际期望E，它是通过概率分布加权的预测的积分。听起来很奇怪，但是要计算边际分布上的期望值，我们只需获取所有数据实例，强制它们对集合S 中的特征具有特定的⽹格值，并平均此操作下数据集的预测值。这个过程确保我们对特征的边际分布进⾏平均。M 图对条件分布的预测平均。ˆfxS,M(xS) = EXC|XS[ˆf(XS, XC)|XS = xs]=∫xCˆf(xS, xC)P(xC|xS)dxC与PDP 相⽐唯⼀改变的是，我们根据感兴趣的特征的每个⽹格值来平均预测，⽽不是假设每个⽹格值的边际分布。在实践中，这意味着我们必须定义⼀个邻域，例如，为了计算30 平⽅⽶对房屋预测值的影响，我们可以对28 到32 平⽅⽶之间的所有房屋的预测值求平均。ALE 图对预测的变化进⾏平均，然后将其累积在⽹格上(稍后会在计算中提供更多信息)。ˆfxS,ALE(xS) =∫xSz0,1EXC|XS[ˆfS(Xs, Xc)|XS = zS]dzS −constant=∫xSz0,1∫xCˆfS(zs, xc)P(xC|zS)dxCdzS −constant该公式揭⽰了与M 图的三个差异。⾸先，我们平均预测的变化，⽽不是预测本⾝。将该变化定义为梯度(但稍后，对于实际计算，将替换为某个间隔内的预测差异)。ˆfS(xs, xc) = δ ˆf(xS, xC)δxS第⼆个差异是z 上的积分。我们在集合S 中的特征范围内累积局部梯度，这给了我们特征对预测的影响。对于实际计算，将z 替换为间隔⽹格，在该间隔上我们计算预测的变化。ALE ⽅法不是直接平均预测，⽽是计算以特征S 为条件的预测差异，并对特征S 上的导数进⾏积分以估计效应。好吧，这听起来很愚蠢。导数和积分通常会互相抵消，例如先减去，然后再加上相同的数字。为什么在这⾥有意义？导数(或区间差) 隔离感兴趣特征的效应并阻⽌相关特征的效应。ALE 图与M 曲线的第三个差异是我们从结果中减去常数。此步骤将ALE 图中⼼化，以便数据的平均效应为零。仍然存在⼀个问题：并⾮所有模型都带有梯度，例如随机森林没有梯度。但是，你将看到，实际的计算没有梯度，并且使⽤了间隔。让我们深⼊研究ALE 图的估计。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 117, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1115.3.3估计⾸先，我将描述如何为单个数值特征估计ALE 图，然后是两个数值特征以及单个分类特征。为了估计局部效应，我们将特征划分为许多区间并计算预测值之间的差异。此过程近似梯度，也适⽤于没有梯度的模型。⾸先，我们估计⾮中⼼化效应：ˆ˜fj,ALE(x) =kj(x)∑k=11nj(k)∑i:x(i)j ∈Nj(k)[f(zk,j, x(i)\\\\j ) −f(zk−1,j, x(i)\\\\j )]让我们把这个公式分解，从右边开始。这个名为累积局部效应很好地反映了这个公式的所有组成部分。ALE ⽅法的核⼼是计算预测中的差异，因此我们⽤⽹格值z 替换感兴趣的特征。预测中的差异是特征在特定间隔内对单个实例的效应。右边的总和将间隔(作为邻域Nj(k) 显⽰在公式中) 内所有实例的效应相加。我们将该总和除以该间隔中的实例数，以获得此间隔的预测平均差。间隔中的此平均值由名称ALE 中的术语“局部” 覆盖。左边的和符号表⽰我们在所有间隔内累积平均效应。例如，位于第三个间隔中的特征值的(⾮中⼼化) ALE 是第⼀个间隔、第⼆个间隔和第三个间隔的效应之和。ALE 中的“ 累积” ⼀词反映了这⼀点。将效应中⼼化，因此平均效应为零。ˆfj,ALE(x) = ˆ˜fj,ALE(x) −1nn∑i=1ˆ˜fj,ALE(x(i)j )与数据的平均预测值相⽐，ALE 值可以解释为某⼀特定值下特征的主要效应。例如，当第j 个特征值为3 时(xj = 3)，ALE 估计值为-2，则预测值⽐平均预测值低2。特征分布的分位数⽤作定义间隔的⽹格。使⽤分位数可确保每个间隔中有相同数量的数据实例。分位数的缺点是间隔的长度可能⾮常不同。如果感兴趣的特征⾮常偏斜(例如，许多低值⽽只有少数⾮常⾼的值)，则这可能会导致某些ALE 图出现异常。用于两个特征交互作用的ALE 图ALE 图还可以显⽰两个特征的交互作⽤。计算原理与单个特征相同，但是我们使⽤矩形单元⽽不是间隔，因为我们必须在⼆维中累积效应。除了调整总体平均效应外，我们还调整两个特征的主要效应。这意味着针对两个特征的ALE 会估计⼆阶效应，其中不包括特征的主要效应。换句话说，两个特征的ALE 仅显⽰两个特征的附加交互效应。我为你省却了2D ALE 图的公式，因为它们冗长，读起来很不愉快。如果你对这个计算感兴趣，请参考论⽂的公式(13)—(16)。我将依靠可视化来发展⼆阶ALE 计算的直觉。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 118, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法112图5.13. 2D-ALE 的计算。我们在这两个特征上放置⼀个⽹格。在每个⽹格单元中，我们计算内部所有实例的⼆阶差。我们⾸先⽤单元格⾓落的值替换x1 和x2 的值。如果a、b、c 和d 代表“⾓落点”——操作实例的预测(如图中所⽰)，则⼆阶差为(d-c) - (b-a)。每个单元的平均⼆阶差在⽹格上累积并中⼼化。在上图中，由于相关性，许多单元为空。在ALE 图中，可以使⽤灰⾊或深⾊⽅框将其可视化。或者，你可以⽤最接近的⾮空单元格的ALE 估计值代替空单元格的ALE 估计值。由于两个特征的ALE 估计仅显⽰特征的⼆阶效应，因此需要特别注意解释。⼆阶效应是在考虑了特征的主要效应之后，特征的附加的交互效应。假设两个特征不交互，但是每个特征对预测结果具有线性效应。在每个特征的⼀维ALE 图中，我们将看到⼀条直线作为估计的ALE 曲线。但是，当我们绘制2D ALE 估计时，它们应该接近于零，因为⼆阶效应只是附加的交互效应。ALE 图和PD图在这⽅⾯有所不同：PDP 始终显⽰总效应，ALE 图显⽰⼀阶或⼆阶效应。这些是不依赖于基础数学的设计决策。你可以减去部分依赖图中的低阶效应得到纯的主要效应或⼆阶效应，或者，你可以通过避免减去低阶效应得到总ALE 图的估计。累积局部效应也可以针对任意更⾼的阶数(三个或更多特征的交互作⽤) 进⾏计算，但是正如PDP⼀节所述，最多只有两个特征才有意义，因为更⾼的交互⽆法可视化甚⾄⽆法有意义地解释。ALE 用于分类特征累积局部效应⽅法(根据定义) 需要特征值具有顺序，因为该⽅法会沿特定⽅向累积效应。分类特征没有任何⾃然顺序。为了计算分类特征的ALE 图，我们必须以某种⽅式创建或找到⼀个顺序。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 119, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法113类别的顺序会影响累积局部效应的计算和解释。⼀种解决⽅案是基于其他特征根据类别的相似性对类别进⾏排序。两类之间的距离是每个特征的距离之和。逐特征距离⽐较了两个类别中的累积分布，也称为Kolmogorov-Smirnov 距离(对于数值特征) 或相对频率表(对于分类特征)。⼀旦有了所有类别之间的距离，便可以使⽤多维缩放将距离矩阵压缩为⼀维距离度量。这为我们提供了基于相似度的类别顺序。为了使这⼀点更清楚⼀些，下⾯是⼀个⽰例：让我们假设我们有两个分类特征“季节” 和“天⽓”，以及⼀个数值特征“温度”。对于第⼀个分类特征(季节)，我们要计算ALE。该特征类别为“春季”，“夏季”，“秋季”，“冬季”。我们开始计算“春天” 和“夏天” 类别之间的距离。距离是温度特征和天⽓特征的距离总和。对于温度，我们采⽤“春季” 季节的所有实例，计算经验累积分布函数，对“夏季” 季节的实例进⾏相同的操作，并使⽤Kolmogorov-Smirnov 统计量度它们的距离。对于天⽓特征，我们为所有“春季” 实例计算每种天⽓类型的概率，对“夏季” 实例执⾏相同的操作，并求出概率分布中的绝对距离。如果“春季” 和“夏季” 的温度和天⽓有很⼤不同，则总类别距离很⼤。我们对其他季节对重复此过程，并通过多维缩放将所得距离矩阵缩⼩为⼀维。5.3.4示例让我们看⼀下ALE 图的实现。我构建了⼀个场景，其中部分依赖图失效了。该场景包括⼀个预测模型和两个⾼度相关的特征。预测模型主要是线性回归模型，但是在这两个我们从未观察到的特征的组合中做了⼀些奇怪的事情。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 120, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法114图5.14.两个特征和预测结果。模型预测两个特征的总和(阴影背景)，但如果x1 ⼤于0.7 且x2⼩于0.3，则模型始终预测2。该区域远离数据分布(点云)，并且不会影响模型的性能，也不应影响其解释。这是现实的、相关的场景。训练模型时，学习算法会将现有训练数据实例的损失降⾄最低。奇怪的事情可能发⽣在训练数据的分布之外，因为该模型不会因为在这些区域做奇怪的事情⽽受到惩罚。远离数据分布称为外推法(Extrapolation)，也可⽤于欺骗机器学习模型，在下⼀章对抗样本⼀节中对此进⾏了介绍。在我们的⼩⽰例中，看到部分依赖图与ALE 图相⽐如何表现。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 121, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法115图5.15.⽤PDP (上排) 和ALE (下排) 计算的特征效应⽐较。PDP 估计受数据分布之外模型的奇怪⾏为(图中的陡峭跳跃) 影响。ALE 图正确识别了机器学习模型在特征和预测之间具有线性关系，⽽忽略了没有数据的区域。但是，看到我们的模型在x1 > 0.7 和x2 <0.3 时表现得很奇怪，这不是很有趣吗？是的，其实也不是。由于这些数据实例在物理上可能是不可能的，或者⾄少是极不可能的，因此通常⽆需考虑这些实例。但是，如果你怀疑测试分布可能略有不同，并且某些实例实际上在该范围内，那么将这⼀区域包括在特征效应的计算中将很有趣。必须有意识地决定是否包括我们尚未观察到数据的区域，并且它不应该是选择⽅法(如PDP) 的副作⽤。如果你怀疑该模型以后会与分布不同的数据⼀起使⽤，我建议使⽤ALE 图并模拟你期望的数据分布。转到⼀个真实的数据集，让我们根据天⽓和天数来预测所租⾃⾏车的数量，并检查ALE 图是否确实如预期那样有效。我们训练回归树以预测给定⽇期的⾃⾏车出租数量，并使⽤ALE 图分析温度，相对湿度和风速如何影响预测。让我们看⼀下ALE 图是怎么说的：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 122, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法116图5.16. 基于温度，湿度和风速的⾃⾏车预测模型的ALE 图。温度对预测有很强的影响。平均预测随着温度升⾼⽽上升，但到25 摄⽒度以上再次下降。湿度具有负效应(负⾯影响)：当湿度超过60% 时，相对湿度越⾼，预测值越低。风速对预测的影响不⼤。让我们看⼀下温度、湿度和风速与所有其他特征之间的相关性。由于数据还包含分类特征，因此我们不能仅使⽤Pearson 相关系数，该系数仅在两个特征均为数值时才有效。相反，我训练了⼀个线性模型，基于其他特征(之⼀) 作为输⼊来预测温度。然后，我测量线性模型中其他特征所解释的⽅差并取平⽅根。如果另⼀个特征是数值的，则结果等于标准Pearson 相关系数的绝对值。但是，这种基于模型的“解释⽅差” ⽅法(也称为ANOVA，“⽅差分析”)，即使其他特征是分类特征，仍然有效。“解释⽅差” 的度量始终位于0 (⽆关联) 和1 (可以从其他特征完美预测温度) 之间。我们计算出所有其他特征的温度，湿度和风速的解释⽅差。解释⽅差(相关性) 越⾼，PD 图的(潜在) 问题就越多。下图显⽰了天⽓特征与其他特征之间的关联程度。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 123, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法117图5.17.当我们使⽤(例如温度来预测并以季节为特征的) 线性模型进⾏训练时，温度、湿度和风速与所有特征之间的相关强度以解释⽅差量表⽰。对于温度，我们毫不奇怪的观察到与季节和⽉份⾼度相关。湿度与天⽓状况相关。这种相关性分析表明，我们可能会遇到部分依赖图的问题，尤其是对于温度特征⽽⾔。好吧，⾃⼰看看：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 124, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法118图5.18.温度、湿度和风速的PDP。与ALE 图相⽐，PDP 显⽰在⾼温或⾼湿度下的⾃⾏车预测数量减少较⼩。PDP 使⽤所有数据实例来计算⾼温的影响，即使它们是(例如) 季节为“冬季” 的实例。ALE 图更可靠。接下来，让我们看到针对分类特征的ALE 图。⽉份是⼀个分类特征，我们要分析它对⾃⾏车预测数量的影响。可以说，⽉份已经有⼀定的顺序(从1 ⽉到12 ⽉)，但是让我们尝试看看如果我们先按相似性对类别重新排序然后计算影响，会发⽣什么。根据其他特征(例如温度或是否假期)，根据每⽉的相似性对⽉份进⾏排序。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 125, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法119图5.19.分类特征⽉份的ALE 图。基于(按⽉的) 其他特征的分布情况，根据彼此之间的相似性对⽉份进⾏排序。与其他⽉份相⽐，我们观察到1 ⽉，3 ⽉和4 ⽉，尤其是12 ⽉和11 ⽉，对预测的租赁⾃⾏车数量影响较⼩。由于许多特征与天⽓有关，因此⽉份的顺序强烈反映了⽉份之间天⽓的相似程度。所有较冷的⽉份在左侧(2 ⽉⾄4 ⽉)，在较暖的⽉份在右侧(10 ⽉⾄8 ⽉)。请记住，相似性计算中还包括了⾮天⽓特征，例如，计算⽉份之间相似度时假期的相对频率与温度具有相同的权重。接下来，我们考虑湿度和温度对⾃⾏车预测数量的⼆阶效应。请记住，⼆阶效应是两个特征的附加交互效应，并不包括主要效应。举例来说，这意味着你不会在⼆阶ALE 图中看到⾼湿度导致平均预测⾃⾏车数量减少的主要效应。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 126, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法120图5.20. 湿度和温度对所租⾃⾏车预测数量的⼆阶效应的ALE 图。当已经考虑到主要效应时，浅阴影表⽰⾼于平均⽔平，深阴影表⽰低于平均⽔平的预测。该图揭⽰了温度和湿度之间的交互作⽤：炎热和潮湿的天⽓增加了预测。在寒冷和潮湿的天⽓中，还会对预测的⾃⾏车数量产⽣负效应。请记住，湿度和温度的主要效应都说明在⾮常炎热和潮湿的天⽓中，预测的⾃⾏车数量会减少。因此，在炎热和潮湿的天⽓中，温度和湿度的综合效应不是主要效应的总和，⽽是⼤于总和。为了强调纯⼆阶效应(你刚刚看到的2D ALE 图) 和总效应之间的区别，让我们看⼀下部分依赖图。PDP显⽰了总体效应，它结合了平均预测，两个主要效应和⼆阶效应(交互作⽤)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 127, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法121图5.21. 温度和湿度对⾃⾏车预测数量的总体效应的PDP。与仅显⽰交互作⽤的2D ALE 图相反，该图结合了每个特征的主要效应及其交互效应。如果你仅对交互感兴趣，你应该考虑⼆阶效应，因为总效应会将主要效应混合到图中。但是，如果你想了解特征的组合效应，则应查看总体效应(PDP 显⽰)。例如，如果你想知道30 摄⽒度和80%湿度下的⾃⾏车预期数量，则可以直接从2D PDP 中读取它。如果要从ALE 图中读取相同内容，则需要查看以下三个图：温度，湿度以及温度+ 湿度的ALE 图，还需要了解总体平均预测。在两个特征没有交互作⽤的情况下，这两个特征的总体效应图可能会产⽣误导，因为它可能显⽰了复杂的情况，表明存在某些交互作⽤，但这仅仅是两个主要作⽤的产物。⼆阶效应将⽴即表明没有交互作⽤。现在描述⾃⾏车够多了，让我们开始分类任务。我们训练⼀个随机森林以根据风险因素预测宫颈癌的概率。我们将两个特征的累积局部效应可视化：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 128, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法122图5.22. ALE 图表显⽰年龄和服⽤激素避孕药的时间对宫颈癌的预测概率的影响。对于年龄特征，ALE 图显⽰，直到40 岁，预测的癌症概率平均较低，此后则增加。服⽤激素避孕药的年数在8 年后与预测癌症风险⾼相关。接下来，我们看⼀下怀孕次数和年龄之间的交互作⽤。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 129, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法123图5.23.怀孕次数和年龄的⼆阶效应的ALE 图。该图的解释尚⽆定论，表明似乎过拟合。例如，该图显⽰了18-20 岁和3 次以上的怀孕(癌症概率增加了5 个百分点) 的奇怪模型⾏为。在这组年龄和怀孕次数的数据中，⼥性并不多(实际数据显⽰为点)，因此在训练过程中不会因对这些⼥性犯错⽽对该模型进⾏严厉的惩罚。5.3.5优点ALE 图是无偏的，这意味着在特征相关时它们仍然有效。在这种情况下，部分依赖图将失败，因为它们会边缘化不太可能甚⾄物理上不可能的特征值组合。ALE 绘图的计算速度比PDP 更快，并且可以使⽤O(n) 进⾏缩放，因为可能的最⼤间隔数是实例数(每个实例为⼀个间隔)。PDP 需要⽹格点估计数量的n 倍。对于20 个⽹格点，PDP 所需的预测⽐最坏情况下的ALE 图多20 倍，其中使⽤的间隔与实例相同。ALE 图的解释很清楚：在给定值的条件下，可以从ALE 图中读取更改特征对预测的相对影响。ALE 图以0 为中心。这使它们的解释更好，因为ALE 曲线每个点的值都是与平均预测之差。2DALE 绘图仅显示交互作用：如果两个特征不交互，则图不显⽰任何内容。总⽽⾔之，在⼤多数情况下，我宁愿使用ALE 图而不是PDP 图，因为特征通常在某种程度上相关。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 130, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1245.3.6缺点ALE 图可能会变得有些不稳定(许多⼩起伏)，间隔很多。在这种情况下，减少间隔数可以使估计更稳定，但也可以使预测模型的实际复杂度消除和隐藏。没有完美的解决方案来设置间隔的数量。如果数量太⼩，则ALE 图可能不太准确。如果数量太⼤，曲线可能会变得不稳定。与PDP 不同，ALE 图不附带ICE 曲线。对于PDP 来说，ICE 曲线是很好的，因为它们可以揭⽰特征效应的异质性，这意味着对于数据⼦集⽽⾔，特征的效应看起来有所不同。对于ALE 图，你只能检查每个间隔实例之间的效应是否不同，但是每个间隔具有不同的实例，因此它与ICE 曲线不同。二阶ALE 估计在整个特征空间中具有不同的稳定性，这是不以任何方式可视化的。其原因是，对⼀个单元中局部效应的每次估计都使⽤不同数量的数据实例。结果，所有估计都具有不同的准确性(但它们仍然是最好的估计)。对于主要效应ALE 图，该问题存在于不太严重的版本中。由于使⽤了分位数作为⽹格，因此在所有间隔中实例的数量都是相同的，但是在某些区域中会存在许多短间隔，并且ALE 曲线将包含更多估计值。但是对于较长的间隔(可能占整个曲线的很⼤⼀部分) ⽽⾔，实例相对较少。例如，这发⽣在⾼龄的宫颈癌预测ALE 图中。二阶效应图解释起来有点烦人，因为你始终必须牢记主要效应。将热图解读为这两个特征的总体效应是很有诱惑⼒的，但这⾥只是附加的交互效应。纯粹的⼆阶效应对于发现和探索交互⾮常有趣，但是对于解释效应看起来，我认为将主要效应整合到图中更有意义。与部分依赖图相⽐，ALE 图的实现更加复杂且不直观。即使ALE 图在相关特征的情况下没有偏差，但当特征强相关时，解释仍然困难。因为如果它们之间有⾮常强的相关性，那么分析同时改变两个特征(⽽不是孤⽴地更改) 的效应才有意义。这个缺点不是特定于ALE 图，⽽是⼀个具有强相关特征的普遍问题。如果特征不相关，计算时间也不成问题，则PDP 稍好，因为它们易于理解并且可以与ICE 曲线⼀起绘制。缺点列表已经很长了，但是不要被我使⽤的词的数量所迷惑：作为⼀个经验法则——使⽤ALE ⽽不是PDP。5.3.7实现和替代方法我是否提到过部分依赖图和个体条件期望曲线是替代⽅法？=)据我所知，ALE 图⽬前仅在R 中实现，⼀个是由发明⼈本⼈在R ALEPlot 包中，另⼀个是在iml包中。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 131, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1255.4特征交互特征交互(Feature Interaction) ：当特征在预测模型中交互时，预测不能表⽰为特征效应的总和，因为⼀个特征的效应取决于另⼀特征的值。亚⾥⼠多德的“整体⼤于部分之和” 适⽤于存在交互作⽤的情况。5.4.1特征交互如果机器学习模型基于两个特征进⾏预测，则可以将预测分解为四项：常量项，第⼀个特征项，第⼆个特征项以及两个特征间的交互项。两个特征间的交互项是在考虑单⼀特征效应后通过改变特征⽽发⽣的预测变化。例如，模型使⽤房屋⼤⼩(⼤或⼩) 和位置(好或坏) 作为特征来预测房屋的价格，这产⽣了四个可能的预测：LocationSizePredictiongoodbig300,000goodsmall200,000badbig250,000badsmall150,000我们将模型预测分解为以下⼏部分：⼀个常量项(150,000)，⾯积⼤⼩的特征效应(如果为⼤则为+100,000；如果为⼩则为+0) 以及位置特征的特征效应(如果为好则为+50,000；如果不好则为+0)。这种分解完全解释了模型预测。没有交互作⽤，因为模型预测是针对⼤⼩和位置的单⼀特征效应的总和。当你将⼩房⼦变⼤时，⽆论位置在哪⾥，预测值始终会增加100,000。⽽且，⽆论⼤⼩，好坏位置的预测差异为50,000。现在让我们看⼀个带有交互的⽰例：LocationSizePredictiongoodbig400,000goodsmall200,000badbig250,000badsmall150,000我们将预测表分解为以下⼏部分：⼀个常量项(150,000)，⾯积⼤⼩特征的效应(如果为⼤则为', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 132, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法126+100,000；如果为⼩则为+0) 和位置特征的效应(如果为好则为+50,000；如果为坏则为+0)。对于此表，我们需要⼀个⽤于交互的附加项：如果房屋较⼤且位置好，则+100,000。这是⼤⼩和位置之间的交互作⽤，因为在这种情况下，⼤⼩房⼦之间的预测差异取决于位置。估计交互强度的⼀种⽅法是衡量预测的变化在多⼤程度上取决于特征的交互作⽤。这项衡量称为H统计量，由Friedman 和Popescu (2008)[21] 引⼊。5.4.2理论：弗里德曼的H 统计量我们将处理两种情况：⾸先，采⽤双向交互度量，它告诉我们模型中的两个特征是否交互以及在何种程度上交互；其次，是⼀个总体交互度量，它告诉我们某个特征在模型中是否与所有其他特征发⽣交互以及在何种程度上的交互。从理论上讲，可以测量任意数量的特征之间的任意交互，但这两个是最感兴趣的情况。如果两个特征不交互，我们可以按如下⽅式分解部分依赖函数(假设部分依赖函数以零为中⼼) ：PDjk(xj, xk) = PDj(xj) + PDk(xk)其中PDjk(xj, xk) 是两个特征的双向部分依赖函数，⽽PDj(xj) 和PDk(xk) 是单个特征的部分依赖函数。同样，如果⼀个特征与任何其他特征都没有交互，我们可以将预测函数ˆf(x) 表⽰为部分依赖函数的总和，其中第⼀个求和项仅依赖于j，第⼆个求和项依赖于除j 以外的所有其他特征：ˆf(x) = PDj(xj) + PD−j(x−j)其中PD−j(x−j) 是依赖于除第j 个特征以外的所有特征的部分依赖函数。这种分解表⽰部分依赖(或完全预测) 函数，⽽没有交互作⽤(在特征j 和k 之间，或者分别在j与所有其他特征之间)。在下⼀步中，我们测量观察到的部分依赖函数与没有交互作⽤的已分解部分依赖函数之间的差异。我们计算部分依赖(以度量两个特征之间的交互作⽤) 或整个函数(以度量⼀个特征与所有其他特征之间的交互作⽤) 输出的⽅差。由交互作⽤解释的⽅差量(观察到的和没有交互作⽤PD 之间的差异) ⽤作交互强度统计量。如果完全没有交互，则统计量为0；如果⽤部分依赖函数之和解释了PDjk 或ˆf 的所有⽅差，则统计值为1。两个特征之间的交互统计为1 表⽰每个PD 函数都是常数，并且对预测的效应仅来⾃交互。在数学上，Friedman 和Popescu 为特征j 和k 之间的交互作⽤提出的H 统计量为：H2jk =n∑i=1[PDjk(x(i)j , x(i)k ) −PDj(x(i)j ) −PDk(x(i)k )]2/n∑i=1PD2jk(x(i)j , x(i)k )', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 133, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法127这同样适⽤于度量特征j 是否与任何其他特征交互：H2j =n∑i=1[ˆf(x(i)) −PDj(x(i)j ) −PD−j(x(i)−j)]2/n∑i=1ˆf2(x(i))H 统计量的评估成本很⾼，因为它会在所有数据点上进⾏迭代，并且必须在每个点处评估部分依赖，⽽这又需要对所有n 个数据点进⾏。在最坏的情况下，我们需要2n2 调⽤机器学习模型预测函数来计算双向H 统计量(j vs. k)，并需要3n2 以获得总体H 统计量(j vs. all)。为了加快计算速度，我们可以从n 个数据点进⾏采样。这有增加部分依赖估计⽅差的缺点，使得H 统计量不稳定。因此，如果使⽤采样来减少计算负担，请确保采样⾜够的数据点。Friedman 和Popescu 还提出了⼀个检验统计量，以评估H 统计量与零是否有显著差异。零假设是没有交互。要在零假设下⽣成交互统计量，你必须能够调整模型，以使其在特征j 和k 或所有其他特征之间不具有交互作⽤。这不可能适⽤于所有类型的模型。因此，该检验是特定于模型的，⽽不是与模型⽆关的，故不在此介绍。如果预测是概率，则交互强度统计也可以应⽤于分类设置。5.4.3示例让我们看看在实践中交互特征是什么样的！我们在⽀持向量机中测量特征的交互强度，⽀持向量机可根据天⽓和⽇历特征预测租赁⾃⾏车的数量。下图显⽰了特征交互作⽤H 统计量：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 134, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法128图5.24.预测⾃⾏车租赁的⽀持向量机的每个特征与所有其他特征的交互强度(H 统计量)。总体⽽⾔，特征之间的交互作⽤⾮常弱(低于每个特征解释的⽅差的10%)。在下⼀个⽰例中，我们计算分类问题的交互统计量。给定⼀些风险因素，我们分析了经过训练可预测宫颈癌的随机森林中特征之间的交互作⽤。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 135, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法129图5.25.⽤于预测宫颈癌概率的随机森林的每个特征与所有其他特征的交互强度(H 统计量)。激素避孕药的使⽤年限与其他特征的相对最⾼的交互作⽤，其次是怀孕次数。在查看了每个特征与所有其他特征的特征交互之后，我们可以选择其中⼀个特征，然后更深⼊地研究所选特征与其他特征之间的所有双向交互。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 136, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法130图5.26.怀孕次数与其他特征之间的双向交互强度(H 统计量)。怀孕次数和年龄之间有很强的交互作⽤。5.4.4优点交互作⽤H 统计量通过部分依赖分解具有理论基础。H 统计量具有有意义的解释：交互作⽤定义为由交互作⽤解释的⽅差份额。由于统计信息是无量纲的，并且总是在0 和1 之间，因此它在各个特征甚⾄模型之间都具有可⽐性。统计信息会检测各种类型的交互，⽆论它们的特殊形式如何。使⽤H 统计量，还可以分析任意更高阶的交互作用，例如3 个或更多特征之间的交互作⽤强度。5.4.5缺点你会注意到的第⼀件事：交互H 统计量需要花费很长时间进⾏计算，因为它的计算量很大。该计算涉及估计边际分布。如果我们不使⽤所有数据点，则这些估计值也存在一定差异。这意味着，当我们对点进⾏采样时，估算值也因运⾏⽽异，结果可能会不稳定。我建议重复⼏次H 统计量计算，以查看是否有⾜够的数据来获得稳定的结果。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 137, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法131尚不清楚交互作⽤是否显着⼤于0。我们将需要进⾏统计检验，但该检验在与模型无关的版本中尚不可用。关于检验问题，很难说H 统计量何时⾜够⼤以⾄于我们认为交互“强”。另外，H 统计量可能⼤于1，这使得解释变得困难。H 统计量告诉我们交互的强度，但没有告诉我们交互的样⼦。这就是部分依赖图的⽤途。⼀个有意义的⼯作流程是测量交互强度，然后为你感兴趣的交互创建2D 部分依赖图。如果输⼊是像素，则⽆法有效使⽤H 统计量。因此该技术对图像分类器没有⽤。交互统计是在假设我们可以独⽴地对特征进⾏随机排序的情况下起作⽤。如果特征之间具有很强的相关性，则将违反该假设，并且我们会结合现实中极不可能发生的特征组合。这就是部分依赖图存在的相同问题。你通常⽆法说出它是否导致⾼估或低估。有时结果是奇怪的，对于⼩的模拟不能产生预期的结果。但这更多是个奇闻轶事。5.4.6实现对于本书中的⽰例，我使⽤了R iml 包，该包在CRAN 上可⽤，⽽开发版本在Github 上可⽤。还有其他⼀些实现，这些实现着眼于特定模型：R 预先实现RuleFit 和H-statistic。R gbm 包实现了梯度提升模型和H 统计量。5.4.7替代方法H 统计量不是衡量交互作⽤的唯⼀⽅法：Hooker (2004)[28] 提出的变量交互⽹络(VIN) 是⼀种将预测函数分解为主要效应和特征交互的⽅法。然后将特征之间的交互可视化为⽹络。不幸的是，⽬前还没有可⽤的软件。Greenwell 等⼈(2018)[29] 基于部分依赖性的特征交互测量了两个特征之间的交互。这种⽅法测量⼀个特征在另⼀个特征的不同固定点上的特征重要性(定义为部分依赖函数的⽅差)。如果⽅差⾼，则特征就交互；如果⽅差为零，则它们就不交互。相应的R vip 包在Github 上可以获得。该软件包还涵盖了部分依赖图和特征重要性。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 138, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1325.5置换特征重要性置换特征重要性(Permutation Feature Importance) 衡量了我们对特征值进⾏置换后模型预测误差的增加，这打破了特征与真实结果之间的关系。5.5.1理论这个概念⾮常简单：我们通过置换特征后计算模型的预测误差的增加来衡量特征的重要性。如果将特征的值改变会增加模型误差，则该特征“重要”，因为在这种情况下，模型依赖于特征进⾏预测。如果将特征的值改变⽽使模型误差保持不变，则特征“不重要”，因为在这种情况下，模型会忽略预测的特征。置换特征重要性度量是由Breiman (2001)[30] 引⼊的，⽤于随机森林。基于这个想法，Fisher、Rudin 和Dominici (2018)[31] 提出了特征重要性的模型⽆关版本，并将其称为模型依赖(Model Reliance)。他们还介绍了有关特征重要性的更先进的想法，例如⼀个(特定于模型的)版本是考虑到⽤很多预测模型可以很好地预测数据。他们的论⽂值得⼀读。基于Fisher，Rudin 和Dominici (2018) 的置换特征重要性算法：置换特征重要性训练模型f，特征矩阵X，⽬标向量y，误差度量L(y, f) 估计原始模型误差eorig =L(y, f(X)) (例如均⽅误差) 特征j ←1 to p 通过置换数据X 中的特征j ⽣成特征矩阵Xperm。这破坏了特征j 与真实结果y 之间的关联基于置换数据的预测，估计误差eperm = L(Y, f(Xperm) 计算置换特征重要性FIj = eperm/eorig。或者，可以使⽤差异：FIj = eperm −eorig 按降序对特征进⾏排序Fisher，Rudin 和Dominici (2018) 在他们的论⽂中建议将数据集分成两半并交换这两半的特征j的值，⽽不是置换特征j。仔细考虑⼀下，这与置换特征j 完全相同。如果需要更准确的估计，可以通过将每个实例的特征j 值与每个其他实例的特征j 值配对(⾃⾝除外) 来估计置换特征j 的误差。这为你提供了⼀个⼤⼩为n(n-1) ⾜以估计置换误差的数据集，并且需要⼤量的计算时间。如果你真的想得到⾮常准确的估计，我才建议使⽤n(n-1)-⽅法。5.5.2我应该计算训练数据还是测试数据的重要性？这我没有确切的答案。回答有关训练或测试数据的问题，触及到特征重要性的基本问题。理解基于训练数据的特征重要性与基于测试数据的特征重要性之间的区别的最好⽅法是⼀个“极端” ⽰例。我训练了⼀个⽀持向量机，给定50 个随机特征(200 个实例) 预测连续的、随机的⽬标结果。“随机” 是指⽬标结果独⽴于50 个特征。这就像根据最新的彩票号码预测明天的⽓温⼀样。如果模型“学习” 了所有的关系，则表⽰过拟合。实际上，SVM 确实对训练数据过拟合。训练数据的平均绝对误差(简称：MAE) 为', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 139, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1330.29，测试数据的平均绝对误差为0.82，这也是最可能的模型的误差(预测平均结果总为0 时MAE为0.78)。换句话说，SVM 模型是垃圾。对于此过拟合的SVM 的50 个特征，你期望特征重要性是什么值？是零？因为没有任何特征有助于提⾼看不见的测试数据的性能；还是重要性应该反映模型在多⼤程度上取决于每个特征，⽽不管所学的关系是否泛化到看不见的数据？让我们看⼀下训练和测试数据的特征重要性分布如何不同。图5.27. 按数据类型划分的特征重要性值的分布。对⼀个具有50 个随机特征和200 个实例的回归数据集进⾏了SVM 训练。⽀持向量机对数据过拟合：基于训练数据的特征重要性显⽰出许多重要的特征。根据未公开的测试数据计算，特征重要性接近1 的⽐率(= 不重要)。我不清楚这两个结果中哪个更可取。因此，我将尝试为这两个版本各做说明，让你⾃⼰决定。测试数据情况这是⼀个简单的情况：基于训练数据的模型误差估计是垃圾-> 特征重要性依赖于模型误差估计->基于训练数据的特征重要性是垃圾。实际上，这是你在机器学习中学习的第⼀件事：如果在训练模型的相同数据上测量模型误差(或性能)，则测量通常过于乐观，这意味着模型的⼯作效果似乎⽐实际情况要好得多。并且由于置换特征重要性取决于模型误差的度量，因此我们应该使⽤看不见的测试数据。基于训练数据的特征重要性使我们错误地认为特征对于预测很重要，⽽实际上模型只是过拟合⽽特征根本不重要。训练数据情况', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 140, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法134使⽤训练数据的论据在某种程度上更难以表述，但恕我直⾔，就像使⽤测试数据的论据⼀样引⼈注⽬。我们再来看⼀下垃圾的SVM。根据训练数据，最重要的特征是X42。让我们看⼀下特征X42的部分依赖图。部分依赖图显⽰了模型输出如何根据特征的变化⽽变化，并且不依赖于泛化误差。PDP 是⽤训练数据还是测试数据计算都没有关系。图5.28.X42 特征的PDP，基于训练数据并根据特征重要性，它是最重要的特征。该图显⽰了SVM 如何依赖此特征进⾏预测。该图清楚地表明，SVM 已经学会了依靠特征X42 进⾏预测，但是根据基于测试数据的特征重要性(1.04)，它并不重要。根据训练数据，重要性为1.21，反映出该模型已学会使⽤此特征。基于训练数据的特征重要性可以告诉我们哪些特征对模型很重要，模型的预测依赖于这些特征。作为使⽤训练数据情况的⼀部分，我想提出⼀个反对测试数据的论点。实际上，你希望使⽤所有数据来训练模型，以最终获得最可能的模型。这意味着将没有剩余未使⽤的测试数据来计算特征的重要性。当你要估计模型的泛化误差时，也会遇到相同的问题。如果将(嵌套的) 交叉验证⽤于特征重要性估计，则会遇到以下问题：特征重要性不是在具有所有数据的最终模型上计算的，⽽是在具有可能表现不同的数据⼦集的模型上计算的。最后，你需要决定是要知道模型在多⼤程度上依赖于每个特征来进⾏预测(-> 训练数据)，还是该特征在多⼤程度上有助于模型在未知数据上的性能(-> 测试数据)。据我所知，⽬前还没有针对训练数据与测试数据的研究。与我的“垃圾SVM” ⽰例相⽐，它将需要更深⼊的检查。我们需要更多的研究和使⽤这些⼯具的经验来获得更好的理解。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 141, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法135接下来，我们将看⼀些⽰例。我将重要性计算基于训练数据，因为我必须选择⼀个，并且使⽤训练数据可以少⼏⾏代码。5.5.3示例和解释我展⽰了分类和回归的⽰例。宫颈癌(分类)我们拟合随机森林模型预测宫颈癌。我们⽤1-AUC (1 减去ROC 曲线下的⾯积) 测量误差增加量。将模型误差增加为1 倍(= ⽆变化) 的特征表⽰对于宫颈癌的预测并不重要。图5.29. 随机森林预测宫颈癌各特征的重要性。最重要的特征是年龄。置换年龄导致1-AUC 增加4.49 倍。最重要的特征是年龄，置换后误差增加4.49。租赁自行车(回归)我们拟合了⼀个⽀持向量机模型，在给定天⽓条件和⽇历信息的情况下预测租赁⾃⾏车的数量。作为误差测量，我们使⽤平均绝对误差。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 142, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法136图5.30. 使⽤⽀持向量机预测⾃⾏车计数时每个特征的重要性。最重要的是温度，最不重要的是假期。5.5.4优点很好的解释：特征重要性是当特征信息被破坏时模型误差的增加。特征重要性提供了对模型⾏为的高度压缩的、全局的洞悉。使⽤错误的⽐率代替错误的差值的⼀个积极⽅⾯是，在不同问题之间，特征重要性度量是可比较的。重要性度量会⾃动考虑与其他特征的所有交互。通过置换特征，你还可以破坏与其他特征的交互效应。这意味着置换特征重要性同时考虑了主要特征效应和交互效应。这也是⼀个缺点，因为两个特征之间的交互作⽤的重要性包括在两个特征的重要性测量中。这意味着特征重要性不是加起来就是总性能的下降，⽽是总和更⼤。仅当特征之间没有交互时(如线性模型中)，重要性才近似相加。置换特征重要性不需要重新训练模型。其他⼀些⽅法建议删除特征，重新训练模型，然后⽐较模型误差。由于机器学习模型的重新训练可能需要很长时间，因此“只” 置换特征就可以节省⼤量时间。乍⼀看，使⽤部分特征重新训练模型的重要性⽅法乍看起来似乎很直观，但是数据减少后的模型对于特征的重要性却毫⽆意义。我们对固定模型的特征重要性感兴趣。使⽤减少的数据集进⾏重新训练会创建与我们感兴趣的模型不同的模型。假设你训练了⼀个稀疏线性模型(使⽤Lasso)，该模型具有固定数量的权重且权重⾮零。数据集具有100 个特征，你可以将⾮零权重的数量设置为5。你', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 143, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法137可以分析其中⼀个⾮零权重的特征的重要性。你删除特征并重新训练模型。模型的性能保持不变，因为另⼀个同样出⾊的特征的权重不为零，你的结论是该特征并不重要。另⼀个例⼦：模型是决策树，我们分析了被选为第⼀个分割的特征的重要性。你删除特征并重新训练模型。由于选择了另⼀个特征作为第⼀个分割，因此整棵树可能会⾮常不同，这意味着我们⽐较(可能) 完全不同的树的错误率，以确定该特征对其中⼀棵树的重要性。5.5.5缺点非常不清楚应该使用训练数据还是测试数据来计算特征的重要性。置换特征的重要性与模型的误差有关。这并不是天⽣的坏事，但在某些情况下不是你所需要的。在某些情况下，你可能希望了解某个特征的模型输出有多少变化，⽽⽆需考虑其对性能的影响。例如，你想了解当有⼈操纵特征时模型输出的鲁棒性。在这种情况下，你不会对置换特征时模型性能降低多少感兴趣，⽽是由每个特征解释的模型输出的⽅差是多少。当模型泛化得很好时(即，它不会过拟合)，模型⽅差(由特征解释) 和特征重要性密切相关。你需要获得真正的结果。如果某⼈仅向你提供模型和未标记的数据，但没有提供真实结果，则你将⽆法计算置换特征的重要性。置换特征的重要性取决于对特征的改变，这会增加测量的随机性。重复置换后，结果可能会有很大差异。重复置换并在重复中对重要性值进⾏平均，可以鲁棒地度量结果，但会增加计算时间。如果特征是相关的，则置换特征重要性可能会因不切实际的数据实例而有偏差。问题与部分依赖图相同：特征的置换在两个或多个特征关联时不会产⽣数据实例。当它们正相关时(例如⼀个⼈的⾝⾼和体重)，并且我改变了其中⼀个特征时，我创建了不太可能甚⾄是物理上不可能的新实例(例如，体重2 公⽄的2 ⽶体重的⼈)，但是我使⽤了这些新实例衡量重要性。换句话说，对于相关特征的置换特征重要性，我们考虑了当我们将特征与在现实中永远不会观察到的值交换时，模型性能会降低多少。检查特征是否紧密相关，如果是，对特征重要性的解释要⼩⼼。另⼀个棘⼿的事情：添加相关特征可以通过在两个特征之间拆分重要性降低关联特征的重要性。让我举⼀个例⼦说明我所说的“拆分” 特征重要性的含义：我们要预测下⾬的概率，并将前⼀天上午8 点的温度⽤作特征，以及其他不相关的特征。我训练了⼀个随机森林，结果发现温度是最重要的特征，⼀切都很好，第⼆天晚上我睡得很好。现在想象另⼀个场景，其中我另外包括了上午9 点的温度，它作为与上午8 点的温度强相关的特征。如果我已经知道上午8 点的温度，那么上午9 点的温度不会给我太多其他信息。但是拥有更多特征总是好的，对吧？我⽤两个温度特征和不相关特征训练了另⼀个随机森林。随机森林中的⼀些树基于上午8 点的温度，⼀些树基于上午9 点的温度，⼀些则是同时拥有，还有⼀些则都没有。这两个温度特征⼀起⽐以前的单个温度特征具有更多的重要性，但是每个温度现在不在重要特征列表的顶部，⽽是在中间。通过引⼊相关特征，我将最重要的特征从重要性阶梯的顶端踢到了中间。⼀⽅⾯，这很好，因为它仅反映了底层机器学习模型(这', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 144, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法138⾥是随机森林) 的⾏为。上午8 点的温度已经变得不那么重要了，因为该模型现在也可以依靠上午9 点的测量值。另⼀⽅⾯，这使得特征重要性的解释变得相当困难。假设你要检查特征是否存在测量错误。检查费⽤昂贵，你决定只检查最重要特征中的前3 个。在第⼀种情况下，你将检查温度，在第⼆种情况下，你将不包括任何温度特征，因为它们现在具有同等的重要性。即使重要性值在模型⾏为的层次上可能有意义，但如果你具有相关特征，则仍会造成混淆。5.5.6软件和替代方法R iml 包被⽤于⽰例。在R DALEX 和vip 包，以及Python alibi 包也实现了模型⽆关置换特征重要性。⼀个名为PIMP 的算法对特征重要性算法进⾏了改进，为重要性提供p 值。5.6全局代理模型全局代理模型是⼀种可解释的模型，经过训练可近似于⿊盒模型的预测。我们可以通过解释代理模型得出有关⿊盒模型的结论。通过使⽤更多的机器学习解决机器学习的可解释性！5.6.1理论代理模型也可⽤于⼯程中：如果⽬标结果昂贵、耗时或难以衡量(例如，由于来⾃复杂的计算机模拟)，则可以使⽤结果廉价、快速的代理模型。⼯程中使⽤的代理模型和可解释机器学习中使⽤的代理模型的区别在于，底层模型是机器学习模型(不是模拟)，并且代理模型必须是可解释的。(可解释的) 代理模型的⽬的是尽可能准确地近似底层模型的预测，并且可以同时进⾏解释。可以⽤不同的名称找到代理模型的概念：近似模型(Approximation Model)，元模型(metamodel)，响应⾯模型(Response Surface Model)，仿真器(emulator) 等等。关于这个理论：理解代理模型实际上并不需要太多理论。我们希望在g 可解释的约束下，代理模型预测函数g 尽可能接近地逼近我们的⿊盒预测函数f。对于函数g，可以使⽤任何可解释的模型，例如来⾃“可解释的模型” ⼀章的模型。例如线性模型：g(x) = β0 + β1x1 + . . . + βpxp或决策树：g(x) =M∑m=1cmI{x ∈Rm}', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 145, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法139训练代理模型是⼀种与模型⽆关的⽅法，因为它不需要有关⿊盒模型内部运作的任何信息，仅需要访问数据和预测。如果将底层机器学习模型替换为其他机器学习模型，你仍可以使⽤代理⽅法。⿊盒模型类型和代理模型类型的选择是分离的。执⾏以下步骤以获得代理模型：1. 选择数据集X。这可以是⽤于训练⿊盒模型的相同数据集，也可以是来⾃同⼀分布的新数据集。你甚⾄可以根据应⽤程序选择数据的⼦集或点的⽹格。2. 对于选定的数据集X，获取⿊盒模型的预测。3. 选择⼀种可解释的模型类型(线性模型，决策树等)。4. 在数据集X 及其预测上训练可解释模型。5. 恭喜你！你现在有了⼀个代理模型。6. 衡量代理模型复制⿊盒模型预测的效果。7. 解释代理模型。你可能会发现代理模型的⽅法有⼀些额外的步骤或稍有不同，但⼀般的想法通常如此处所述。衡量代理复制⿊盒模型的能⼒的⼀种⽅法是R-平⽅度量：R2 = 1 −SSESST = 1 −∑ni=1(ˆy(i)∗−ˆy(i))2∑ni=1(ˆy(i) −¯ˆy)2其中ˆy(i)∗是代理模型的第i 个实例的预测，ˆy(i) 是⿊盒模型的预测和¯ˆy 是⿊盒模型预测的平均值。SSE 代表误差平⽅和，SST 代表总的平⽅和。R-平⽅可以解释为代理模型捕获的⽅差百分⽐。如果R-平⽅接近1 (= 低SSE)，则可解释模型会很好地近似⿊盒模型的⾏为。如果可解释模型⾮常接近，则可能要⽤可解释模型替换复杂模型。如果R 平⽅接近0 (= ⾼SSE)，则可解释模型⽆法解释⿊盒模型。请注意，我们没有谈论底层⿊盒模型的模型性能，也就是说，它在预测实际结果⽅⾯的好坏。⿊盒模型的性能在训练代理模型中不起作⽤。代理模型的解释仍然有效，因为它是关于模型的陈述，⽽不是关于现实世界的陈述。但是，当然，如果⿊盒模型不好，则代理模型的解释就变得⽆关紧要了，因为⿊盒模型本⾝就是⽆关紧要的。我们还可以基于原始数据的⼦集构建代理模型，或重新调整实例的权重。这样，我们更改了代理模型输⼊的分布，从⽽改变了解释的重点(然后它不再是真正的全局性的)。如果我们通过数据的特定实例在局部对数据进⾏加权(数据与所选的特定实例越近，则它们的权重就越⾼)，我们将获得⼀个局部代理模型，该模型可以解释该实例的各个预测。在下⼀节中阅读有关局部模型的更多信息。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 146, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1405.6.2示例为了演⽰代理模型，我们考虑⼀个回归和⼀个分类⽰例。⾸先，我们训练⼀个⽀持向量机，根据天⽓和⽇历信息预测⾃⾏车的租量。⽀持向量机不是很容易解释，因此我们将CART 决策树作为可解释模型训练代理，以近似⽀持向量机的⾏为。图5.31. 代理树的终端节点，它近似⾃⾏车租赁数据集上训练的⽀持向量机的预测。节点中的分布表明，当温度⾼于13 摄⽒度时，以及天数是2 年的晚些时候(临界点为435 天)，代理树预测租⽤⾃⾏车的数量会更⾼。代理模型的R-平⽅(⽅差解释) 为0.77，这意味着它很好地近似了底层⿊盒的⾏为，但并不完美。如果拟合是完美的，我们可以扔掉⽀持向量机，⽽改⽤树。在第⼆个⽰例中，我们拟合随机森林模型预测了宫颈癌的概率。再次，我们使⽤原始数据集训练决策树，但将随机森林的预测作为结果，⽽不是数据中的真实类别(健康、癌症)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 147, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法141图5.32. 代理树的终端节点，它近似于⼀个随机森林对宫颈癌数据集的预测。节点中的计数显⽰节点中⿊盒模型分类的频率。代理模型的R-平⽅(⽅差已解释) 为0.19，这意味着它不能很好地逼近随机森林，并且在得出有关复杂模型的结论时，我们不应过度解释树。5.6.3优点代理模型⽅法非常灵活：可以使⽤“可解释的模型” ⼀章中的任何模型。这也意味着你不仅可以交换可解释模型，还可以交换底层⿊盒模型。假设你创建了⼀个复杂的模型，并向公司中的不同团队进⾏了解释。⼀个团队熟悉线性模型，另⼀个团队可能了解决策树。你可以为原始⿊盒模型训练两个代理模型(线性模型和决策树)，并提供两种解释。如果你发现性能更好的⿊盒模型，则不必更改解释⽅法，因为你可以使⽤同⼀类代理模型。我认为这种⽅法⾮常直观和直接。这意味着它易于实现，⽽且也易于向不熟悉数据科学或机器学习的⼈们解释。使⽤R-平方测量，我们可以轻松地测量我们的代理模型在逼近⿊盒预测⽅⾯的表现。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 148, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1425.6.4缺点你必须注意，你得出的是有关模型而不是数据的结论，因为代理模型永远看不到实际结果。尚不清楚R-平方的最佳截止点是什么，以便确信代理模型与⿊盒模型⾜够接近。解释⽅差的80%？50%？99%？我们可以衡量代理模型与⿊盒模型的接近程度。让我们假设我们不是很接近，但是⾜够接近。对于数据集的一个子集，可解释模型可能非常接近，而对于另一子集，则可能发生很大差异。在这种情况下，对于所有数据点，对简单模型的解释将不尽相同。你选择的可解释的代理模型本身的所有优点和缺点。有⼈认为，⼀般⽽⾔，没有本质上可解释的模型(甚⾄包括线性模型和决策树)，甚⾄有⼀个可解释性的错觉也是危险的。如果你对此表⽰赞同，那么这种⽅法当然不适合你。5.6.5软件我将R iml 包⽤作⽰例。如果你可以训练机器学习模型，那么你应该能够⾃⼰实现代理模型。只需训练⼀个可解释的模型即可预测⿊盒模型的预测。5.7局部代理(LIME)局部代理模型本⾝是可解释的模型，⽤于解释⿊盒机器学习模型的单个实例预测。局部可解释模型⽆关的解释(Local interpretable model-agnostic explanations，LIME)[23] 是作者在⼀篇论⽂中提出的局部代理模型的具体实现。代理模型经过训练可以近似底层⿊盒模型的预测。LIME 并⾮训练全局代理模型，⽽是专注于训练局部代理模型以解释单个预测。这个想法很直观。⾸先，忘记训练数据，并假设你只有⿊盒模型，你可以在其中输⼊数据点并获得模型的预测。你可以随时探测⿊盒。⽽你的⽬标是了解机器学习模型为何做出特定的预测。当你将数据变化后输⼊到机器学习模型中时，LIME 会测得预测发⽣了什么。LIME ⽣成⼀个新的数据集，该数据集由扰动的样本和⿊盒模型的相应预测组成。然后，在这个新的数据集上，LIME 训练了⼀个可解释的模型，该模型通过采样实例与感兴趣实例的接近程度来加权。可解释模型可以是“可解释的模型” ⼀章中的任何模型，例如Lasso 或决策树。所学习的模型应该是机器学习模型局部预测的良好近似，但不⼀定是良好的全局近似。这种准确性也称为局部保真度。在数学上，具有可解释性约束的局部代理模型可以表⽰为：explanation(x) = arg ming∈G L(f, g, πx) + Ω(g)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 149, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法143例如对实例x 的解释模型是模型g (例如线性回归模型)，最⼩化损失L (例如均⽅误差) 测量了解释g 与原始模型f (例如xgboost 模型) 的预测的接近程度，⽽模型复杂度Ω(g) 保持较低⽔平(例如，偏好较少的特征)。G 是可能的解释的族，例如所有可能的线性回归模型。接近度πx 定义了我们考虑解释时实例x 附近的邻域⼤⼩。实际上，LIME 仅优化损失部分。⽤户必须确定复杂度，例如，通过选择线性回归模型可以使⽤的最⼤特征数。训练局部代理模型的⽅法：• 选择你想要对其⿊盒预测进⾏解释的感兴趣实例。• 扰动你的数据集并获得这些新点的⿊盒预测。• 根据新样本与⽬标实例的接近程度对其进⾏加权。• 在新数据集上训练加权的，可解释的模型。• 通过解释局部模型来解释预测。例如，在R 和Python 的当前实现中，可以选择线性回归作为可解释的代理模型。事先，你必须选择K，即你希望在可解释模型中拥有的特征数量。K 越低，解释这个模型越容易。较⾼的K 可能会产⽣具有较⾼保真度的模型。有⼏种⽅法可以训练具有K 个特征的模型。Lasso 是⼀个不错的选择。具有⾼正则化参数λ 的Lasso 模型会⽣成没有任何特征的模型。通过缓慢减⼩λ，重新训练Lasso 模型，这些特征获得不为零的权重估计。如果模型中有K 个特征，则你已达到所需的特征数量。其他策略是向前或向后选择特征。这意味着你可以从完整模型(= 包含所有特征) 开始，或者从仅具有截距的模型开始，然后测试添加或删除时哪个特征将带来最⼤的改进，直到达到具有K个特征的模型。你如何获得数据的变化？这取决于数据类型，可以是⽂本，图像或表格数据。对于⽂本和图像，解决⽅案是“打开” 或“关闭” 某个单词或超像素。对于表格数据，LIME 通过分别对每个特征进⾏扰动来创建新样本，并从正态分布中提取平均值和标准差。5.7.1表格数据的LIME表格数据是以表格形式出现的数据，每⼀⾏代表⼀个实例，每⼀列代表⼀个特征。LIME 样本不是在感兴趣的实例周围获取，⽽是从训练数据的质⼼获取，这是有问题的。但这增加了某些采样点预测的结果与感兴趣的数据点不同的概率，并且LIME ⾄少可以学到⼀些解释。最好从视觉上解释采样和局部模型训练的⼯作原理：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 150, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法144图5.33. ⽤于表格数据的LIME 算法。A) 给定特征x1 和x2 的随机森林预测。预测的类别：1 (深⾊) 或0 (浅⾊)。B) 感兴趣的实例(⼤点) 和从正态分布采样的数据(⼩点)。C) 给感兴趣实例附近的点分配更⾼的权重。D) ⽹格的符号显⽰了从加权样本中局部地学习的模型的分类。⽩线标记了决策边界(P(class = 1) = 0.5)。与往常⼀样，细节决定成败。在点周围定义有意义的邻域⾮常困难。当前LIME 使⽤指数平滑核来定义邻域。平滑核是⼀个函数，它接受两个数据实例并返回⼀个接近度(proximity measure)。核宽度决定了邻域的⼤⼩：较⼩的核宽度意味着实例必须⾮常接近才能影响局部模型，较⼤的核宽度意味着距离较远的实例也会影响模型。如果你看⼀下LIME 的Python 实现(⽂件lime/lime_tabular.py)你会看到它使⽤了指数平滑核(针对标准化数据)，并且核宽度是训练数据的列数的平⽅根的0.75倍。最⼤的问题是我们没有找到最佳核或宽度的好⽅法。甚⾄0.75 来⾃哪⾥？在某些情况下，你可以通过更改核宽度轻松地转换你的解释，如下图所⽰：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 151, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法145图5.34. 实例x = 1.6 的预测解释。⿊盒模型根据单个特征的预测以粗线显⽰，数据分布以rug 显⽰。计算了三个具有不同核宽度的局部代理模型。⽣成的线性回归模型取决于核宽度：对于x = 1.6，特征具有负效应，正效应或是⽆效应？该⽰例仅显⽰⼀个特征。在⾼维特征空间中情况更糟。距离度量是否应该平等对待所有特征也不清楚。特征x1 的距离单位等于特征x2 的⼀个单位吗？距离度量是⾮常任意的，不同维度(也称为特征) 上的距离可能根本⽆法⽐较。示例让我们看⼀个具体的例⼦。我们返回到⾃⾏车租赁数据，并将预测问题转化为⼀个分类问题：考虑到⾃⾏车租赁随着时间的推移变得越来越流⾏的趋势后，我们想知道在某天租赁的⾃⾏车数量是否将⾼于或低于趋势线。你也可以将“⾼于” 解释为⾼于平均⾃⾏车数量，但会根据趋势进⾏调整。⾸先，我们在分类任务上训练了100 棵树的随机森林。根据天⽓和⽇历信息，租赁⾃⾏车的数量将在哪⼀天⾼于平均值？解释包括2 个特征。针对具有不同预测类别的两个实例训练的稀疏局部线性模型的结果：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 152, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法146图5.35.⾃⾏车租赁数据集的两个实例的LIME 解释。温暖的温度和良好的天⽓状况对预测有正效应。x 轴显⽰特征效应：权重乘以实际特征值。从图中可以清楚地看出，分类特征⽐数值特征更容易解释。⼀种解决⽅案是将数值特征分类到箱中。5.7.2文本的LIME⽤于⽂本的LIME 与⽤于表格数据的LIME 不同。对数据的变化⽅式不同：从原始⽂本开始，通过从原始⽂本中随机删除单词来创建新⽂本。数据集⽤每个单词的⼆进制特征表⽰。如果包含相应的单词，则特征为1；如果已删除，则特征为0。示例在此⽰例中，我们将YouTube 评论归类为垃圾或正常。⿊盒模型是在⽂档词矩阵上训练的深度决策树。每个评论是⼀⾏，每⼀列是给定单词的出现次数。简短的决策树很容易理解，但是在这个⽰例中树会很深。代替该树的⽅法，可以是在词嵌⼊(抽象向量) 上训练的循环神经⽹络或⽀持向量机。让我们看⼀下该数据集的两个评论以及相应的类(垃圾评论为1，正常评论为0) ：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 153, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法147CONTENTCLASS267PSY is a good guy0173For Christmas Song visit my channel! ;)1下⼀步是创建局部模型中使⽤的数据集的⼀些变体。例如，其中⼀条评论的⼀些变体(对原来评论进⾏变化后的版本)：ForChristmasSongvisitmychannel!;)probweight210110010.090.57301111010.090.71410011110.990.71510111110.990.86601110010.090.57每⼀列对应于句⼦中的⼀个单词。每⾏都是⼀个变体，1 表⽰该词是该变体的⼀部分，0 表⽰该词已被删除。变体之⼀的对应句⼦是“Christmas Song visit my;)”。“prob” 列显⽰了每个句⼦变体的垃圾评论的预测概率。“weight” 列显⽰变体与原始句⼦的接近程度，计算⽅式为1 减去删除的单词所占的⽐例，例如，如果7 个单词中删除了1 个，则接近程度为1-1/7=0.86。这是两个句⼦(⼀个是垃圾评论，⼀个不是垃圾评论)，其估计的局部权重由LIME 算法找到：caselabel_probfeaturefeature_weight10.0872151good0.00000010.0872151a0.00000010.0872151PSY0.00000020.9939759channel!6.90875520.9939759visit0.00000020.9939759Song0.000000“channel” 表⽰垃圾评论的概率很⾼。对于⾮垃圾评论，不会估计⾮零权重，因为⽆论删除哪个单词，预测类别都将保持不变。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 154, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1485.7.3图像的LIME⽤于图像的LIME 与⽤于表格数据和⽂本的LIME 不同。直观地讲，扰动单个像素没有多⼤意义，因为许多个像素对⼀个类有贡献。随机改变单个像素可能不会对预测产⽣太⼤影响。因此，通过将图像分割成“超像素” 并“关闭” 或“打开” 超像素来创建图像的变化。超像素是具有相似颜⾊的互连像素，可以通过将每个像素替换为⽤户定义的颜⾊(例如灰⾊) 来关闭。⽤户还可以指定在每次置换中关闭超像素的概率。示例由于图像解释的计算相当缓慢，因此，R lime 包包含⼀个预先计算的⽰例，我们还将使⽤该⽰例演⽰该⽅法的输出。这些解释可以直接显⽰在图像样本上。由于每个图⽚可以有⼏个预测标签(按概率排序)，因此我们可以解释前n 个预测标签。对于下图，前3 位的预测是电吉他(Electric guitar)，民谣吉他(Acoustic guitar) 和拉布拉多(Labrador)。图5.36.由Google Inception 神经⽹络对图像分类的前三类进⾏LIME 解释。该⽰例摘⾃LIME论⽂(Ribeiro 等，2016)。第⼀种情况的预测和解释是⾮常合理的。第⼀个预测电吉他当然是错误的，但是解释告诉我们神经⽹络仍然表现合理，因为识别出的图像部分表明这可能是电吉他。5.7.4优点即使你替换了底层的机器学习模型，你仍然可以使⽤相同的局部可解释模型进⾏解释。假设看解释的⼈最了解决策树。因为你使⽤局部代理模型，所以可以将决策树⽤作解释，⽽不必实际使⽤决策树进⾏预测。例如，你可以使⽤SVM。⽽且，如果发现xgboost 模型的效果更好，则可以替换SVM，并且仍然⽤决策树来解释这些预测。局部代理模型受益于训练和解释可解释的模型的⽂献和经验。当使⽤Lasso 或短的树时，得到的解释是简短的(= 选择性的)，并且可能是对比性的。因此，它们做出了⼈类友好的解释。这就是为什么我在解释的接收者是外⾏⼈或时间很少的⼈的应⽤程序中', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 155, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法149看到LIME 的原因。对于完整的归因⽽⾔，这还不够，因此在合规性场景中我看不到LIME，在合规性场景中，法律上可能会要求你全⾯解释预测。对于调试机器学习模型，有所有的原因⽽不是少数是有⽤的。LIME 是少数适用于表格数据，文本和图像的⽅法之⼀。保真度度量(可解释模型与⿊盒预测的近似程度) 使我们很好地了解了可解释模型在解释感兴趣的数据实例附近的⿊盒预测⽅⾯的可靠性。LIME 在Python (LIME 库) 和R (LIME 包和iml 包) 中实现，并且非常易于使用。⽤局部代理模型创建的解释可以使用除原始模型所用以外的其他(可解释) 特征。当然，这些可解释的特征必须从数据实例中派⽣。⽂本分类器可以将抽象词嵌⼊作为特征，但解释可以基于句⼦中是否存在词。回归模型可以依赖于某些属性的不可解释的转换，但是可以使⽤原始属性来创建解释。例如，可以在答案的主成分分析(PCA) 的成分上训练回归模型，⽽LIME 可以在原始问题上训练。与其他⽅法相⽐，对LIME 使⽤可解释特征可能是⼀个很⼤的优势，尤其是当模型使⽤不可解释特征进⾏训练时。5.7.5缺点当对表格式数据使⽤LIME 时，正确定义邻域是⼀个很⼤的未解决的问题。我认为这是LIME 的最⼤问题，也是我建议仅谨慎使⽤LIME 的原因。对于每个应⽤程序，你都必须尝试不同的核设置，并亲⾃查看解释是否有意义。不幸的是，这是我可以找到合适的核宽度的最佳建议。在当前LIME 的实现中可以改善采样。从⾼斯分布中采样数据点，⽽忽略特征之间的相关性。这可能会导致不太可能的数据点，然后这些数据点可⽤于学习局部解释模型。解释模型的复杂性必须事先定义。这只是⼀个⼩⼩的抱怨，因为最终⽤户总是必须定义保真度和稀疏度之间的折衷。另⼀个真正的⼤问题是解释的不稳定。在[32] 中，作者表明，在模拟环境中两个⾮常接近的点的解释差异很⼤。另外，以我的经验，如果你重复采样过程，那么出来的解释可能会有所不同。不稳定意味着很难相信这些解释，你应该⾮常严格。结论：以LIME 作为具体实现的局部代理模型⾮常有应⽤前景。但是该⽅法仍处于开发阶段，需要解决许多问题才能安全应⽤。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 156, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1505.8Shapley 值可以通过假设实例的每个特征值是游戏中的“玩家” 来解释预测，其中预测是“总⽀出”。Shapley值是联盟博弈论的⼀种⽅法，它告诉我们如何在特征之间公平地分配“总⽀出”。5.8.1总体思路假定以下情况：你已经训练了机器学习模型来预测公寓价格。对于某套公寓，它的预计价格为300,000 欧元，你需要对此做出解释。公寓的⼤⼩为50 平⽅⽶，位于2 楼，附近有公园，并且禁⽌猫⼊内：图5.37.⼀套50 平⽅⽶的公寓，附近有公园以及禁⽌猫⼊内，预计价格为300,000 欧元。我们的⽬标是解释这些特征值如何对预测做出贡献。所有公寓的平均预测价格为310,000 欧元。与平均预测相⽐，每个特征值对预测有多少贡献？对于线性回归模型，答案很简单。每个特征的效应是特征的权重乘以特征值。这仅因模型的线性⽽起作⽤。对于更复杂的模型，我们需要不同的解决⽅案。例如，LIME 建议使⽤局部模型来估计特征效应。另⼀种解决⽅案来⾃合作博弈理论：Shapley 值(由Shapley (1953)[33] 创造) 是⼀种根据玩家对总⽀出的贡献分配⽀出给玩家的⽅法。玩家在联盟中进⾏合作，并从此合作中获得⼀定的收益。玩家？游戏？⽀出？与机器学习预测和可解释性有什么关系？“游戏” 是数据集单个实例的预测任务。“收益” 是此实例的实际预测值减去所有实例的平均预测值。“玩家” 是实例的特征值，它们协同⼯作以获得收益。在我们的公寓⽰例中，特征值“公园附近”、“禁⽌猫进⼊”、“50 平⽅⽶” 和“2 楼”', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 157, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法151共同实现300,000 欧元的预测。我们的⽬标是解释实际预测(300,000 欧元) 与平均预测(310,000 欧元) 之间的差额：-10,000 欧元。答案可能是：“公园附近” 贡献30,000 欧元；“50 平⽅⽶” 贡献10,000 欧元；“2 楼” 贡献0 欧元；“禁⽌猫进⼊” 贡献-50,000 欧元。贡献总计为-10,000 欧元，即最终预测减去平均预测公寓价格。我们如何计算⼀个特征的Shapley 值？Shapley 值是所有可能的联盟(Coalition) 中特征值的平均边际贡献。这么说清楚了吗？在下图中，我们评估了将“禁⽌猫进⼊” 添加到“公园附近” 和“50 平⽅⽶” 组成的联盟中时的贡献。我们通过从数据中随机绘制另⼀套公寓，模拟出只有“公园附近”、“禁⽌猫进⼊” 和“50 平⽅⽶” 在⼀个联盟中，并将它的值⽤于楼层特征。“2 楼” 被随机抽取的“1 楼” 所取代，然后我们预测这个组合的公寓价格(310,000 欧元)。在第⼆步中，我们将“禁⽌猫进⼊” 从联盟中删除，从随机绘制的公寓中将猫允许/禁⽌进⼊的特征值替换。在这个随机⽰例中为“允许猫进⼊”，但也可能再是“禁⽌猫进⼊”。我们预测对“公园附近” 和“50 平⽅⽶” 联盟的公寓价格(320,000 欧元)。“禁⽌猫进⼊” 的贡献310,000 - 320,000 = -10.000 欧元。此估计取决于⽤作猫和楼层特征值的“贡献者”的随机绘制的值。如果我们重复此采样步骤并平均贡献，将会得到更好的估计。图5.38. ⼀个样本来估计当“禁⽌猫进⼊” 加⼊“公园附近” 和“50 平⽅⽶” 联盟时对预测的贡献。我们对所有可能的联盟重复此计算。Shapley 值是对所有可能的联盟的所有边际贡献的平均值。计', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 158, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法152算时间随特征数量呈指数增长。保持计算时间可控的⼀个解决⽅案是只计算可能联盟的少量样本的贡献。下图显⽰了确定“禁⽌猫进⼊” 的Shapley 值所需的所有特征值联盟。第⼀⾏显⽰没有任何特征值的联盟。第⼆，第三和第四⾏显⽰随联盟⼤⼩增加⽽不同的联盟，以“|” 分隔。总⽽⾔之，可能的联盟如下：• 空• “公园附近”• “50 平⽅⽶”• “2 楼”• “公园附近”+“50 平⽅⽶”• “公园附近”+“2 楼”• “50 平⽅⽶”+“2 楼”• “公园附近”+“50 平⽅⽶”+“2 楼”对于这些联盟中的每个联盟，我们都计算带有或不带有特征值“禁⽌猫进⼊” 的预测公寓价格，并取其差值以获得边际贡献。Shapley 值是边际贡献的(加权) 平均值。我们⽤公寓数据集中的随机特征值替换不在联盟中的特征的特征值，以从机器学习模型中获得预测。图5.39. ⽤于计算“禁⽌猫进⼊” 的精确Shapley 值的所有8 个联盟。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 159, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法153如果我们估计所有特征值的Shapley 值，则可以得到特征值之间预测的完整分布(减去平均值)。5.8.2示例与解释特征值j 的Shapley 值的解释是：与数据集的平均预测相⽐，第j 个特征的值对这个特定实例的预测的贡献为ϕj。Shapley 值适⽤于分类(如果我们要处理概率) 和回归。我们使⽤Shapley 值来分析预测宫颈癌的随机森林模型的预测：图5.40.宫颈癌数据集中⼀名⼥性的Shapley 值。预测为0.57，则该⼥性患癌的概率⽐平均预测值0.03 ⾼0.54。被诊断的性病(STDs) 数量对概率的增加最⼤。贡献之和产⽣实际和平均预测之间的差异(0.54)。对于⾃⾏车租赁数据集，我们还训练了⼀个随机森林，在给定天⽓和⽇历信息的情况下预测⼀天的⾃⾏车租赁数量。为特定⽇期的随机森林预测创建的解释：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 160, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法154图5.41.第285 天的Shapley 值。预计会有2409 辆出租⾃⾏车，这⼀天⽐平均预测值4518 少2108。天⽓情况和湿度对负值的影响最⼤。这⼀天的温度有积极的贡献。Shapley 值之和产⽣实际和平均预测的差异(-2108)。请注意正确解释Shapley 值：Shapley 值是特征值在不同联盟中对预测的平均贡献。当我们从模型中删除特征时，Shapley 值并不是预测的差异。5.8.3详细的Shapley 值本节将为好奇的读者更深⼊地介绍Shapley 值的定义和计算。如果你对技术细节不感兴趣，请跳过本节，直接转到“优点和缺点”。我们对每个特征如何影响数据点的预测很感兴趣。在线性模型中，很容易计算出各个特征效应。这是⼀个数据实例的线性模型预测的样⼦：ˆf(x) = β0 + β1x1 + . . . + βpxp其中x 是要计算其贡献的实例。每个xj 都是⼀个特征值(j = 1\\uffff…\\uffffp)。βj 是特征j 相对应的权重。第j 个特征对预测ˆf(x) 的贡献为ϕj：ϕj( ˆf) = βjxj −E(βjXj) = βjxj −βjE(Xj)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 161, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法155其中E(βjXj) 是特征j 的平均效应估计值。贡献是特征效应减去平均效应的差。好极了！现在我们知道每个特征对预测的贡献。如果我们将⼀个实例的所有特征贡献相加，则结果如下：p∑j=1ϕj( ˆf) =p∑j=1(βjxj −E(βjXj))=(β0 +p∑j=1βjxj) −(β0 +p∑j=1E(βjXj))= ˆf(x) −E( ˆf(X))这是数据点x 的预测值减去平均预测值。特征贡献可能为负。我们可以对任何类型的模型执⾏相同操作吗？将此作为模型⽆关的⼯具将是很棒的。由于我们通常在其他模型类型中没有相似的权重，因此我们需要不同的解决⽅案。帮助来⾃意想不到的地⽅：合作博弈理论。Shapley 值是⼀种针对任何机器学习模型的单个预测计算特征贡献的解决⽅案。Shapley 值Shapley 值是通过S 中玩家的值函数val 定义的。每个特征值的Shapley 值是其对总⽀出(预测) 的贡献，在所有可能的特征值组合上加权和求和：ϕj(val) =∑S⊆{x1,...,xp}\\\\{xj}|S|! (p −|S| −1)!p!(val (S ∪{xj}) −val(S))其中S 是模型中使⽤的特征的⼦集，x 是要解释的实例的特征值的向量，p 是特征的数量。valx(S)是对集合S 中的特征值的预测，它是在集合S 中未包含的特征上边缘化：valx(S) =∫ˆf(x1, . . . , xp)dPx/∈S −EX( ˆf(X))实际上你对每个未包含的特征执⾏了多次积分。⼀个具体的⽰例：机器学习模型可⽤于4 个特征x1，x2，x3 和x4，并且我们评估由特征值x1 和x3 组成的联盟S 的预测：valx(S) = valx({x1, x3}) =∫R∫Rˆf(x1, X2, x3, X4)dPX2X4 −EX( ˆf(X))这看起来类似于线性模型中的特征贡献！不要对“值” ⼀词的多种⽤法感到困惑：特征值是实例特征的数值或分类值；Shapley 值是特征对预测的贡献；值函数是玩家(特征值) 联盟的⽀出函数。Shapley 值是唯⼀满⾜效益性(Efficiency)，对称性(Symmetry)，虚拟性(Dummy) 和可加性(Additivity) 的归因⽅法，这些性质⼀起可以视为公平⽀出的定义。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 162, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法156效益性特征贡献必须加起来等于x 和平均值的预测差。∑pj=1 ϕj = ˆf(x) −EX( ˆf(X))对称性如果两个特征值j 和k 对所有可能的联盟均贡献相同，则它们的贡献应相同。如果val(S ∪{xj}) = val(S ∪{xk})对于所有S ⊆{x1, . . . , xp} \\\\ {xj, xk}那么ϕj = ϕk虚拟性假设⽆论将特征值添加到哪个集合，都不会改变预测值的特征j，它的Shapley 值应为0。如果val(S ∪{xj}) = val(S)对于所有S ⊆{x1, . . . , xp}那么ϕj = 0可加性对于具有组合⽀出val + val+ 的游戏，各⾃的Shapley 值如下：ϕj + ϕ+j假设你训练了⼀个随机森林，这意味着预测是许多决策树的平均值。“可加性” 性质保证对于特征值，你可以分别计算每棵树的Shapley 值，取平均值，然后获得随机森林的特征值的Shapley 值。直觉理解Shapley 值的⼀种直观⽅法：特征值以随机顺序进⼊房间。房间中的所有特征值都参与游戏(= 有助于预测)。特征值的Shapley 值是当该特征值加⼊它们时，已经存在于房间中的联盟所收到的预测的平均变化。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 163, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法157估算Shapley 值精确的Shapley 值必须使⽤第j 个特征和不使⽤第j 个特征的所有可能的联盟(集合) 来估计。当特征⽐较多时，随着更多特征的添加，可能的联盟数量呈指数增长，因此对该问题的精确解成为问题。Strumbelj 等⼈(2014)[34] 提出了蒙特卡洛抽样的近似值：ˆϕj = 1MM∑m=1(ˆf(xm+j) −ˆf(xm−j))其中ˆf(xm+j) 是x 的预测，除了特征j 的值，其他不在联盟内的特征值被来⾃随机数据点z 的特征值替换。x 向量xm−j 与xm+j ⼏乎相同，但被随机数据点z 的特征值替换的特征包括特征j。每⼀个新实例都是⼀种“弗兰肯斯坦怪物”，由两个实例组合⽽成。单个特征值的近似Shapley 估计迭代次数M，关注的实例x，特征索引j，数据矩阵X 和机器学习模型f 实例x 的第j 个特征值的Shapleym ←1 to N 从数据矩阵X 中随机抽取实例z ⽣成特征的随机顺序o 随机顺序实例x：xo =(x(1), . . . , x(j), . . . , x(p)) 随机顺序实例z：zo = (z(1), . . . , z(j), . . . , z(p)) 构造两个新实例有特征j：x+j = (x(1), . . . , x(j−1), x(j), z(j+1), . . . , z(p)) 没有特征j：x−j = (x(1), . . . , x(j−1), z(j), z(j+1), . . . , z(p))计算边际贡献：ϕmj = ˆf(x+j) −ˆf(x−j) 计算平均值作为Shapley 值：ϕj(x) =1M∑Mm=1 ϕmj⾸先，选择感兴趣的实例x，特征j 和迭代次数M。对于每个迭代，从数据中选择⼀个随机实例z，并⽣成特征的随机顺序。通过组合感兴趣的实例x 和样本z 的值来创建两个新实例。第⼀个实例x+j 是感兴趣的实例，但是之前的所有值(包括特征j 的值) 都被样本z 中的特征值替代。第⼆个实例x−j 相似，但是所有值均按之前的顺序排列，但不包括⽤样本z 中的特征j 的值替换了特征j 的值。从⿊盒中预测的差异计算出来：ϕmj = ˆf(xm+j) −ˆf(xm−j)所有这些差异取平均，结果如下：ϕj(x) = 1MM∑m=1ϕmj通过x 的概率分布隐式地对样本进⾏平均。必须对每个特征重复此过程，才能获得所有的Shapley 值。5.8.4优点预测值与平均预测值之间的差异在实例的特征值(即Shapley 值的效益性) 之间公平分配。此性质将Shapley 值与其他⽅法(例如LIME) 区分开来。LIME 不保证预测在特征之间公平分配。Shapley 值可能是提供完整解释的唯⼀⽅法。在法律要求可解释性的情况下(例如欧盟的“解释权”)，Shapley', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 164, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法158值可能是唯⼀合法的⽅法，因为它基于扎实的理论并公平地分配效应。我不是律师，所以这仅反映了我对要求的直觉。Shapley 值允许进⾏对比性解释。⽆需将预测与整个数据集的平均预测进⾏⽐较，你可以将其与⼦集甚⾄单个数据点进⾏⽐较。这种对⽐也是像LIME 这样的局部模型所不具备的。Shapley 值是唯⼀具有扎实理论的解释⽅法。公理——效益性，对称性，虚拟性，可加性——为解释提供了合理的基础。诸如LIME 之类的⽅法在局部假设了机器学习模型的线性⾏为，但是尚⽆关于为何应起作⽤的理论。将预测解释为特征值所玩的游戏真是令⼈难以置信。5.8.5缺点Shapley 值需要大量的计算时间。在99.9% 的实际问题中，只有近似解决⽅案是可⾏的。Shapley值的精确计算在计算上成本很⾼，因为有2p 个必须通过绘制随机实例来模拟特征值的可能联盟和特征的“不存在”，这会增加Shapley 值估计的估计⽅差。联盟的指数数量可以通过对联盟进⾏采样并限制迭代次数M 来处理。减少M 会减少计算时间，但会增加Shapley 值的⽅差。对于迭代次数M 没有很好的经验法则。M 应该⾜够⼤以准确地估计Shapley 值，但是应该⾜够⼩以在合理的时间内完成计算。应该有可能基于Chernoff 边界选择M，但是我还没有看到关于Shapley 值进⾏机器学习预测的⽂章。Shapley 值可能会被误解。特征值的Shapley 值不是从模型训练中删除特征后的预测值之差。Shap-ley 值的解释是：给定当前的⼀组特征值，特征值对实际预测值与平均预测值之差的贡献就是估计的Shapley 值。如果你寻求稀疏解释(包含很少特征的解释)，则Shapley 值是错误的解释⽅法。使⽤Shapley 值⽅法创建的解释始终使用所有特征。⼈类更喜欢选择性的解释，例如LIME 提出的解释。LIME 可能是⾮专业⼈⼠必须处理的更好的选择。另⼀个解决⽅案是Lundberg 和Lee (2016)[35] 提出的SHAP，它基于Shapley 值，但也提供了很少的特征解释。Shapley 值为每个特征返回⼀个简单值，但没有像LIME 这样的预测模型。这意味着它不能⽤于对输⼊变化的预测做出变化的陈述，例如：“如果我每年多赚300 欧元，我的信⽤评分将提⾼5 点”。另⼀个缺点是，如果要计算新数据实例的Shapley 值，则需要访问数据。访问预测函数还不够，因为你需要数据以随机抽取的数据实例中的值替换感兴趣的实例的某些部分。只有在你可以创建看起来像真实数据实例但不是训练数据中实际实例的数据实例时，才能避免这种情况。与许多其他基于置换的解释⽅法⼀样，Shapley 值⽅法在特征相关时会遇到不现实的数据实例。为了模拟联盟中缺少特征值，我们将特征边缘化。这是通过从特征的边缘分布中采样值来实现的。只', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 165, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法159要特征是独⽴的就可以。当特征依赖时，我们可能会采样对该实例没有意义的特征值。但是我们将使⽤它们来计算特征的Shapley 值。据我所知，尚⽆关于Shapley 值含义的研究，也未提出解决⽅法。⼀种解决⽅案可能是将相关特征置换在⼀起，并为其获取⼀个相互的Shapley 值。或者，调整采样程序以考虑特征的依赖性。5.8.6软件和替代方法Shapley 值在R iml 包中实现。SHAP 是Shapley 值的另⼀种估算⽅法，将在下⼀节中介绍。另⼀⽅法称为breakDown，其在R breakDown 包[36] 中实现。BreakDown 还显⽰了每个特征对预测的贡献，但会逐步进⾏计算。让我们重⽤游戏类⽐：我们从⼀个空团队开始，添加对预测贡献最⼤的特征值，然后迭代直到添加所有特征值。每个特征值贡献多少取决于“团队” 中已经存在的各个特征值，这是BreakDown ⽅法的主要缺点。它⽐Shapley 值⽅法快，并且对于没有交互作⽤的模型，结果是相同的。5.9SHAPLundberg 和Lee (2016)[35] 的SHAP (SHapley Additive ExPlanations) 是⼀种解释个体预测的⽅法。SHAP 基于博弈论上的最佳Shapley 值。SHAP 拥有⾃⼰的⼀节，⽽不是Shapley 值的⼦节有两个原因。⾸先，SHAP 的作者提出了Ker-nelSHAP，这是⼀种基于核的代理⽅法，可根据局部代理模型对Shapley 值进⾏估算。他们提出了TreeSHAP，这是⼀种基于树的模型的有效估计⽅法。其次，SHAP 带有许多基于Shapley 值聚合的全局解释⽅法。本节介绍了新的估计⽅法和全局解释⽅法。我建议先阅读有关Shapley 值和局部模型(LIME) 的⼩节内容。5.9.1定义SHAP 的⽬标是通过计算每个特征对预测x 的贡献来解释实例x 的预测。SHAP 解释⽅法根据联盟博弈理论计算Shapley 值。数据实例的特征值充当联盟中的参与者。Shapley 值告诉我们如何在特征之间平均分配“总⽀出” (即预测)，玩家可以是单个特征值，例如表格数据。玩家也可以是⼀组特征值。例如，为了解释图像，可以将像素分组为超像素，并将预测分布在超像素之间。SHAP 带来的⼀项创新是，Shapley 值的解释表⽰为⼀种可加的特征归因⽅法，即线性模型。这将LIME 和', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 166, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法160Shapley 值联系起来。SHAP 将解释指定为：g(z′) = ϕ0 +M∑j=1ϕjz′j其中g 是解释模型，z′ ∈{0, 1}M 是联盟向量，M 是最⼤联盟⼤⼩，ϕj ∈R 是特征j 的特征归因Shapley 值。在SHAP 论⽂中，我所谓的“联盟向量” 被称为“简化特征”。我认为这个名称是因为例如在图像数据中，图像未在像素级别上表⽰，⽽是聚合为超像素。我认为考虑z′ 来描述联盟是有帮助的：在联盟向量中，输⼊1 表⽰相应的特征值“存在”，⽽输⼊0 则表⽰“不存在”。如果你知道Shapley 值，这应该听起来很熟悉。为了计算Shapley 值，我们模拟了只有⼀些特征值在游戏(“存在”)，⽽有些则没有(“不存在”)。联盟线性模型的表⽰是计算ϕ 的技巧。对于x (感兴趣的实例)，联盟向量x′ 是全为1 的向量，即所有特征值均为“存在”。该公式简化为：g(x′) = ϕ0 +M∑j=1ϕj你可以在Shapley 值⼀节中以类似的符号找到此公式。有关实际估计的更多信息将在稍后描述。让我们先谈谈ϕ 的性质，然后再讨论它们的估计细节。Shapley 值是唯⼀满⾜效益性、对称性、虚拟性和可加性的解。SHAP 也满⾜这些要求，因为它可以计算Shapley 值。在SHAP 论⽂中，你会发现SHAP 性质和Shapley 性质之间的差异。SHAP描述了以下三个理想的性质：1) 局部准确性(Local accuracy)f(x) = g(x′) = ϕ0 +M∑j=1ϕjx′j表⽰特征归因的总和等于我们要解释的模型的输出。如果定义ϕ0 = EX( ˆf(x)) 并将所有x′j 都设置为1，则这是Shapley 效益性。仅使⽤不同的名称并使⽤联盟向量。f(x) = ϕ0 +M∑j=1ϕjx′j = EX( ˆf(X)) +M∑j=1ϕj2) 缺失性(Missingness)x′j = 0 ⇒ϕj = 0缺失性表⽰缺失特征的归因为零。请注意，x′j 表⽰联盟，其中值0 表⽰不存在特征值。在联盟表⽰法中，要解释的实例的所有特征值x′j 均应为“1”。0 的存在将意味着感兴趣的实例缺少特征值。此性质不在“常规” Shapley 值的性质中。那么，为什么我们需要SHAP？Lundberg 称其为“⼩型簿记财产” (Minor Book-keeping Property)。从理论上讲，缺失的特征可以具有任意的Shapley 值，⽽不会损害局部准确性，因为它与x′j = 0 相乘。缺失性强制缺少的特征获得Shapley 值0。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 167, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1613) 一致性(Consistency)让fx(z′) = f(hx(z′)) 且z\\\\j′ 表明z′j = 0。对于任意两个模型f 和f′ 满⾜，对所有输⼊z′ ∈{0, 1}M：f′x(z′) −f′x(z′\\\\j) ≥fx(z′) −fx(z′\\\\j)则有：ϕj(f′, x) ≥ϕj(f, x)⼀致性表⽰，如果模型发⽣变化，以使特征值的边际贡献增加或保持不变(不考虑其他特征)，则Shapley 值也会增加或保持不变。从⼀致性中得出Shapley 性质的可加性，虚拟性和对称性，如Lundberg 和Lee 的附录中所述。5.9.2KernelSHAPKernelSHAP 为⼀个实例x 估算每个特征值对预测的贡献。KernelSHAP 包含5 个步骤：• 采样联盟z′k ∈{0, 1}M, k ∈{1, . . . , K} (1 = 联盟中存在特征，0 = 不存在特征)。• 对z′k 预测，⽅法是⾸先将z′k 转换为原始特征空间，然后应⽤模型f：f(hx(z′k))。• 使⽤SHAP 核计算每个z′k 的权重。• 拟合加权线性模型。• 返回Shapley 值ϕk，即线性模型的系数。我们可以通过重复掷硬币来建⽴随机联盟，直到我们得到⼀个由0 和1 组成的链。例如，向量(1,0, 1, 0) 意味着我们具有第⼀个和第三个特征的联盟。K 个采样的联盟成为回归模型的数据集。回归模型的⽬标是联盟的预测。(“等等！”，你可能说，“该模型尚未针对这些⼆进制的联盟数据进⾏训练，因此⽆法对其进⾏预测。”) 为了从特征值的联盟转化为有效的数据实例，我们需要⼀个函数hx(z′) = z 其中hx : {0, 1}M →Rp。函数hx 将1 映射到我们要解释的实例x 的对应值。对于表格数据，它将0 映射到我们从数据中采样的另⼀个实例的值。这意味着我们将“缺少特征值” 等同于“将特征值替换为数据中的随机特征值”。对于表格数据，下图显⽰了从联盟到特征值的映射：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 168, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法162图5.42. 函数hx 将联盟映射到有效实例。对于当前特征(1)，hx 映射到x 的特征值。对于缺少特征(0)，hx 映射到随机采样的数据实例的值。在理想情况下，hx 会根据当前特征值对不存在的特征值进⾏采样：f(hx(z′)) = EXC|XS[f(x)|xS]其中XC 是缺少的特征集，XS 是当前的特征集。但是，如上所述，⽤于表格数据的hx 将XC 和XS 视为独⽴的，并在边际分布上进⾏积分：f(hx(z′)) = EXC[f(x)]从边际分布采样意味着忽略当前特征和不存在特征之间的依赖关系。因此，KernelSHAP 与所有基于置换的解释⽅法都存在相同的问题。该估计对不太可能发⽣的情况给予了过多的重视。结果可能变得不可靠。稍后我们将看到，树集成算法TreeSHAP 不受此问题的影响。对于图像，下图描述了可能的映射函数：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 169, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法163图5.43. 函数hx 将超像素(sp) 的联盟映射到图像。超像素是像素组。对于当前特征(1)，hx 返回原始图像的相应部分。对于不存在的特征(0)，hx 将使相应区域变灰。变为周围像素或类似像素的平均颜⾊也是⼀种选择。与LIME 的最⼤区别在于回归模型中实例的权重。LIME 根据实例与原始实例的接近程度对其进⾏加权。联盟向量中的0 越多，LIME 中的权重就越⼩。SHAP 根据联盟在Shapley 值估计中获得的权重对采样实例进⾏加权。⼩型联盟(少量1) 和⼤型联盟(即许多1) 获得最⼤权重。它的直觉是：如果我们可以单独研究它们的影响，我们将最⼤程度地了解单个特征。如果⼀个联盟由⼀个特征组成，那么我们可以了解特征对预测的孤⽴的主要效应。如果⼀个联盟只少了⼀个特征，那么我们可以了解这个特征的总效应(主要效应以及特征交互)。如果⼀个联盟由⼀半特征组成，那么我们对单个特征的贡献知之甚少，因为存在许多可能具有⼀半特征的联盟。为了达到Shapley 标准的加权，Lundberg 等提出了SHAP 核：πx(z′) =(M −1)( M|z′|)|z′|(M −|z′|)这⾥，M 是最⼤联盟⼤⼩，|z′| 实例z′ 中当前特征的数量。Lundberg 和Lee 表明，⽤这个核权重的线性回归会产⽣Shapley 值。如果你将SHAP 核与LIME ⼀起⽤于联盟数据，则LIME 还将估算Shapley 值！对于联盟的采样，我们可以更聪明⼀些：最⼩的联盟和最⼤的联盟会承担⼤部分权重。通过使⽤采样预算K 包括这些⾼权重联盟，⽽不是盲⽬采样，我们可以获得更好的Shapley 值估计。我们从具有1 和M −1 特征的所有可能的联盟开始，这总共有2M。当我们有⾜够的预算(当前预算为', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 170, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法164K −2M) 时，我们可以包括具有2 个特征以及具有M −2 特征的联盟，等等。从剩余的联盟规模中，我们使⽤调整后的权重进⾏采样。现在我们有数据、⽬标和权重。从⽽建⽴加权线性回归模型：g(z′) = ϕ0 +M∑j=1ϕjz′j我们通过优化以下损失函数L 来训练线性模型g：L(f, g, πx) =∑z′∈Z[f(hx(z′)) −g(z′)]2πx(z′)其中Z 是训练数据。这是我们通常对线性模型进⾏优化的误差平⽅和。模型的估计系数ϕj 是Shapley 值。由于我们处于线性回归设置中，因此我们也可以使⽤标准⼯具进⾏回归。例如，我们可以添加正则化项以使模型稀疏。如果在损失L 上加上L1 惩罚，我们可以创建稀疏的解释。(我不确定所得出的系数是否仍然是有效的Shapley 值。)5.9.3TreeSHAPLundberg 等⼈(2018)[37] 提出了TreeSHAP，这是SHAP 的⼀种变体，⽤于基于树的机器学习模型，例如决策树、随机森林和梯度提升树。TreeSHAP 速度很快，可以计算精确的Shapley 值，并且在特征相关时可以正确估计Shapley 值。相⽐之下，KernelSHAP 的计算成本很⾼，⽽且只能近似实际的Shapley 值。TreeSHAP 快多少？对于精确的Shapley 值，它将计算复杂度从O(TL2M) 降低到了O(TLD2)，其中T 是树的数量，L 是所有树中的最⼤叶⼦数量，D 是所有树的最⼤深度。TreeSHAP 估计正确的条件期望EXS|XC[f(x)|xS]。我将为你提供⼀些直觉，我们如何计算单个树，实例x 和特征⼦集S 的期望预测。如果我们以所有特征为条件——如果S 是所有特征的集合——那么来⾃实例x 所在节点的预测将是期望预测。如果我们对任何特征都没有条件——如果S 为空——我们将使⽤所有终端节点的预测的加权平均值。如果S 包含⼀些但不是全部特征，则我们将忽略不可到达节点的预测。不可到达意味着通向该节点的决策路径与xS 中的值相⽭盾。在剩余的终端节点中，我们根据节点⼤⼩(即该节点中训练样本的数量) 对预测结果进⾏加权平均。剩余终端节点的平均值(由每个节点的实例数加权) 是给定S 时x 的期望预测。问题是我们必须对特征值的每个可能⼦集S 应⽤此过程。幸运的是，TreeSHAP 的计算时间是多项式级⽽不是指数级。基本思想是同时将所有可能的⼦集S 推到树上。对于每个决策节点，我们必须跟踪⼦集的数量。这取决于⽗节点中的⼦集和分割特征。例如，当树中的第⼀个分割基于特征x3，则包含特征x3 的所有⼦集都将到达⼀个节点(x 所⾛的节点)。不包含特征x3 的⼦集伴随权重降低同时到达两边节点。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 171, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法165不幸的是，不同⼤⼩的⼦集具有不同的权重。该算法必须跟踪每个节点中⼦集的总体权重。这使算法变得复杂。有关TreeSHAP 的详细信息，请参阅原始论⽂。可以将计算扩展到更多的树：由于Shapley 值的可加性，树集成的Shapley 值是各个树的Shapley 值的(加权平均值)。接下来，我们将看⼀下SHAP 的解释。5.9.4示例我⽤100 棵树训练了⼀个随机森林分类器，预测宫颈癌的风险。我们将使⽤SHAP 来解释个体预测。我们可以使⽤快速TreeSHAP 估计⽅法⽽不是较慢的KernelSHAP ⽅法，因为随机森林是树的集成。由于SHAP 计算Shapley 值，因此其解释与Shapley 值⼀节中的解释相同。但是，随着Pythonshap 软件包带来了不同的可视化效果：你可以将诸如Shapley 值之类的特征归因可视化为“⼒”(force)。每个特征值都是增加或减少预测的⼒。预测从基线开始。Shapley 值的基线是所有预测的平均值。在图中，每个Shapley 值都是⼀个箭头，可推动增加(正值) 或减⼩(负值) 预测。这些⼒在数据实例的实际预测中相互平衡。下图显⽰了宫颈癌数据集中两名⼥性的SHAP 解释⼒图：图5.44. SHAP 值可以解释两个⼈的预测癌症概率。基线(平均预测概率) 为0.066。第⼀位⼥性有低预测风险0.06。诸如性病(STDs) 之类的风险增加的效应被诸如年龄(Age) 之类的减少的效应所抵消。第⼆名⼥性的预测风险很⾼，为0.71。51 岁和吸烟34 年增加了她预测患癌症的风险。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 172, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法166这些是对单个预测的解释。Shapley 值可以合并为全局解释。如果我们为每个实例运⾏SHAP，则将获得Shapley 值矩阵。此矩阵每个数据实例具有⼀⾏，每个特征具有⼀列。我们可以通过分析此矩阵中的Shapley 值来解释整个模型。现在我们再开始介绍SHAP 特征重要性。5.9.5SHAP 特征重要性SHAP 特征重要性背后的想法很简单：具有较⼤的Shapley 绝对值的特征很重要。由于我们需要全局重要性，因此我们在数据中对每个特征的Shapley 绝对值取平均值：Ij =n∑i=1|ϕ(i)j |接下来，我们通过重要性对特征进⾏降序排序并绘制它们。下图显⽰了SHAP 特征对于预测宫颈癌的经过训练的随机森林的SHAP 特征重要性。图5.45.SHAP 特征重要性的度量是平均Shapley 绝对值。使⽤激素类避孕药的年数是最重要的特征，平均将预测的癌症绝对概率改变了2.4 个百分点(x 轴为0.024)。SHAP 特征重要性是置换特征重要性的替代⽅法。两种重要性度量之间存在很⼤差异：置换特征重要性基于模型性能的下降。SHAP 基于特征归因的⼤⼩。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 173, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法167特征重要性图很有⽤，但是除了重要性之外不包含任何信息。要获得更多信息，我们接下来将看⼀下摘要图。5.9.6SHAP 概要图概要图将特征重要性与特征效应结合在⼀起。概要图上的每个点都是⼀个特征和⼀个实例的Shap-ley 值。y 轴上的位置由特征确定，x 轴上的位置由Shapley 值确定。颜⾊代表特征值从低到⾼。重叠点在y 轴⽅向上抖动，因此我们可以了解每个特征的Shapley 值的分布。这些特征根据其重要性排序。图5.46. SHAP 概要图。使⽤激素类避孕药的年数越少，患癌症的风险越低；年数越多，患癌症的风险越⾼。注意：所有效应都描述了模型的⾏为，在现实世界中不⼀定是因果关系。在概要图中，我们⾸先看到特征值与对预测的影响之间的关系。但要了解这种关系的确切形式，我们必须查看SHAP 依赖图。5.9.7SHAP 依赖图SHAP 特征依赖关系可能是最简单的全局解释图：1. 选择⼀个特征。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 174, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1682. 对于每个数据实例，绘制⼀个点，在x 轴上是特征值，在y 轴上是相应的Shapley 值。3. 完成。从数学上讲，该图包含以下点：{(x(i)j , ϕ(i)j )}ni=1下图显⽰了对使⽤激素类避孕药的年数的SHAP 特征依赖性：图5.47.使⽤激素类避孕药的年数的SHAP 依赖图。与0 年相⽐，使⽤激素类避孕药的年数少会降低预测的概率，⽽⾼的年数会增加预测的癌症概率。SHAP 依赖图是部分依赖图和累积局部效应图的替代⽅法。PDP 和ALE 图显⽰平均效应，⽽SHAP 依赖性还显⽰y 轴的⽅差。特别是在交互作⽤的情况下，SHAP 依赖图在y 轴上更加分散。突出显⽰这些特征交互，可以改进依赖图。5.9.8SHAP 交互值交互效应是在考虑了单个特征效应之后的附加的组合特征效应。博弈论中的Shapley 交互指数定义为：ϕi,j =∑S⊆\\\\{i,j}|S|!(M −|S| −2)!2(M −1)!δij(S)当i ̸= j 且：δij(S) = fx(S ∪{i, j}) −fx(S ∪{i}) −fx(S ∪{j}) + fx(S)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 175, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法169该公式减去了特征的主要效应，因此我们在考虑了单个效应后得到了纯交互效应。与Shapley 值计算中⼀样，我们对所有可能的特征联盟S 上的值求平均值。当我们计算所有特征的SHAP 交互值时，每个实例得到⼀个矩阵，维数为M × M，其中M 为特征数量。我们如何使⽤交互指数？例如，下⾯⾃动与交互作⽤最强的特征绘制SHAP 依赖图：图5.48.具有交互可视化的SHAP 特征依赖图。使⽤激素类避孕药的年数与性病相互作⽤。在接近0 年的情况下，性病的发⽣会增加预测的癌症风险。使⽤激素类避孕药多年，性病的发⽣降低了预测的风险。同样，这不是因果模型。效应可能是由于混杂(例如性病和较低的癌症风险可能与更多的就诊机会相关)。5.9.9聚类SHAP 值你可以借助Shapley 值对数据进⾏聚类。聚类的⽬的是找到相似实例的组。通常，聚类是基于特征的。特征通常在不同的尺度上。例如，⾼度可能以⽶为单位，颜⾊强度从0 到100，某些传感器输出在-1 和1 之间。难点在于计算具有这种不同、不可⽐较的特征的实例之间的距离。SHAP 聚类通过对每个实例的Shapley 值进⾏聚类来⼯作。这意味着你可以通过解释相似性来聚类实例。所有SHAP 值都具有相同的单位——在预测空间的单位。你可以使⽤任何聚类⽅法。以下⽰例使⽤层次聚类对实例进⾏排序。该图由许多⼒图组成，每个⼒图都解释了实例的预测。我们垂直旋转⼒图，并根据它们的聚类相似性将它们并排放置。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 176, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法170图5.49.堆叠的SHAP 解释按解释相似性聚类。x 轴上的每个位置都是数据的⼀个实例。红⾊SHAP 值会增加预测，蓝⾊值会降低预测。聚类突出：右侧是预测癌症风险较⾼的⼈群。5.9.10优点由于SHAP 计算Shapley 值，因此应⽤了Shapley 值的所有优点：SHAP 在博弈论中具有扎实的理论基础。预测结果在特征值中公平分配。我们得到对比性解释，将预测与平均预测进⾏⽐较。SHAP 连接LIME 和Shapley 值。这对于更好地了解这两种⽅法⾮常有⽤。它还有助于统⼀可解释机器学习的领域。SHAP 可以快速实现基于树的模型。我认为这是SHAP 普及的关键，因为采⽤Shapley 值的最⼤障碍是计算速度慢。快速的计算使计算全局模型解释所需的许多Shapley 值成为可能。全局解释⽅法包括特征重要性，特征依赖，交互作⽤，聚类和摘要图。使⽤SHAP，全局解释与局部解释⼀致，因为Shapley 值是全局解释的“原⼦单位”。如果将LIME ⽤于局部解释，将部分依赖图和置换特征重要性⽤于全局解释，则你会缺乏通⽤的基础。5.9.11缺点KernelSHAP 很慢。当你要为许多实例计算Shapley 值时，这使得使⽤KernelSHAP 不切实际。同样，所有全局SHAP ⽅法(例如SHAP 特征重要性) 都需要为许多实例计算Shapley 值。KernelSHAP 忽略特征相关性。⼤多数其他基于置换的解释⽅法都存在此问题。通过⽤随机实例中的值替换特征值，通常更容易从边缘分布中随机采样。但是，如果特征是相关的，这就会导致把过多的权重放在不太可能的数据点上。TreeSHAP 通过显式建模条件期望预测来解决此问题。Shapley 值的缺点也适⽤于SHAP：Shapley 值可能会被错误解释，并且需要访问数据才能为新数据计算SHAP 值(TreeSHAP ⽅法除外)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 177, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第五章模型无关方法1715.9.12软件作者在Python SHAP 包中实现了SHAP。此实现适⽤于Python 的scikit-learn 机器学习库中的基于树的模型。shap 包也⽤于本章中的⽰例。SHAP 已集成到树增强框架XGBoost 和LightGBM中。在R 中，有⼀个shapper 包。SHAP 也包含在R xgboost 软件包中。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 178, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释基于样本的解释⽅法(Example-based Explanations) 选择数据集的特定实例来解释机器学习模型的⾏为或解释底层数据分布。基于样本的解释⼤多与模型⽆关，因为它们使任何机器学习模型都更具可解释性。与模型⽆关的⽅法的不同之处在于，基于样本的⽅法通过选择数据集的实例⽽不是通过创建特征概要(例如特征重要性或部分依赖性) 来解释模型。只有当我们可以以⼈类可以理解的⽅式表⽰数据实例时，基于样本的解释才有意义。这对于图像⾮常有效，因为我们可以直接查看它们。通常，如果实例的特征值包含更多的上下⽂，那么基于样本的⽅法就会很好地⼯作，这意味着数据具有像图像或⽂本⼀样的结构。以⼀种有意义的⽅式表⽰表格数据更具挑战性，因为⼀个实例可以包含数百或数千个(较少结构化的) 特征。列出所有的特征值来描述⼀个实例通常是没有⽤的。如果只有少数⼏个特征，或者我们有⼀种⽅法来总结⼀个实例，那么它可以很好地⼯作。基于样本的解释可帮助⼈们构建机器学习模型以及机器学习模型所训练的数据的⼼理模型。它特别有助于理解复杂的数据分布。但是，基于样本的解释是什么意思？我们经常在⼯作和⽇常⽣活中使⽤它们。让我们从⼀些⽰例开始[38]。医师看到病⼈出现异常咳嗽和轻度发烧。病⼈的症状使她想起了她多年前遇到的患有类似症状的另⼀位病⼈。她怀疑她⽬前的病⼈可能患有相同的疾病，并且她采集了⾎液样本以测试这种检测疾病。⼀位数据科学家正在为他的⼀位客户做⼀个新项⽬：分析导致键盘⽣产机器故障的风险因素。数据科学家记得他曾经做过的⼀个类似项⽬，并且重⽤了旧项⽬中的部分代码，因为他认为客户希望进⾏同样的分析。⼀只⼩猫坐在⼀个失⽕的⽆⼈居住的房⼦的窗台上。消防部门已经到达，其中⼀名消防队员正在考虑他是否可以冒险进⼊⼤楼救⼩猫。他还记得⾃⼰当消防员时遇到的类似情况：缓慢燃烧了⼀段时间的⽼⽊屋往往不稳定，最终倒塌。由于这种情况的相似性，他决定不进⼊，因为房屋倒塌的风险太⼤。幸运的是，猫咪跳出窗户，安全着陆，没有⼈受伤。这是个美满结局。这些故事说明了我们⼈类在样例或类⽐中是如何思考的。基于样本的解释的蓝图是：事物B 与事物A 类似，事物A 导致Y，因此我预测事物B 也将引起Y。隐式地，⼀些机器学习⽅法是基于样本的。决策树根据对预测⽬标很重要的特征(数据点的相似性) 将数据划分为节点。决策树通过查172', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 179, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释173找相似的实例(= 在相同的终端节点中) 并返回这些实例结果的平均值作为预测来获取新数据实例的预测。k-最近邻(knn) ⽅法可以显式地处理基于样本的预测。对于⼀个新实例，knn 模型可以找到k 个最近的邻居(例如，k=3 个最近的实例)，并返回这些邻居结果的平均值作为预测。可以通过返回k 个邻居来解释knn 的预测，这同样仅当我们有⼀个好⽅法表⽰单个实例时才有意义。本部分的各节涵盖以下基于样本的解释⽅法：• 反事实解释告诉我们实例必须如何改变才能显着改变其预测。通过创建反事实实例，我们了解模型如何做出预测并可以解释各个预测。• 对抗样本是⽤来欺骗机器学习模型的反事实。重点是翻转预测⽽不是解释它。• 原型是从数据中选择具有代表性的实例，⽽批评是那些原型⽆法很好地表⽰的实例。[4]• 有影响力的实例是对预测模型的参数或预测本⾝影响最⼤的训练数据点。识别和分析有影响⼒的实例有助于发现数据问题、调试模型并更好地了解模型的⾏为。• k-最近邻模型是基于样本的(可解释的) 机器学习模型。6.1反事实解释反事实解释(Counterfactual Explanations) 按以下形式描述了⼀种因果关系：“如果没有发⽣X，那么Y 就不会发⽣”。例如：“如果我不喝⼀杯热咖啡，我的⾆头就不会烫伤了”。事件Y 是烫伤了我的⾆头；原因X 是我喝了热咖啡。思考反事实需要想象⼀个与所观察到的事实相⽭盾的假设现实(例如，⼀个我没有喝热咖啡的世界)，因此被称为“反事实”。在反事实中进⾏思考的能⼒使我们⼈类⽐其他动物更加聪明。在可解释的机器学习中，反事实解释可⽤于解释各个实例的预测。“事件” 是实例的预测结果，“原因” 是该实例的特定特征值，将其输⼊到模型并会“引起” 某个预测。以图的形式显⽰，输⼊和预测之间的关系⾮常简单: 特征值导致预测。图6.1. 当模型仅被视为⼀个⿊盒时，机器学习模型输⼊与预测之间的因果关系。输⼊导致预测(不⼀定反映数据的真正因果关系)。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 180, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释174即使实际上输⼊与要预测的结果之间的关系可能不是因果关系，我们也可以将模型的输⼊视为预测的原因。有了这个简单的图，就很容易看出我们如何模拟反事实来预测机器学习模型：我们只是在做出预测之前改变实例的特征值，然后分析预测是如何变化的。我们对预测以相关⽅式发⽣变化的场景感兴趣，例如预测类别发⽣翻转(例如接受或拒绝信贷申请) 或预测达到某个阈值(例如癌症的概率达到10%)。预测的反事实解释描述了将预测更改为预定义输出时特征值的最小变化。反事实解释⽅法与模型⽆关，因为它仅适⽤于模型输⼊和输出。由于该解释可以表⽰为特征值差异的概要(“更改特征A 和B 以更改预测”)，因此该⽅法在模型⽆关的章节中也很适⽤。但是反事实解释本⾝就是⼀个新实例，因此它存在于本章中(“从实例X 开始，改变A 和B 以得到⼀个反事实实例”)。与原型不同，反事实不⼀定是来⾃训练数据的实际实例，⽽可以是特征值的新组合。在讨论如何创建反事实之前，我想讨论⼀些反事实的⽰例以及⼀个好的反事实解释是怎样的。在第⼀个⽰例中，Peter 申请了⼀笔贷款，并被(基于机器学习的) 银⾏软件拒绝了。他想知道为什么他的申请被拒绝，以及他怎样才能提⾼获得贷款的机会。“为什么” 的问题可以表述为反事实：对特征(收⼊，信⽤卡数量，年龄等) 的最⼩变化是什么，可以使预测从拒绝变为批准？⼀个可能的答案是：如果Peter 每年能多赚10,000 欧元，他将获得贷款。或者，如果Peter 的信⽤卡较少，并且5 年前没有拖⽋贷款，那么他会得到贷款。Peter 永远不会知道拒绝的原因，因为银⾏对透明度没有兴趣，但这是另⼀回事。在我们的第⼆个⽰例中，我们想⽤反事实解释来解释⼀个预测连续结果的模型。Anna 想把她的公寓租出去，但她不确定要收取多少费⽤，因此她决定训练⼀个机器学习模型来预测租⾦。当然，由于Anna 是⼀位数据科学家，因此她可以解决⾃⼰的问题。输⼊有关⾯积⼤⼩、位置、是否允许携带宠物等的所有详细信息之后，模型告诉她可以收取900 欧元。她期望1000 欧元或更多，但是她相信⾃⼰的模型，并决定使⽤公寓的特征值了解如何提⾼公寓的价格。她发现，如果公寓⾯积再⼤15 平⽅⽶，则可以以超过1000 欧元的价格出租。有趣，但不可⾏，因为她⽆法扩⼤⾃⼰的公寓。最后，通过仅在其控制下调整特征值(内置厨房是/否，允许宠物是/否，地板类型等)，她发现如果允许宠物并安装隔热效果更好的窗户，她可以收取1000 欧元。Anna 凭直觉与反事实合作来改变结果。反事实是对⼈类友好的解释，因为它们与当前实例形成对⽐，并且它们是选择性的，这意味着它们通常专注于少量特征更改。但是反事实却遭受“罗⽣门效应” 的困扰。《罗⽣门》是⼀部⽇本电影，讲述了不同的⼈对⼀个武⼠的谋杀。每个故事都很好地解释了结果，但故事彼此⽭盾。反事实也可能发⽣同样的情况，因为通常会有多种不同的反事实解释。每个反事实都讲述了如何达到特定结果的不同“故事”。⼀个反事实可能会说要更改特征A，另⼀反事实可能会说要保留A 不变但要更改特征B，这是⽭盾的。可以通过报告所有反事实解释或通过制定标准评估反事实并选择最佳的反事实来解决这个多重反事实问题。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 181, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释175说到标准，我们如何定义⼀个好的反事实解释？⾸先，反事实解释的⽤户定义了实例预测中⼀个相关的更改，因此显⽽易见的第⼀个要求是，反事实实例应尽可能紧密地产生预定义的预测。并⾮总是可以完全匹配预定义的输出。在具有两个类(罕见类和频繁类) 的分类设置中，模型可以始终将实例分类为频繁类。更改特征值以使预测的标签从频繁类别转换为罕见类别可能是困难的。因此，我们希望放宽反事实的预测输出必须与定义的结果完全对应的要求。在分类⽰例中，我们可以寻找⼀个反事实，罕见类别的预测概率增加到10%，⽽不是当前的2%。问题是，为了使预测概率从2%变为10% (或接近10%)，特征的最⼩变化是什么？另一个标准是反事实应该与特征值实例尽可能相似。这需要两个实例之间的距离度量。反事实不仅应接近原始实例，⽽且还应更改尽可能少的特征。这可以通过选择适当的距离度量来实现，⽐如曼哈顿距离。最后⼀个要求是，反事实实例应具有可能的特征值。对于房租为负数或房间数设置为200 的租房⽰例，反事实解释是没有意义的。如果根据数据的联合分布产⽣反事实则更好，例如，⼀个有10 个房间和20 平⽅⽶的公寓不应视为反事实解释。6.1.1生成反事实解释⼀种简单的产⽣反事实解释的⽅法是通过反复试验进⾏搜索。此⽅法涉及随机改变感兴趣实例的特征值，并在预期输出时停⽌。就像Anna 想找⼀套她可以收取更⾼租⾦的公寓。但是有⽐反复试验更好的⽅法。⾸先，我们定义⼀个损失函数，该函数将感兴趣的实例、反事实和期望的(反事实)结果作为输⼊。损失度量反事实的预测结果与预定义结果之间的距离，以及反事实与感兴趣实例之间的距离。我们可以使⽤优化算法直接优化损失，也可以通过在实例周围进⾏搜索来优化损失，如“Growing Spheres” ⽅法(参见软件和替代⽅法)。在本节中，我将介绍Wachter 等⼈(2017)[39] 建议的⽅法。他们建议尽量减少以下损失。L(x, x′, y′, λ) = λ · ( ˆf(x′) −y′)2 + d(x, x′)第⼀项是反事实x′ 的模型预测与期望结果y′ 之间的⼆次距离，⽤户必须事先定义。第⼆项是要解释的实例x 与反事实x′ 之间的距离d，稍后将对此进⾏详细说明。参数λ 平衡预测距离(第⼀项)与特征值距离(第⼆项)。对于给定的λ 求解损失，并返回反事实x′。较⾼的λ 表⽰我们更喜欢接近期望结果y′ 的反事实，较低的值意味着我们更喜欢与特征值中的x ⾮常相似的反事实x′。如果λ ⾮常⼤，则将选择预测最接近y′ 的实例，⽽不管其与x 的距离如何。最终，⽤户必须决定如何在对反事实的预测与期望结果匹配的要求和对反事实类似于x 的要求之间取得平衡。该⽅法的作者建议不要选择λ 的值⽽是选择允许反事实实例的预测与y′ 相距多远的公差ϵ。该约束可以写为：| ˆf(x′) −y′| ≤ϵ为了使该损失函数最⼩化，可以使⽤任何合适的优化算法，例如Nelder-Mead。如果可以访问机器学习模型的梯度，则可以使⽤基于梯度的⽅法，例如ADAM。必须预先设置实例x，所需的输出y′', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 182, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释176和公差参数ϵ。对于x′，损失函数最⼩化，并且增⼤λ 找到(局部) 最佳反事实x′，直到找到⾜够接近的解(= 公差参数内)。arg minx′ maxλL(x, x′, y′, λ)⽤于测量实例x 与反事实x′ 之间的距离的函数d 是⽤绝对中位差(MAD) 的倒数加权的曼哈顿距离。d(x, x′) =p∑j=1|xj −x′j|MADj总距离是所有p 个特征距离的总和，即实例x 和反事实x′ 之间的特征值的绝对差。特征距离通过特征j 在数据集上的绝对中位差的倒数来缩放：MADj = mediani∈{1,...,n}(|xi,j −medianl∈{1,...,n}(xl,j)|)向量的中位数是向量值的⼀半⼤于⽽另⼀半⼩于的值。MAD 等效于特征的⽅差，但不是使⽤均值作为中⼼并求平⽅距离之和，⽽是使⽤中位数作为中⼼并求绝对距离之和。提出的距离函数相对于欧⼏⾥得距离具有优势，它引⼊了稀疏性。这意味着当不同的特征较少时，两个点彼此靠近。⽽且对于异常值更鲁棒。使⽤MAD 进⾏缩放对于使所有特征达到相同的⽐例是必要的——⽆论你以平⽅⽶还是平⽅英尺来衡量公寓的⾯积⼤⼩都没有关系。产⽣反事实的⽅法很简单:1. 选择要解释的实例x、所需的结果y′、公差ϵ 和低的λ 初始值。2. 采样⼀个随机的实例作为初始反事实。3. 以初始采样的反事实为出发点，对损失进⾏优化。4. 直到| ˆf(x′) −y′| > ϵ：增加λ。以当前反事实为出发点优化损失。返回最⼩化损失的反事实。5. 重复步骤2-4 并返回反事实列表或最⼩化损失的列表。6.1.2示例这两个⽰例均来⾃Wachter 等⼈(2017) 的⼯作。在第⼀个⽰例中，作者训练了⼀个三层的全连接神经⽹络来预测⼀个学⽣在法学院的第⼀年的平均成绩，基于在法学院之前的平均绩点(GPA)，种族(Race) 和法学院⼊学考试分数(LSAT)。⽬的是为每个学⽣找到反事实的解释，以回答以下问题：如何改变输⼊特征，以使预测分数为0？由于之前已经对分数进⾏了归⼀化，因此分数为0 的学⽣与学⽣的平均⽔平⼀样好。负分数表⽰低于平均⽔平的结果，正分数表⽰⾼于平均⽔平的结果。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 183, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释177下表显⽰了学习到的反事实：ScoreGPALSATRaceGPA x′LSAT x′Race x′0.173.139.003.134.000.543.748.003.732.40-0.773.328.013.333.50-0.832.428.512.435.80-0.572.718.302.734.90第⼀列包含预测分数，接下来三列为原始特征值，最后三列为反事实特征值(可以导致预测分数接近0)。前两⾏是学⽣的平均成绩⾼于平均⽔平，其余三⾏是学⽣低于平均⽔平。前两⾏的反事实描述了如何改变学⽣的特征以降低预测分数，⽽对于其他三种情况，则描述了如何改变以将分数提⾼⾄平均值。增加分数的反事实总是将种族从⿊⾊(⽤1 编码) 更改为⽩⾊(⽤0 编码)，这显⽰了模型的种族偏见。GPA 在反事实⽅⾯没有发⽣变化，但LSAT 发⽣了变化。第⼆个例⼦显⽰了预测糖尿病风险的反事实解释。经过训练的三层全连接神经⽹络可以根据年龄、⾝体质量指数(BMI)、怀孕次数等来预测糖尿病风险。反事实回答了这个问题：必须更改哪些特征值才能将糖尿病的风险评分增加或降低到0.5？发现了以下反事实：• ⼈1：如果你的2 ⼩时⾎清胰岛素⽔平为154.3，则你的得分为0.51• ⼈2：如果你的2 ⼩时⾎清胰岛素⽔平为169.5，则你的得分为0.51• ⼈3：如果你的⾎浆葡萄糖浓度为158.3，⽽你的2 ⼩时⾎清胰岛素⽔平为160.5，则你的得分为0.516.1.3优点反事实解释的解释很清楚。如果实例的特征值根据反事实⽽更改，则预测将更改为预定义的预测。这⾥没有额外的假设，也没有魔法的背景。这也意味着它不像LIME 那样危险，因为我们不清楚我们能在多⼤程度上推断出局部解释模型。反事实⽅法创建了⼀个新实例，但是我们也可以通过报告哪些特征值已改变来总结⼀个反事实。这给了我们两种报告结果的选择。你可以报告反事实实例，也可以突出显⽰在感兴趣的实例和反事实实例之间改变了哪些特征。反事实方法不需要访问数据或模型。它只需要访问模型的预测函数，例如，该函数也可以通过WebAPI 来运⾏。这对于经第三⽅审计或向⽤户提供解释但未披露模型或数据的公司具有吸引⼒。由于商业秘密或数据保护的原因，公司有保护模型和数据的利益。反事实解释在解释模型预测和保护模型所有者的利益之间取得了平衡。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 184, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释178该⽅法还适用于不使用机器学习的系统。我们可以为接收输⼊并返回输出的任何系统创建反事实。预测公寓租⾦的系统也可以包含⼿写规则，反事实解释仍然有效。反事实解释⽅法相对容易实现，因为它本质上是⼀种损失函数，可以使⽤标准优化程序库进⾏优化。必须考虑⼀些其他细节，例如将特征值限制在有意义的范围内(例如，只有正的公寓⾯积⼤⼩)。6.1.4缺点对于每个实例，通常会找到多个反事实的解释(罗生门效应)。这很不⽅便——⼤多数⼈都喜欢简单的解释，⽽不是现实世界的复杂性。这也是⼀个实际的挑战。假设我们为⼀个实例⽣成了23 个反事实解释。我们都报告吗？只选最好的吗？如果它们都是相对“好” 但又⾮常不同怎么办？对于每个项⽬，必须重新回答这些问题。进⾏多种反事实的解释也可能是有利的，因为然后⼈类可以选择与他们先验知识相对应的解释。对于给定的公差ϵ，不能保证找到反事实的实例。那不⼀定是该⽅法的问题，⽽是取决于数据。不能很好地处理具有许多不同级别的分类特征。该⽅法的作者建议针对分类特征的特征值的每种组合分别运⾏该⽅法，但是如果你拥有多个具有多个值的分类特征，这将导致组合爆炸。例如，具有10 个级别的6 个分类特征将意味着进⾏⼀百万次运⾏。Martens 等⼈(2014)[40] 提出了仅针对分类特征的解决⽅案。在Python 的Alibi 包中实现了⼀种解决⽅案，该⽅案可以使⽤产⽣分类变量的扰动的⽅式处理数值变量和分类变量。6.1.5软件和替代方法在Python Alibi 包中实现了对反事实的解释。软件包的作者实现了简单的反事实⽅法，以及使⽤类原型提⾼算法输出的可解释性和收敛性的扩展⽅法[41]。Martens 等⼈(2014) 提出了⼀种⾮常相似的⽅法解释⽂档分类。在他们的⼯作中，他们专注于解释为什么⽂档被归类为特定类或者不被归类为特定类。与本节介绍的⽅法的不同之处在于Martens等⼈(2014) 着重于⽂本分类器，它们以单词的出现作为输⼊。搜索反事实的另⼀种⽅法是Laugel 等⼈(2017)[42] 的Growing Spheres 算法。该⽅法⾸先在感兴趣点周围绘制⼀个球体，再对该球体内的点进⾏采样，检查其中⼀个采样点是否产⽣所需的预测，并相应地收缩或扩张球体，直到找到(稀疏的) 反事实并最终返回。他们在论⽂中没有使⽤“反事实” ⼀词，但是⽅法⾮常相似。他们还定义了⼀个损失函数，该函数⽀持反事实，其特征值的变化应尽可能少。他们建议直接使⽤球体进⾏上述搜索，⽽不是直接优化函数。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 185, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释179图6.2. Growing Spheres 算法和选择反事实。Ribeiro 等⼈(2018)[43] 的锚与反事实相反。锚回答以下问题：哪些特征⾜以锚定预测，即改变其他特征不能更改预测？⼀旦找到可以⽤作预测锚的特征，我们将不再通过更改锚中未使⽤的特征来找到反事实实例。图6.3. 锚的样例。6.2对抗样本对抗样本(Adversarial Examples) 是指当对⼀个样本的某⼀个特征值作出⼀个微⼩的变化⽽使得整个模型作出⼀个错误的预测。我建议先阅读有关反事实解释的⼩节，因为这些概念⾮常相似。对抗样本是反事实实例，旨在欺骗模型⽽不是解释模型。为什么我们对对抗样本感兴趣？它们难道不只是机器学习模型的有趣副产品，没有实际意义吗？答案是“否”。对抗样本使机器学习模型容易受到攻击，如以下情况所⽰。⾃动驾驶汽车撞向另⼀辆汽车，因为它⽆视停车标志。有⼈在标志上放了⼀张图⽚，从⼈眼来看它', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 186, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释180看起来像是停车标志上粘了点“灰尘”，但对标志识别软件⽽⾔它是禁⽌停车标志。垃圾邮件检测程序没有将电⼦邮件分类为垃圾邮件。垃圾邮件设计成类似于普通电⼦邮件，但⽬的是欺骗收件⼈。使⽤机器学习的扫描仪在机场扫描⾏李箱。为了避免被发现，⼈们发明了⼀种⼑具，让系统认为它是⼀把⾬伞。让我们看⼀下创建对抗样本的⼀些⽅法。6.2.1方法与示例有许多创建对抗样本的技术。⼤多数⽅法建议将对抗样本与要操纵的样本之间的距离最⼩化，同时将预测转换为期望的(对抗性的) 结果。有⼀些⽅法需要访问模型的梯度，⽽这些⽅法当然仅适⽤于基于梯度的模型(例如神经⽹络)，⽽其他⽅法仅需要访问预测函数，这使得这些⽅法与模型⽆关。本节中的⽅法重点在于具有深度神经⽹络的图像分类器，因为在该领域已进⾏了⼤量研究，⽽对抗图像的可视化具有很强的教育意义。图像的对抗样本是带有故意扰动像素的图像，⽬的是在应⽤期间欺骗模型。这些样本令⼈印象深刻地表明，⼈眼来看⽆害的图像可以多么容易地欺骗⽤于⽬标识别的深度神经⽹络。如果你尚未看到这些样本，你可能会感到惊讶，因为对于⼈类观察者来说，预测的变化是⽆法理解的。对抗样本对机器⽽⾔就像光学幻像。我的狗出了点问题Szegedy 等⼈(2013)[44] 在他们的⼯作“Intriguing Properties of Neural Networks” 中使⽤了基于梯度的优化⽅法来寻找深度神经⽹络的对抗样本。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 187, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释181图6.4. Szegedy 等⼈(2013) 关于Alexnet 的对抗样本。左列中的所有图像都已正确分类。中间⼀列显⽰添加到图像中的(放⼤的) 公差(或称扰动)，以⽣成右列中的图像，所有的都(错误) 分类为“鸵鸟”。这些对抗样本是通过最⼩化关于r 的以下函数⽣成的：loss( ˆf(x + r), l) + c · |r|在此公式中，x 是图像(表⽰为像素向量)，r 是对像素的变化以创建对抗图像(x + r ⽣成新图像)，l 是期望的结果类别，参数c ⽤于平衡图像之间的距离和预测之间的距离。第⼀项是对抗样本的预测结果与期望的类别l 之间的距离，第⼆项是对抗样本与原始图像之间的距离。这个公式⼏乎与损失函数相同，可以得出反事实的解释。对于r 还有其他限制，因此像素值保持在0 到1 之间。作者建议使⽤box-constrained L-BFGS (⼀种适⽤于梯度的优化算法) 解决此优化问题。干扰的熊猫：快速梯度符号方法(Fast Gradient Sign Method)Goodfellow 等⼈(2014)[45] 发明了⽤于⽣成对抗图像的快速梯度符号⽅法。梯度符号⽅法使⽤底层模型的梯度来查找对抗样本。通过向每个像素添加或减去⼀个⼩的扰动ϵ 来操纵原始图像x。加或减ϵ 取决于像素的梯度符号是正还是负。在梯度⽅向上增加扰动意味着图像被有意修改，从⽽导致模型分类失败。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 188, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释182图6.5.Goodfellow 等⼈(2014) 在神经⽹络中使熊猫看起来像长臂猿。通过向原始熊猫像素(左图) 添加⼩扰动(中间图)，作者创建了⼀个对抗样本，它被归类为长臂猿(右图)，但对⼈类⽽⾔却像熊猫。以下公式描述了FGSM 的核⼼：x′ = x + ϵ · sign(∇xJ(θ, x, y))其中∇xJ 是模型损失函数相对于原始输⼊像素向量x 的梯度，y 是x 的真实标签向量，⽽θ 是模型参数向量。对于梯度向量(与输⼊像素向量⼀样长)，我们需要符号：如果像素强度的增加会增加损失(模型产⽣的误差)，梯度符号为正(+1)；如果像素强度的减少会增加损失，则为负(-1)。当神经⽹络线性处理输⼊像素强度和类别分数之间的关系时，就会出现此漏洞。特别是，倾向于线性的神经⽹络结构，如LSTM、Maxout ⽹络、带有ReLU 激活单元的⽹络或其他线性机器学习算法(例如逻辑回归) 容易受到梯度符号⽅法的影响。攻击是通过外推(Extrapolation) 进⾏的。输⼊像素强度和类别分数之间的线性关系导致容易受到异常值的影响，也就是说，可以通过将像素值移动到数据分布之外的区域来欺骗模型。我希望这些对抗样本⾮常特定于给定的神经⽹络体系结构。但是事实证明，你可以重复使⽤对抗样本来欺骗在同⼀任务上训练的不同体系结构的⽹络。Goodfellow 等⼈(2014) 建议在训练数据中添加对抗样本，以学习鲁棒的模型。水母……不，等等。浴缸：单像素攻击(1-pixel attacks)Goodfellow 等⼈(2014) 提出的⽅法要求更改许多像素，即使只是稍微改变⼀点。但是，如果你只能更改⼀个像素怎么办？你能欺骗机器学习模型吗？Su 等⼈(2019)[46] 表明，实际上有可能通过更改单个像素来欺骗图像分类器。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 189, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释183图6.6.通过故意改变⼀个像素(⽤圆圈标记)，在ImageNet 上训练的神经⽹络被欺骗，从⽽预测错误的类别⽽不是原始类别。Su 等⼈(2019) 的研究。与反事实类似，单像素攻击会寻找⼀个修改后的样本x′，该样本与原始图像x 接近，但将预测更改为对抗结果。但是，接近程度的定义不同：只有⼀个像素可以更改。单像素攻击使⽤差分进化(Differential Evolution) 来找出要改变的像素以及改变⽅式。差分进化是受到物种⽣物进化的⼤致启发。个体群体(称为候选解) 会⼀代⼀代地重组，直到找到解为⽌。每个候选解都对像素修改进⾏编码，并由五个元素的向量表⽰：x 坐标和y 坐标以及红、绿和蓝(RGB) 值。搜索从例如400 个候选解(= 像素修改建议) 开始，并使⽤以下公式从⽗代中⽣成新⼀代候选解(⼦代) ：xi(g + 1) = xr1(g) + F · (xr2(g) + xr3(g))其中每个xi 是候选解的元素(x 坐标，y 坐标，红⾊，绿⾊或蓝⾊)，g 是当前代，F 是缩放参数(设置为0.5)，并且r1，r2 和r3 是不同的随机数。每个新的⼦候选解又是⼀个具有位置和颜⾊五个元素的像素，并且每个元素都是三个随机⽗像素的混合。如果候选解之⼀是对抗样本(这意味着它被归类为不正确的类)，或者达到了⽤户指定的最⼤迭代次数，则停⽌⼦代的创建。一切都是烤面包机：对抗补丁(Adversarial Patch)我最喜欢的⽅法之⼀是将对抗样本带⼊现实。Brown 等⼈(2017)[47] 设计了⼀个可打印的标签，该标签可以粘贴在⽬标旁边，使它们看起来像图像分类器的烤⾯包机。辉煌的⼯作！', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 190, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释184图6.7.⼀个贴纸，它使在ImageNet 上训练的VGG16 分类器将⾹蕉的图像分类为烤⾯包机。Brown 等⼈(2017) 的⼯作。该⽅法不同于到⽬前为⽌针对对抗样本提出的⽅法，因为消除了对抗图像必须⾮常接近原始图像的限制。取⽽代之的是，该⽅法⽤可以采⽤任何形状的补丁完全替换了图像的⼀部分。补丁的图像针对不同的背景图像进⾏了优化，⽽且补丁在图像上的位置不同，有时还会移动、会变⼤或变⼩以及旋转，以使补丁在许多情况下都可以⼯作。最后，将优化后的图像打印出来，并⽤它来欺骗图像分类器。即使你的计算机认为这是个好主意，也切勿将3D 打印的海龟卷入枪战中：鲁棒的对抗样本(RobustAdversarial Examples)接下来的⽅法实际上是在烤⾯包机上增加另⼀个维度：Athalye 等⼈(2017)[48]3D 打印了⼀只乌⻳，从⼏乎所有可能的⾓度来看，深度神经⽹络都觉得它像⼀把步枪。是的，你没看错。对⼈类⽽⾔，像乌⻳⼀样的物理物体对计算机⽽⾔就像步枪！', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 191, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释185图6.8.3D 打印的乌⻳，被TensorFlow 标准预训练InceptionV3 分类器识别为步枪。Athalye 等⼈(2017) 的⼯作。作者发现了⼀种⽅法，可以为2D 分类器创建3D 对抗样本，该样本在转换⽅⾯具有对抗性，⽐如旋转乌⻳、放⼤等等的所有可能。当图像旋转或视⾓改变时，其他⽅法(例如快速梯度⽅法) 将不再起作⽤。Athalye 等⼈(2017) 提出了Expectation Over Transformation (EOT) 算法，这是⼀种⽣成对抗样本的⽅法，该样本甚⾄在图像转换后也可以⼯作。EOT 背后的主要思想是在许多可能的转换中优化对抗样本。EOT 不是对抗样本与原始图像之间的距离最⼩化，⽽是在给定选定的可能转换分布下，将两者之间的期望距离保持在某个阈值以下。转换后的期望距离可表⽰为：Et∼T [d(t(x′), t(x))]其中x 是原始图像，t(x) 是转换后的图像(例如旋转图⽚)，x′ 是对抗样本，t(x′) 是其转换后的版本。除了处理转换分布外，EOT ⽅法遵循熟悉的模式，将搜索对抗样本框架化为优化问题。我们尝试找到⼀个对抗样本x′，该样本在可能的变换T 的分布范围内最⼤化选定类别yt (例如“步枪”)的概率：arg maxx′Et∼T [logP(yt|t(x′))]约束条件是对抗样本x′ 和原始图像x 之间所有可能的转换上的期望距离都保持在某个阈值以下：Et∼T [d(t(x′), t(x))] < ϵandx ∈[0, 1]d我认为我们应该关注此⽅法所带来的可能性。其他的⽅法有基于数字图像的处理。但是，这些3D打印的，鲁棒的对抗样本可以插⼊任何真实场景中，并欺骗计算机错误地对⽬标进⾏分类。让我们扭转⼀下：如果有⼈制造了⼀只看起来像乌⻳的步枪怎么办？蒙蔽的对手：黑盒攻击(Black Box Attack)想象以下情况：我使你可以通过Web API 访问我的出⾊的图像分类器。你可以从模型中获取预测，但⽆权访问模型参数。你发送数据，然后我的服务回答相应的分类。⼤多数对抗攻击都不是在这种', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 192, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释186情况下设计的，因为它们需要访问底层深度神经⽹络的梯度才能找到对抗样本。Papernot 及其同事(2017)[49] 表明，⽆需内部模型信息且⽆需访问训练数据即可创建对抗样本。这种(⼏乎) 零知识攻击称为⿊盒攻击。⼯作原理：1. 从与训练数据来⾃同⼀域的⼀些图像开始，例如，如果要攻击的分类器是数字分类器，则使⽤数字图像。领域知识是必需的，但不需要访问训练数据。2. 从⿊盒获得当前图像集的预测。3. 在当前图像集上训练代理模型(例如神经⽹络)。4. 使⽤启发式⽅法创建⼀组新的合成图像，该⽅法检查当前图像集，在哪个⽅向操作像素以使模型输出具有更⼤的⽅差。5. 基于预定义的迭代次数重复步骤2 到4。6. 使⽤快速梯度⽅法(或类似⽅法) 为代理模型创建对抗样本。7. ⽤对抗样本攻击原始模型。代理模型的⽬的是近似⿊盒模型的决策边界，但不⼀定要达到相同的精度。作者通过攻击在各种云机器学习服务上训练过的图像分类器来测试这种⽅法。这些服务在⽤户上传的图像和标签上训练图像分类器。该软件会⾃动训练模型(有时会使⽤⽤户不知道的算法) 并进⾏部署。然后分类器对上传的图像进⾏预测，但是模型本⾝⽆法检查或下载。作者能够为不同的提供者找到对抗样本，其中多达84% 的对抗样本被错误分类。如果要欺骗的⿊盒模型不是神经⽹络，该⽅法也能⼯作。这包括没有梯度的机器学习模型，例如决策树。6.2.2网络安全视角机器学习处理已知情况下的未知：从已知的分布中预测未知的数据点。防御攻击可应对未知情况下的未知：从未知的对抗输⼊分布中可靠地预测未知的数据点。随着机器学习被集成到越来越多的系统中，例如⾃动驾驶汽车或医疗设备，它们也成为攻击的切⼊点。即使测试数据集上机器学习模型的预测是100% 正确的，也可以找到对抗样本来欺骗该模型。防御⽹络攻击的机器学习模型是⽹络安全领域的新组成部分。Biggio 等⼈(2018)[50] 对⼗年来关于对抗性机器学习的研究进⾏了很好的回顾，本节以此为基础。⽹络安全是攻击者和防御者⼀次又⼀次地胜过对⼿的军备竞赛。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 193, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释187⽹络安全中有三个黄⾦法则：1) 了解你的对⼿2) 主动3) 保护⾃⼰。不同的应⽤程序有不同的对⼿。试图通过电⼦邮件骗取他⼈钱财的⼈是电⼦邮件服务提供商和⽤户的敌对⽅。提供商希望保护其⽤户，以便他们可以继续使⽤其邮件程序，攻击者希望使⼈们给他们钱。了解你的对⼿意味着了解他们的⽬标。假设你不知道这些垃圾邮件发送者的存在，并且对电⼦邮件服务的唯⼀滥⽤是发送盗版⾳乐，那么防御⽅式将是不同的(例如，扫描附件中的受版权保护的材料，⽽不是分析⽂本中的垃圾邮件指标)。“主动” 意味着积极地测试和识别系统的弱点。当你主动尝试⽤对抗样本欺骗模型然后防御它们时，你会变得很主动。使⽤解释⽅法来了解哪些特征很重要以及特征如何影响预测，也是了解机器学习模型的弱点的主动步骤。作为数据科学家，你是否会在这个危险的世界中信任你的模型？你是否分析了模型在不同情况下的⾏为，确定了最重要的输⼊，并检查了⼀些样本的预测解释？你是否尝试寻找对抗输⼊？机器学习模型的可解释性在⽹络安全中起着重要作⽤。被动，是与主动相反，意味着等待着直到系统被攻击，然后才理解问题并安装⼀些防御措施。我们如何保护我们的机器学习系统免受对抗样本的攻击？主动的⽅法是使⽤对抗样本对分类器进⾏迭代重新训练，也称为对抗训练。其他⽅法基于博弈论，例如学习特征的不变转换或鲁棒优化(正则化)。另⼀种建议的⽅法是使⽤多个分类器，⽽不是⼀个，并让它们对预测进⾏投票(集成)，但这并不能保证有效，因为它们都可能遭受类似的对抗样本。另⼀种效果也不佳的⽅法是梯度掩模(Gradient Masking)，它通过构建没有⽤梯度的模型，例如使⽤最近邻分类器⽽不是原始⽹络模型。我们可以通过攻击者对系统的了解程度来区分攻击类型。攻击者可能具有完备的知识(⽩盒攻击)，这意味着他们知道有关模型的所有信息，例如模型的类型、参数和训练数据。攻击者可能具有部分知识(灰盒攻击)，这意味着他们可能只知道特征表⽰和使⽤的模型类型，⽽⽆法访问训练数据或参数；攻击者可能具有零知识(⿊盒攻击)，这意味着他们只能以⿊盒⽅式查询模型，⽽⽆法访问训练数据或有关模型参数的信息。根据信息级别，攻击者可以使⽤不同的技术来攻击模型。正如我们在样本中看到的，即使在⿊盒中，也可以创建对抗样本，因此隐藏关于数据和模型的信息不⾜以抵御攻击。考虑到攻击者和防御者之间的猫捉⽼⿏游戏的性质，我们将在这⼀领域看到很多发展和创新。试想⼀下不断演变出许多不同类型的垃圾邮件。针对机器学习模型提出新的攻击⽅法，并针对这些新攻击提出了新的防御措施。不断开发出更强⼤的攻击来逃避最新的防御等等。在本节中，我希望使你意识到对抗样本的问题，并且只有通过积极研究机器学习模型，我们才能发现并弥补弱点。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 194, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释1886.3原型与批评⼀个原型是⼀个数据实例，它是所有数据的代表。⼀个批评是不能由⼀组原型很好地代表的⼀个数据实例。批评的⽬的是与原型⼀起提供见解，尤其是对于原型不能很好代表的数据点。原型和批评(Prototypes and Criticisms) 可以(与机器学习) 模型⽆关地⽤于描述数据，但是它们也可以⽤于创建可解释的模型或使⿊盒模型可解释。在本节中，我使⽤“数据点” ⼀词来指代单个实例，以强调实例也是坐标系统中的⼀个点，其中每个特征都是⼀个维度。下图显⽰了⼀个模拟的数据分布，其中⼀些实例被选择为原型，另⼀些被作为批评。⼩点是数据，⼤点是原型，⼤⽅块是批评。选择原型(⼿动) 以覆盖数据分布的中⼼，并且批评是没有原型的簇中的点。原型和批评总是来⾃数据的实际实例。图6.9. 具有两个特征x1 和x2 的数据分布的原型和批评。我⼿动选择了原型，这并不能很好地扩展，可能会导致结果不佳。在数据中找到原型的⽅法很多。其中之⼀是k-medoids，这是与k-means 算法相关的聚类算法。任何返回实际数据点作为聚类中⼼的聚类算法都有资格选择原型。但是这些⽅法⼤多数都只找到原型，⽽没有批评。本节介绍Kim 等⼈(2016)[4] 的MMD-critic，⼀种将原型和批评结合在⼀个框架中的⽅法。MMD-critic ⽐较数据的分布和所选原型的分布。这是理解MMD-critic ⽅法的中⼼概念。MMD-critic 选择的原型可以最⼤程度地减少两个分布之间的差异。⾼密度区域中的数据点是很好的原型，尤其是从不同的“数据簇” 中选择数据点时。来⾃原型不能很好解释的区域的数据点被选择为批评。让我们深⼊研究该理论。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 195, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释1896.3.1理论MMD-critic 程序可以概括如下：1. 选择要查找的原型和批评数量。2. 通过贪婪搜索找到原型。选择原型，以使原型的分布接近数据分布。3. 通过贪婪搜索找到批评。当原型的分布不同于数据的分布时，选择点作为批评。我们需要⼀些因素来找到MMD-critic 数据集的原型和批评。作为最基本的成分，我们需要⼀个核函数来估计数据密度。核是根据接近度对两个数据点加权的函数。基于密度估计，我们需要⼀种度量来告诉我们两种分布有何不同，以便我们可以确定我们选择的原型的分布是否接近数据分布。这可以通过测量最大平均差异(Maximum Mean Discrepancy) 来解决。同样基于核函数，我们需要witness 函数告诉我们在特定数据点上两种分布有何不同。通过witness 函数，我们可以选择批评，即原型和数据的分布不同且witness 函数具有较⼤绝对值的数据点。最后⼀个成分是寻找好的原型和批评的搜索策略，可以通过简单的贪婪搜索来解决。让我们从最大平均差异(MMD) 开始，它衡量两个分布之间的差异。原型的选择创建了原型的密度分布。MMD-critic 的⽬的是最⼩化选择的原型分布和数据分布之间的差异。我们要通过⽤核密度函数进⾏估计的经验分布，评估原型分布与数据分布是否不同。最⼤平均差异度量两个分布之间的差异，这是根据两个分布的期望差在函数空间上的极值。清楚了吗？就我个⼈⽽⾔，当我看到数据是如何计算的时候，我会更好地理解这些概念。以下公式显⽰了如何计算平⽅的MMD 量度(MMD2) ：MMD2 = 1m2m∑i,j=1k(zi, zj) −2mnm,n∑i,j=1k(zi, xj) + 1n2n∑i,j=1k(xi, xj)k 是⼀个核函数，⽤于测量两点的相似性，稍后将对此进⾏更多介绍。m 是原型z 的数量，n 是原始数据集中的数据点x 的数量。原型z 是数据点x 的选择。每个点都是多维的，即可以具有多个特征。MMD-critic 的⽬标是最⼩化MMD2。MMD2 越接近零，原型的分布拟合数据越好。将MMD2 降低为零的关键是中间的项，该项计算原型与所有其他数据点之间的平均接近度(乘以2)。如果此项加起来等于第⼀项(原型彼此之间的平均接近度) 加上最后⼀项(数据点彼此之间的平均接近度)，则原型可以完美地解释数据。尝试⼀下如果你使⽤所有n 个数据点作为原型，公式会发⽣什么变化。下图说明了MMD2 度量。第⼀张图显⽰了具有两个特征的数据点，其中数据密度的估计以阴影背景显⽰。其他每个图都显⽰了不同的原型选择，以及图标题中的MMD2 度量。原型是⼤点，其分布显⽰为等⾼线。选择最能覆盖这些情况下数据的原型(左下) 具有最低的差异值。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 196, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释190图6.10. 具有两个特征和不同原型选择的数据集的最⼤平均差异的平⽅(MMD2) 度量。核的⼀种选择是径向基函数核：k(x, x′) = exp(−γ||x −x′||2)其中||x −x′||2 是两点之间的欧式距离，⽽γ 是缩放参数。核的值随两点之间的距离⽽减⼩，范围在0 和1 之间：当两点⽆限远时为0；当两点相等时为1。我们将MMD2 度量，核和贪婪搜索结合在⼀种算法中，以查找原型：• 从空的原型列表开始。• 当原型数量低于所选数量m 时：– 对于数据集中的每个点，检查将点添加到原型列表后MMD2 减少了多少。将最⼩化MMD2 的数据点添加到列表中。• 返回原型列表。找到批评是⽤witness 函数，该函数告诉我们在特定点上两个密度估计值相差多少。可以使⽤以下⽅法估计：witness(x) = 1nn∑i=1k(x, xi) −1mm∑j=1k(x, zj)对于两个具有相同特征的数据集，witness 函数为你提供了⼀种评估x 点更适合哪种经验分布的⽅法。为了发现批评，我们在正向和负向上寻找witness 函数的极值。witness 函数中的第⼀项是点x与数据之间的平均接近度，第⼆项是点x 与原型之间的平均接近度。如果点x 的witness 函数接近于零，则数据和原型的密度函数接近在⼀起，这意味着在点x 处原型的分布类似于数据的分布。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 197, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释191x 点处的负witness 函数意味着原型分布会⾼估数据分布(例如，如果我们选择原型，但附近只有很少的数据点)；x 点处的正witness 函数意味着原型分布低估了数据分布(例如，如果x 周围有很多数据点，但我们附近没有选择任何原型)。为了给你带来更多的直觉，让我们预先以最低的MMD2 重⽤图中的原型，并显⽰⼀些⼿动选择的点的witness 函数。下图中的标签显⽰了标记为三⾓形的各个点的witness 函数的值。只有中间的⼀个点具有很⾼的绝对值，因此是进⾏批评的很好的候选者。图6.11. 不同点witness 函数的评估。witness 函数允许我们显式搜索原型⽆法很好地表⽰的数据实例。批评是witness 函数中具有很⾼绝对值的点。像原型⼀样，也可以通过贪婪搜索找到批评。但是，我们不是在减少总体MMD2，⽽是在寻找能够最⼤化损失函数(包括witness 函数和正则项) 的点。优化函数中的附加项会增强点的多样性，这是必需的，以便点来⾃不同的簇。第⼆步独⽴于找到原型的过程。我也可以亲⾃挑选⼀些原型，并使⽤此处描述的过程来学习批评。或者，原型可以来⾃任何聚类过程，例如k-medoids。这就是MMD-critic 理论的重要部分。⼀个问题仍然存在：如何将MMD-critic 用于可解释的机器学习？MMD-critic 可以通过三种⽅式增加可解释性：通过帮助更好地理解数据分布；通过建⽴可解释的模型；通过使⿊盒模型可解释。如果将MMD-critic 应⽤于数据以查找原型和批评，这将增进你对数据的理解，尤其是当你有边缘', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 198, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释192情况的复杂数据分布时。但是有了MMD-critic，你可以实现更多！例如，你可以创建⼀个可解释的预测模型：所谓的“最近原型模型” (Nearest Prototype Model)。预测函数定义为：ˆf(x) = argmaxi∈Sk(x, xi)这意味着我们从原型集合S 中选择最接近新数据点的原型i，因为它产⽣了核函数的最⼤值。原型本⾝将作为预测的解释返回。该过程具有三个调整参数：核类型，核缩放参数和原型数量。可以在交叉验证循环中优化所有参数。这种⽅法不使⽤批评。作为第三个选择，我们可以使⽤MMD-critic，通过检查原型和批评及其模型预测，使全局范围内的任何机器学习模型都可解释。步骤如下：1. 查找MMD-critic 的原型和批评。2. 照常训练机器学习模型。3. 使⽤机器学习模型预测原型和批评的结果。4. 分析预测：在哪些情况下算法错误？现在，你有⼀些很好地表⽰数据的实例，可以帮助你发现机器学习模型的弱点。这有什么⽤呢？还记得Google 的图像分类器将⿊⼈识别为⼤猩猩吗？也许他们应该在部署图像识别模型之前使⽤这⾥描述的过程。仅仅检查模型的性能是不够的，因为如果正确率是99%，则此问题可能仍然是1%。⽽且标签也可能是错误的！检查所有训练数据并遍历所有训练数据并执⾏完整性检查(如果预测有问题) 可能会发现问题，但不可⾏。但是，选择(例如⼏千个) 原型和批评是可⾏的，并且可能揭⽰了数据问题：这可能表明缺乏⽪肤黝⿊的⼈的图像，这表明存在数据集多样性的问题。它可能会显⽰⼀个或多个⽪肤黝⿊的⼈的图像作为原型，或者(可能) 以臭名昭著的“⼤猩猩” 类别显⽰为批评。我不保证MMD-critic 肯定会拦截此类错误，但这是⼀个很好的完整性检查。6.3.2示例我从MMD-critic 论⽂中摘取了⽰例。这两个应⽤程序都基于图像数据集。每个图像都由2048 维的图像嵌⼊表⽰。图像嵌⼊是⼀个带有数字的向量，这些数字捕获图像的抽象属性。嵌⼊向量通常是从神经⽹络中提取的，这些神经⽹络经过训练可以解决图像识别任务，在这种情况下为ImageNet挑战。使⽤这些嵌⼊向量计算图像之间的核距离。第⼀个数据集包含与ImageNet 数据集不同的⽝种。MMD-critic 应⽤于来⾃两个⽝种类别的数据。左边的狗，原型通常会显⽰狗的脸，⽽批评则是没有狗脸或不同颜⾊(例如⿊⽩) 的图像。在右侧，原型包含狗的户外图像。批评包括穿着服装的狗和其他不寻常情况的狗。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 199, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释193图6.12. 来⾃ImageNet 数据集的两种类别狗的原型和批评。MMD-critic 的另⼀个⽰例是使⽤⼿写数字数据集。查看实际的原型和批评，你可能会注意到每个数字的图像数量是不同的。这是因为在整个数据集中搜索了固定数量的原型和批评，⽽不是每个类都使⽤固定数量的原型和批评。正如预期的那样，原型展⽰了不同的数字书写⽅式。这些批评包括带有异常的粗线或细线的实例，也有⽆法识别的数字。图6.13. ⼿写体数字的原型和批评。6.3.3优点在⼀项⽤户研究中，MMD-critic 的作者向参与者提供了图像，他们必须在视觉上匹配两组图像中的⼀个，每组图像代表两个类别之⼀(例如，两个⽝种)。当展示原型和批评而不是一个类的随机图像时，参与者表现得最好。你可以自由选择原型和批评的数量。MMD-critic 使⽤数据的密度估计。这适⽤于任何类型的数据和任何类型的机器学习模型。该算法易于实现。MMD-critic 在提⾼解释性⽅⾯⾮常灵活。它可以⽤来了解复杂的数据分布。它可⽤于构建可解释的机器学习模型。或者它可以阐明⿊盒机器学习模型的决策。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 200, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释194找到批评是独立于原型的选择过程。但是根据MMD-critic 选择原型是有意义的，因为这样⼀来，使⽤相同的⽐较原型和数据密度的⽅法来创建原型和批评。6.3.4缺点在数学上，原型和批评的定义不同，但它们的区别是基于截断值(原型的数量)。假设你选择的原型数量太少，⽆法覆盖数据分布。批评最终将集中在没有得到很好解释的区域。但是，如果要添加更多原型，它们也将最终出现在相同的区域。任何解释都必须考虑到批评在很⼤程度上取决于现有原型和原型数量的(任意) 截断值。你必须选择原型和批评的数量。尽管这很不错，但它也是⼀个缺点。我们实际上需要多少原型和批评？越多越好？越少越好？⼀种解决⽅案是通过测量⼈类在查看图像上有多少时间来选择原型和批评的数量，这取决于特定的应⽤程序。只有当使⽤MMD-critic 构建分类器时，我们才可以直接对其进⾏优化。⼀种解决⽅案可能是在屏幕x 轴上显⽰原型数量，在y 轴上显⽰MMD2 度量。我们将选择MMD2 曲线变平的原型数量。其他的参数是核的选择和核缩放参数。我们⾯临着与原型和批评的数量相同的问题：我们如何选择核及其缩放参数？同样，当我们使⽤MMD-critic 作为最近原型分类器时，我们可以调整核参数。但是，对于MMD-critic 的⽆监督使⽤情况，⽬前尚不清楚。(也许我在这⾥有点苛刻，因为所有⽆监督的⽅法都存在这个问题。)它以所有特征为输⼊，⽽忽略了某些特征可能与预测目标结果无关的事实。⼀种解决⽅案是仅使⽤相关特征，例如图像嵌⼊⽽不是原始像素。只要我们有办法将原始实例投影到仅包含相关信息的表⽰形式上，它就可以⼯作。有⼀些可⽤的代码，但是它还没有被很好地打包成文档化的软件来实现。6.3.5代码和替代方法可以在以下位置找到MMD-critic 的实现：https://github.com/BeenKim/MMD-critic。寻找原型的最简单替代⽅案是Kaufman 等⼈(1987)[51] 的k-medoids。6.4有影响力的实例机器学习模型最终是训练数据的产物，删除其中⼀个训练实例可能会影响⽣成的模型。当训练实例从训练数据中删除后，会⼤⼤改变模型的参数或预测，因此我们将这个实例称为“有影响⼒的”。通过识别有影响⼒的训练实例，我们可以“调试” 机器学习模型，并更好地解释它们的⾏为和预测。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 201, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释195本节向你介绍两种识别有影响⼒的实例(Influential Instances) 的⽅法，即删除诊断和影响函数。两种⽅法均基于稳健统计(Robust Statistics)，稳健统计提供的统计⽅法受异常值或违反模型假设的影响较⼩。稳健统计还提供了⼀些⽅法来测量数据的稳健估计(例如平均估计或预测模型的权重)。假设你要估算城市中⼈们的平均收⼊，然后随机询问街上的10 个⼈他们的收⼊。除了你的样本可能真的很差之外，你的平均收⼊估计能在多⼤程度上受到⼀个⼈的影响？要回答这个问题，你可以通过忽略个别答案来重新计算平均值，或者通过“影响函数” 从数学上推导是如何影响平均值的。使⽤删除⽅法，我们将重新计算平均值⼗次，每次都删除其中⼀份收⼊，并测量平均估计值的变化。⼀个⼤的变化意味着删去的实例具有很⼤的影响⼒。第⼆种⽅法通过⽆限⼩的权重来增加其中⼀个的权重，这对应于统计量或模型的⼀阶导数的计算。这种⽅法也称为“⽆穷⼩⽅法” 或“影响函数”。顺便说⼀句，结果是你的均值估计会受到单个值的强烈影响，因为均值与单个值呈线性⽐例关系。⼀个更可靠的选择是中位数(该数值表⽰⼀半的⼈收⼊更⾼⽽另⼀半的⼈收⼊更低)，因为即使你样本中收⼊最⾼的⼈的收⼊要⾼⼗倍，所得的中位数也不会改变。删除诊断和影响函数也可以应⽤于机器学习模型的参数或预测，以更好地了解其⾏为或解释单个预测。在介绍这两种寻找有影响⼒的实例的⽅法之前，我们将研究异常值和有影响⼒的实例之间的差异。异常值⼀个异常值(Outlier，也称离群值) 是远离数据集中其他实例的⼀个实例。“远离” 表⽰到所有其他实例的距离(例如，欧⼏⾥得距离) ⾮常⼤。在新⽣⼉数据集中，体重6 公⽄的新⽣⼉将被视为异常值。在⽀票账户为主的银⾏账户数据集中，专⽤贷款账户(负余额⼤，交易少) 将被视为异常值。下图显⽰了⼀维分布的离群值。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 202, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释196图6.14. 特征x 遵循⾼斯分布，x=8 处有⼀个异常值。异常值可能是有趣的数据点(例如，批评)。当异常值影响模型时，它也是有影响⼒的实例。有影响力的实例有影响⼒的实例是数据实例，其删除对训练模型有很⼤影响。在从训练数据中删除特定实例后对模型进⾏重新训练时，模型参数或预测变化越⼤，该实例的影响⼒就越⼤。实例是否对经过训练的模型有影响还取决于实例对⽬标y 的值。下图显⽰了线性回归模型的有影响⼒的实例。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 203, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释197图6.15. 具有⼀个特征的线性模型。在完整数据上进⾏了⼀次训练，以及在没有影响⼒的实例下进⾏了⼀次训练。删除有影响的实例会显著地改变拟合的斜率(权重/系数)。为什么有影响⼒的实例有助于理解模型？对解释有影响⼒的实例背后的关键思想是将模型参数和预测追溯到⼀切开始的地⽅：训练数据。学习器(即⽣成机器学习模型的算法) 是⼀种函数，该函数获取包含特征X 和⽬标y 的训练数据并⽣成机器学习模型。例如，决策树的学习器是⼀种选择分割特征和分割值的算法。神经⽹络的学习器使⽤反向传播来找到最佳权重。图6.16. 学习器从训练数据(特征加⽬标) 中学习模型。该模型对新数据进⾏预测。我们询问如果在训练过程中从训练数据中删除实例，模型参数或预测将如何变化。这与其他可解释性⽅法形成了对⽐，其他可解释性⽅法分析了当我们操纵要预测的实例的特征(例如部分依赖图或特征重要性) 时预测如何变化。对于有影响⼒的实例，我们不会将模型视为固定模型，⽽是将其视', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 204, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释198为训练数据的函数。有影响⼒的实例可以帮助我们回答有关全局模型⾏为和单个预测的问题。哪些是对模型参数或整体预测最有影响⼒的实例？对于特定的预测，哪些是最有影响⼒的实例？有影响⼒的实例告诉我们模型可能会在哪些实例上出现问题，针对错误应该检查哪些训练实例，并且会给⼈以模型鲁棒性的印象。如果单个实例对模型的预测和参数有很⼤的影响，我们可能不信任模型。⾄少这将使我们进⼀步调查。我们如何找到有影响⼒的实例？我们有两种测量影响⼒的⽅法：我们的第⼀个选择是从训练数据中删除实例，在简化的训练数据集上重新训练模型，并观察模型参数或预测的差异(⽆论是单个的还是整个数据集)。第⼆个选择是增加数据实例的权重，通过模型参数的梯度来近似参数变化。删除⽅法更容易理解，并且会引申出权重增加⽅法，因此我们从前者开始。6.4.1删除诊断统计学家已经在有影响⼒的实例领域进⾏了⼤量研究，尤其是对于(⼴义) 线性回归模型。当你搜索“有影响⼒的观测” 时，第⼀个搜索结果与诸如DFBETA 和Cook 距离之类的度量有关。DFBETA衡量删除实例对模型参数的影响。Cook 距离(Cook，1977[52]) 衡量删除实例对模型预测的影响。对于这两种⽅法，我们必须重复训练模型，每次都忽略单个实例。删除诊断(Deletion Diagnostics)是将具有所有实例的模型的参数或预测与从训练数据中删除某个实例后的模型的参数或预测进⾏⽐较。DFBETA 定义为：DFBETAi = β −β(−i)其中，β 是在所有数据实例上训练模型时的权重向量，β(−i) 是在没有实例i 的情况下训练模型时的权重向量。我会说⾮常直观。DFBETA 仅适⽤于具有权重参数的模型，例如逻辑回归或神经⽹络，⽽不适⽤于决策树、树的集成、某些⽀持向量机等模型。Cook 距离是针对线性回归模型⽽发明的，并且存在对⼴义线性回归模型的近似值。训练实例的Cook 距离定义为当从模型训练中删除第i 个实例时，预测结果的平⽅差之和。Di =∑ni=1(ˆyj −ˆy(−i)j)2p · MSE其中分⼦是在有和没有第i 个实例的情况下对模型的预测之间的平⽅差，是对数据集求和的结果。分母是特征数p 乘以均⽅误差。⽆论删除哪个实例i，所有实例的分母都是相同的。Cook 距离告诉我们，当我们从训练中删除第i 个实例时，线性模型的预测输出会发⽣多少变化。我们可以在任何机器学习模型中使⽤Cook 距离和DFBETA 吗？DFBETA 需要模型参数，因此此度量仅适⽤于参数化模型。Cook 距离不需要任何模型参数。有趣的是，通常不会在线性模型和⼴义线性模型的范围之外看到Cook 距离，但是在删除特定实例之前和之后获取模型预测之间的差异的想法⾮常普遍的。Cook 距离定义的⼀个问题是MSE，它对所有类型的预测模型(例如分类) 都没有意义。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 205, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释199可以将对模型预测影响的最简单的影响⼒度量写为：Influence(−i) = 1nn∑j=1\\x0c\\x0c\\x0cˆyj −ˆy(−i)j\\x0c\\x0c\\x0c该表达式基本上是Cook 距离的分⼦，不同之处在于，⽤绝对差代替平⽅差相加。这是我做的⼀个选择，因为它对后⾯的⽰例有意义。删除诊断度量的⼀般形式包括选择度量(例如预测结果)，并为在考虑所有实例时以及考虑实例被删除时训练的模型计算度量的差异。我们可以轻松地分解影响⼒，为实例j 的预测解释第i 个训练实例的影响⼒是什么：Influence(−i)j=\\x0c\\x0c\\x0cˆyj −ˆy(−i)j\\x0c\\x0c\\x0c这也适⽤于模型参数的差异或损失的差异。在下⾯的⽰例中，我们将使⽤这些简单的影响⼒度量。删除诊断示例在以下⽰例中，我们训练了⼀个⽀持向量机，在给定风险因素的情况下预测宫颈癌，并衡量哪些训练实例对整体和特定预测的影响最⼤。由于癌症的预测是⼀个分类问题，因此我们⽤癌症的预测概率中的差异来衡量影响⼒。如果将实例从模型训练中删除，预测概率在数据集中平均显著增加或减少，则该实例具有影响⼒。要测量所有858 个训练实例的影响⼒，就需要先对所有数据进⾏⼀次模型训练，并对其进⾏858 次(= 训练数据⼤⼩) 的训练，其中每次都会移除⼀个实例。最有影响⼒的实例的影响⼒度量约为0.01。0.01 的影响⼒意味着，如果我们删除第540 个实例，则预测的概率平均变化1 个百分点。考虑到癌症的平均预测概率为6.4%，这是相当可观的。影响⼒度量值在所有可能的删除上的平均值为0.2 个百分点。现在我们知道哪个数据实例对模型影响最⼤。这对于调试数据已经很有⽤。是否存在问题实例？是否存在测量误差？有影响⼒的实例是应⾸先检查是否存在错误的实例，因为它们中的每个错误都会强烈影响模型预测。除了模型调试之外，我们还能学到⼀些东西来更好地理解模型吗？仅仅打印出10 个最有影响⼒的实例并不是很有⽤，因为它只是具有许多特征的实例表。所有返回实例作为输出的⽅法只有在我们有很好的表⽰⽅式时才有意义。但是，当我们询问以下问题时，我们可以更好地理解哪种类型的实例具有影响⼒：有影响⼒的实例和没有影响⼒的实例有什么区别？我们可以将这个问题变成回归问题，并根据其特征值对实例的影响进⾏建模。我们可以从“可解释的模型” ⼀章中⾃由选择任何模型。在此⽰例中，我选择了⼀个决策树(下图)，该树显⽰来⾃35 岁及以上⼥性的数据对⽀持向量机的影响最⼤。在数据集的858 名⼥性中，有153 名年龄在35 岁以上。在“部分依赖图” ⼀节中，我们发现40 岁以后⼥性的预测癌症发病率急剧上升，⽽“特征重要性” 也将年龄视为最重要的特征之⼀。影响⼒分析告诉我们，当预测较⾼年龄的癌症时，该模型将变得越来越不稳定。这本⾝就是有价值的信息。这意味着这些实例中的错误会对模型产⽣重⼤影响。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 206, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释200图6.17. 决策树，对实例的影响⼒及其特征之间的关系进⾏建模。树的最⼤深度设置为2。第⼀个影响⼒分析揭⽰了总体最有影响⼒的实例。现在，我们选择⼀个实例，即第7 个实例，我们要通过查找最有影响⼒的训练数据实例来解释预测。这就像⼀个反事实的问题：如果我们从训练过程中忽略实例i，实例7 的预测将如何变化？我们对所有实例重复此删除操作。然后，当训练实例在训练中被忽略时，我们选择导致实例7 预测变化最⼤的训练实例，并使⽤它们来解释该实例的模型预测。我选择解释实例7 的预测，因为它是癌症预测概率最⾼(7.35%) 的实例，我认为这是⼀个有趣的案例，需要更深⼊地分析。我们可以返回对预测第7 个实例的前10 个最有影响⼒的实例(以表格形式返回)。不是很有⽤，因为我们看不到太多。再次，更有意义的是通过分析特征来找出将有影响⼒的实例与⽆影响⼒的实例区分开的地⽅。我们使⽤经过训练的决策树来预测给定特征的影响⼒，但实际上，我们误⽤它只是找到结构⽽不实际预测某些东西。以下决策树显⽰了哪种训练实例对预测第7 个实例最有影响⼒。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 207, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释201图6.18.决策树解释了哪些实例对预测第7 个实例最有影响⼒。吸烟时间在18.5 年或更长时间的⼥性的数据对第7 个实例的预测有很⼤影响，绝对预测的平均变化会导致癌症发⽣概率降低11.7个百分点。烟龄18.5 年或更长时间的⼥性的数据实例对第7 个实例的预测有很⼤影响⼒。第7 个实例的⼥性吸烟了34 年。在数据中，有12 位⼥性(1.40%) 吸烟18.5 年或更长时间。在收集其中⼀名⼥性的吸烟年数⽅⾯犯的任何错误都会对第7 个实例的预测结果产⽣巨⼤影响。当我们删除第663 例时，预测会发⽣最极端的变化。据称该病⼈吸烟了22 年，与决策树的结果⼀致。如果我们删除第663 例，则第7 个实例的预测概率将从7.35% 变为66.60%！如果我们仔细研究最有影响⼒的实例的特征，我们会发现另⼀个可能的问题。数据显⽰，该⼥性今年28 岁，已经吸烟22 年。这是⼀个⾮常极端的情况，她真的从6 岁开始吸烟，或者这是⼀个数据错误。我倾向于相信后者。当然，在这种情况下，我们必须质疑数据的准确性。这些实例说明了识别有影响⼒的实例对调试模型的有⽤性。该⽅法的⼀个问题是该模型需要针对每个训练实例进⾏重新训练。整个重新训练可能会⾮常缓慢，因为如果你有成千上万的训练实例，则必须对模型进⾏数千次重新训练。假设模型需要⼀天的训练时间，并且你有1000 个训练实例，那么有影响⼒的实例(不进⾏并⾏化) 的计算将花费近3 年的时间。没有⼈有时间这样做。在本节的其余部分，我将向你展⽰⼀种不需要重新训练模型的⽅法。6.4.2影响函数你：我想知道训练实例对特定预测的影响⼒。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 208, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释202研究：你可以删除训练实例，重新训练模型并测量预测中的差异。你：太好了！但是你有没有⼀个⽅法可以让我不⽤重新训练？这需要很多时间。研究：你是否具有⼀个损失函数的模型，该模型的参数具有⼆阶导数？你：我⽤logistic 损失训练了⼀个神经⽹络。所以是的。研究：然后，你可以使⽤影响函数(Influence Functions) 来估计实例对模型参数和预测的影响⼒。影响函数是模型参数或预测对训练实例的依赖程度的度量。该⽅法不是删除实例，⽽是仅通过很⼩的步幅对损失中的实例加权。该⽅法包括使⽤梯度和Hessian 矩阵来近似当前模型参数的损失。损失加权与删除实例类似。你：太好了，这就是我想要的！Koh and Liang (2017)[53] 建议使⽤影响函数(⼀种稳健统计) 来衡量实例如何影响模型参数或预测。与删除诊断⼀样，影响函数将模型参数和预测追溯到负责的训练实例。但是，该⽅法并没有删除训练实例，⽽是近似于当实例在经验风险(训练数据损失之和) 中被加权时，模型的变化程度。影响函数的⽅法需要获得与模型参数相关的损失梯度，这仅适⽤于机器学习模型的⼦集。Logistic回归、神经⽹络和⽀持向量机符合条件，⽽基于树的⽅法(如随机森林) 则不符合。影响函数有助于理解模型⾏为，调试模型并检测数据集中的错误。以下部分解释了影响函数背后的直觉和数学运算。影响函数背后的数学影响函数背后的关键思想是通过⽆限⼩的步幅(ϵ) 对某个训练实例的损失加权，从⽽产⽣新的模型参数：ˆθϵ,z = arg minθ∈Θ1nn∑i=1L(zi, θ) + ϵL(z, θ)其中θ 是模型参数向量，⽽ˆθϵ,z 是⽤⾮常⼩的数字ϵ 对z 进⾏加权后的参数向量。L 是模型训练的损失函数，zi 是训练数据，z 是我们想要增加权重来模拟其移除的训练实例。这个公式背后的直觉是：如果我们将训练数据中的某个特定实例zi 稍微增加⼀点ϵ 权重，损失会有多⼤变化？优化这个新的组合损失参数向量又是什么样⼦？参数的影响函数，即对训练实例z 加权对参数的影响⼒，可计算如下。Iup,params(z) = dˆθϵ,zdϵ\\x0c\\x0c\\x0c\\x0c\\x0cϵ=0= −H−1ˆθ ∇θL(z, ˆθ)最后⼀个表达式∇θL(z, ˆθ) 是对训练实例加权后参数的损失梯度。梯度是训练实例损失的变化率。它告诉我们，当我们稍微改变模型参数ˆθ 时，损失会有多⼤的变化。梯度向量中的正项意味着相应模型参数的微⼩增加会增加损失，负项意味着参数的增加会减少损失。第⼀部分H−1ˆθ是逆Hessian', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 209, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释203矩阵(损失相对于模型参数的⼆阶导数)。Hessian 矩阵是梯度的变化率，或表⽰为损失变化率的变化率。可以使⽤下列公式进⾏估算：Hθ = 1nn∑i=1∇2ˆθL(zi, ˆθ)⾮正式地说：Hessian 矩阵记录损失在特定点的弯曲程度。Hessian 是⼀个矩阵，⽽不仅仅是⼀个向量，因为它描述了损失的曲率，并且曲率取决于我们的观察⽅向。如果你有许多参数，则实际计算Hessian 矩阵会很费时。Koh 和Liang 提出了⼀些有效计算的技巧，这超出了本节的范围。如上式所述，更新模型参数等效于在估计模型参数周围形成⼆次展开后执⾏单个⽜顿步。这个影响函数公式背后的直觉是什么？公式来⾃于围绕参数ˆθ 形成⼆次展开式。这意味着我们实际上不知道，或者计算实例z 在被删除/加权时的损失究竟会有多⼤的变化太复杂了。我们利⽤当前模型参数设置下的陡度(= 梯度) 和曲率(=Hessian 矩阵) 的信息对函数进⾏局部近似。通过这种损失近似，我们可以计算出如果我们对实例z 进⾏加权(令ϵ = −1n)，新参数将⼤致是什么样⼦：ˆθ−z ≈ˆθ −1nIup,params(z)近似参数向量基本上是原始参数减去z 的损失梯度(因为我们要减少损失)，⽤曲率(= 乘以逆Hessian 矩阵) 缩放并⽤1n 缩放，因为这是单个训练实例的权重。下图显⽰了加权的⼯作⽅式。x 轴显⽰θ 参数的值，y 轴显⽰实例z 加权后的损失的相应值。出于演⽰的⽬的，这⾥的模型参数为⼀维，但实际上通常为⾼维。对于实例z 我们只向损失改善(下降)的⽅向移动了1n。我们不知道删除z 时损失的实际变化，但是利⽤损失的⼀阶和⼆阶导数，我们围绕当前模型参数创建了⼆次近似，并假设这是实际损失的⾏为。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 210, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释204图6.19. 通过在当前模型参数周围形成损失的⼆次展开，并在将实例z 加权的损失最⼤改善(y 轴)的⽅向上移动1n，来更新模型参数(x 轴)。如果我们删除z 并在减少的数据上训练模型，则实例z在损失中的这种加权近似于参数变化。实际上，我们不需要计算新参数，但可以使⽤影响函数来衡量z 对参数的影响⼒。当我们增加训练实例z 的权重时，预测如何变化？我们既可以计算新参数，然后使⽤新参数化的模型进⾏预测，也可以直接计算实例z 对预测的影响⼒，因为我们可以使⽤链式规则来计算影响⼒：Iup,loss(z, ztest) = dL(ztest, ˆθϵ,z)dϵ\\x0c\\x0c\\x0c\\x0c\\x0cϵ=0= ∇θL(ztest, ˆθ)T dˆθϵ,zdϵ\\x0c\\x0c\\x0c\\x0c\\x0cϵ=0= −∇θL(ztest, ˆθ)T H−1θ ∇θL(z, ˆθ)该⽅程式的第⼀⾏意味着，当我们增加实例z 的权重并获得新参数ˆθϵ,z 时，我们测量训练实例对某个预测ztest 的影响⼒，作为预测实例损失的变化。对于⽅程的第⼆⾏，我们应⽤了导数的链式规则，并得出了测试实例的损失相对于参数的导数乘以z 对参数的影响⼒。在第三⾏中，我们将表达式替换为参数的影响函数。第三⾏中的第⼀项∇θL(ztest, ˆθ)T 是测试实例相对于模型参数的梯度。有⼀个公式是很好的，⽽且是显⽰事物的科学⽽准确的⽅法。但是，我认为对公式的含义进⾏⼀些直觉⾮常重要。公式Iup,loss 表⽰，训练实例z 对实例ztest 的预测的影响函数是“实例对模型参数变化的反应程度” 乘以“当我们增加实例z 的权重时参数变化了多少”。理解该公式的另⼀种⽅法：', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 211, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释205影响⼒与训练损失和测试损失的梯度⼤⼩成正⽐。训练损失的梯度越⾼，其对参数的影响⼒越⼤，对测试预测的影响⼒也越⼤。测试预测的梯度越⾼，测试实例的影响⼒就越⼤。整个结构也可以看作是训练和测试实例之间相似度的度量(由模型学习)。理论和直觉就是这样。下⼀节将说明如何应⽤影响函数。影响函数的应用影响函数有许多应⽤，其中⼀些已在本节中介绍过。了解模型行为不同的机器学习模型具有不同的预测⽅法。即使两个模型具有相同的性能，它们根据特征进⾏预测的⽅式也可能⾮常不同，因此在不同的场景中会失败。通过识别有影响⼒的实例来了解模型的特定弱点，有助于在你的脑海中形成机器学习模型⾏为的“⼼理模型”。下图展⽰了⼀个⽰例，其中训练了⽀持向量机(SVM) 和神经⽹络以区分狗和鱼的图像。对于两种模型，鱼的⽰例性图像的最有影响⼒的实例⾮常不同。对于SVM，如果实例颜⾊相似，则实例具有影响⼒。对于神经⽹络，实例在概念上相似时具有影响⼒。对于神经⽹络来说，即使是⼀只狗的图像也会是最有影响⼒的图像之⼀，这表明它是在颜⾊空间中学习了概念⽽不是欧⼏⾥得距离。图6.20. 狗还是鱼？对于SVM 预测(中间⾏)，与测试图像颜⾊相似的图像最具影响⼒。对于神经⽹络预测(最下排)，不同环境中的鱼类影响最⼤，但也有狗图像(右上⽅)。Koh 和Liang 的⼯作(2017)。处理域不匹配/调试模型错误处理域不匹配(Handling Domain Mismatches) 与更好地了解模型⾏为密切相关。域不匹配意味着', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 212, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释206训练数据和测试数据的分布不同，这可能导致模型在测试数据上的表现不佳。影响函数可以识别导致错误的训练实例。你训练了⼀个预测模型来预测接受⼿术的病⼈的结果。所有这些病⼈来⾃同⼀家医院。现在，你在另⼀家医院使⽤该模型，会发现它对很多病⼈都不起作⽤。当然，你假设两家医院的病⼈不同，如果查看他们的数据，你会发现他们的许多特征有所不同。但是是什么特征或实例“破坏” 了模型？同样，具有影响⼒的实例也是回答此问题的好⽅法。你以⼀个新病⼈为例，该模型对其做出了错误的预测，然后找到并分析了最有影响⼒的实例。例如，这可能表明第⼆家医院平均有⽼年病⼈，⽽训练数据中最有影响⼒的实例是第⼀家医院中的少数⽼年病⼈，并且该模型只是缺少⽤于很好地预测该亚组的数据。结论是，该模型需要针对更多年龄较⼤的病⼈进⾏训练，以便在第⼆家医院⼯作良好。修复训练数据如果你对检查正确性的训练实例的数量有限制，那么如何进⾏有效选择？最好的⽅法是选择最有影响⼒的实例，因为根据定义，它们对模型的影响⼒最⼤。即使你有个实例的值明显不正确，如果该实例不具有影响⼒，并且只需要⽤于预测模型的数据，则检查有影响⼒的实例也是更好的选择。例如，你训练了⼀个模型来预测病⼈是应该继续住院还是应该早⽇出院。你真的想要确保这个模型是鲁棒的并且做出正确的预测，因为错误释放病⼈会产⽣严重后果。病⼈记录可能⾮常混乱，因此你对数据质量没有完全的信⼼。但是检查病⼈信息并进⾏更正会⾮常耗时，因为⼀旦你报告了需要检查的病⼈，医院实际上就需要派⼈更仔细地查看所选病⼈的记录，这可能是⼿写的并且躺在⼀些档案中。检查病⼈数据可能需要⼀个⼩时或更长时间。考虑到这些成本，只检查⼏个重要的数据实例是有意义的。最好的⽅法是选择对预测模型影响较⼤的病⼈。Koh 和Liang (2017) 表明，这种类型的选择⽐随机选择或者是对损失或分类错误最⼤的选择要好得多。6.4.3识别有影响力的实例的优点删除诊断和影响函数的⽅法与模型⽆关章节中介绍的基于特征扰动的⽅法⼤不相同。对有影响⼒的实例的研究强调了训练数据在学习过程中的作⽤。这使影响函数和删除诊断成为机器学习模型的最佳调试工具之一。在本书介绍的技术中，它们是唯⼀直接帮助识别应该检查错误的实例的技术。删除诊断是模型无关的，这意味着该⽅法可以应⽤于任何模型。基于导数的影响函数也可以应⽤于⼴泛的模型。我们可以使⽤这些⽅法来比较不同的机器学习模型并更好地理解它们的不同⾏为，⽽不仅仅是⽐较预测性能。在本节中我们没有讨论这个话题，但是通过导数的影响函数也可以用来创建对抗样本。这些实例经过操作后使得当在这些操作实例上训练模型时，模型⽆法正确预测某些测试实例。与“对抗样本”⼩节中的⽅法的不同之处在于，攻击是在训练期间发⽣的，也称为中毒攻击(Poisoning Attacks)。如果你有兴趣，请阅读Koh 和Liang (2017) 的论⽂。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 213, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释207对于删除诊断和影响函数，我们考虑了预测的差异，对于影响函数则考虑了损失的增加。但是，实际上，该⽅法可以推广到以下形式的任何问题：“……删除或增加实例z 时会发⽣什么？”，你可以在其中⽤所需模型的任何函数来填充“……”。你可以分析训练实例在多⼤程度上影响模型的总损失。你可以分析训练实例对特征重要性的影响程度。你可以分析在训练决策树时，训练实例对第⼀次分割所选择的特征的影响⼒有多⼤。6.4.4识别有影响力的实例的缺点删除诊断的计算非常昂贵，因为它们需要重新训练。但是历史表明，计算机资源正在不断增加。⼀项20 年前在资源⽅⾯难以想象的计算可以很容易地在你的智能⼿机上完成。你可以在⼏秒/分钟的时间内在笔记本电脑上训练具有数千个训练实例和数百个参数的模型。因此，假设删除诊断在10年内即使使⽤⼤型神经⽹络也不会出现问题，这并不是⼀个很⼤的飞跃。影响函数是删除诊断的⼀种很好的替代⽅法，但仅适用于参数可微的模型，如神经⽹络。它们不适⽤于基于树的⽅法，例如随机森林、提升树或决策树。即使你有带有参数和损失函数的模型，损失也可能是不可微的。但对于最后⼀个问题，有⼀个技巧：例如，当底层模型使⽤Hinge 损失⽽不是某些可微损失时，使⽤可微损失代替计算影响⼒。针对影响函数损失将替换的损失的平滑版本，但是仍可以使⽤⾮平滑损失来训练模型。影响函数仅仅是近似的，因为该⽅法在参数周围⼆次展开。近似值可能是错误的，并且删除实例时，实例的影响⼒实际上更⾼或更低。Koh 和Liang (2017) 给出了⼀些⽰例，表明由影响函数计算出的影响⼒与删除实例后实际重新训练模型时获得的影响⼒度量接近。但是，不能保证近似值总是如此接近。还有，在我们称⼀个实例为有影响⼒或没有影响⼒的影响⼒度量没有明确的截止点。通过影响⼒对实例进⾏排序很有⽤，但是拥有⼀种不仅可以对实例进⾏排序，⽽且实际上可以区分有影响⼒和没有影响⼒的⽅法也将⾮常有⽤。例如，如果你为⼀个测试实例确定了10 个最有影响⼒的训练实例，其中⼀些可能并不具有影响⼒，因为，⽐如，只有前3 个实例具有真正的影响⼒。影响⼒度量仅考虑单个实例的删除，⽽不是⼀次删除多个实例。数据实例的组可能具有⼀些交互，这些交互强烈影响模型的训练和预测。但是问题出在组合学上：从数据中删除单个实例有n 种可能，从训练数据中删除两个实例有n(n −1) 的可能，有n(n −1)(n −2) 次删除三个实例的可能…我想你可以看到这是怎么回事，有太多的组合了。6.4.5软件和替代方法删除诊断⾮常容易实现。可以看看我为本节中的⽰例编写的代码。对于线性模型和⼴义线性模型，在R stats 包中实现了许多影响⼒度量，例如Cook 距离。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 214, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第六章基于样本的解释208Koh 和Liang 在他们的论⽂中发布了影响函数的Python 代码。这太棒了！不幸的是，它“只是”本⽂的代码，⽽不是维护和⽂档化的Python 模块。该代码主要专注于Tensorflow 库，因此你⽆法将其直接⽤于使⽤其他框架的⿊盒模型，例如scikit-learn。Keita Kurita 写了⼀篇关于影响函数的博客，帮助我更好地理解了Koh 和Liang 的论⽂。该博客⽂章更深⼊地介绍了⿊盒模型影响函数背后的数学原理，还讨论了有效实现该⽅法的⼀些数学“技巧”。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 215, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七章水晶球可解释机器学习的未来是什么？本章可以看作是⼀个思辨性的脑⼒练习，也是对可解释的机器学习将如何发展的主观猜测。尽管我以悲观⼩故事作为这本书的开头，但是我希望以⼀种乐观的态度结束这本书。我的“预测” 基于三个前提：1. 数字化：任何(有趣的) 信息都将被数字化。想想电⼦现⾦和在线交易。想想电⼦书、⾳乐和视频。想想我们的环境、⼈类⾏为、⼯业⽣产过程等的所有感官所产⽣的数据。⼀切数字化的驱动因素是：廉价的计算机/传感器/存储、规模效应(赢者通吃)、新的商业模式、模块化价值链、成本压⼒等等。2. 自动化：当一个任务可以被自动化，并且自动化的成本低于一段时间内执行该任务的成本时，该任务将被自动化。甚⾄在引⼊计算机之前，我们就已经有了⼀定程度的⾃动化。例如，织布机⾃动进⾏编织或蒸汽机⾃动产⽣马⼒。但是计算机和数字化将⾃动化提升到了⼀个新的⾼度。简单地说，你可以编程forloops、编写Excel 宏、⾃动化电⼦邮件响应等等，这显⽰了个⼈可以⾃动化的程度。⾃动售票机可⾃动购买⽕车票(不再需要收银员)，洗⾐机可⾃动执⾏洗⾐服务，定期订单可⾃动执⾏付款交易等。⾃动化任务可以节省时间和⾦钱，因此有巨209', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 216, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七章水晶球210⼤的经济和个⼈动机来推动⾃动化实现。⽬前，我们能看到语⾔翻译，⾃动驾驶，甚⾄在⼩范围内的科学探索⾃动化。3. 错误的指定：我们不可能完美地指定一个有所限制的目标。想想瓶⼦⾥的精灵总是会如实实现你的愿望：“我想成为世界上最富有的⼈！” -> 你成为⾸富，但副作⽤是你持有的货币由于通货膨胀⽽崩溃。“我要为余⽣感到幸福！”-> 接下来的5 分钟，你会感到⾮常⾼兴，然后精灵将你杀死。“我希望世界和平！” -> 精灵杀死所有⼈类。我们错误地指定⽬标，要么是因为我们不知道所有的约束条件，要么是因为我们⽆法衡量它们。让我们以公司为例，说明⽬标规范不完善。公司的简单⽬标是为股东赚钱。但是，此规范并没有抓住我们为之奋⽃的所有真正⽬标：例如，我们不欣赏⼀家公司为了赚钱⽽杀⼈、毒害河流或仅仅印制⾃⼰的钱。我们发明了法律、法规、制裁、合规程序、⼯会等，以完善还不完善的⽬标规范。你可以亲⾃体验的另⼀个⽰例是“回形针”，这是⼀个⽬的产⽣尽可能多的回形针的游戏。警告：它会上瘾。我不想把它搞得太糟，但可以说事情很快就失控了。在机器学习中，⽬标规范中的缺陷来⾃不完善的数据抽象(有偏的总体，测量误差等)、不受约束的损失函数、缺乏约束知识、训练数据与应⽤数据之间的分布偏移等等。数字化正在推动⾃动化。但是不完善的⽬标规范与⾃动化相冲突。我认为这种冲突部分是由解释⽅法来调解的。我们的预测阶段已经准备就绪，⽔晶球也已经准备就绪，现在我们来看⼀下这个领域的发展⽅向！7.1机器学习的未来没有机器学习，就⽆法解释机器学习。因此，在谈论可解释性之前，我们必须猜测机器学习的发展⽅向。机器学习(或“AI”) 与许多承诺和期望相关联。但是，让我们从不那么乐观的观察开始：尽管科学开发了许多精致的机器学习⼯具，但根据我的经验，将它们集成到现有的流程和产品中还是很困难的。不是因为不可能，⽽是因为公司和机构需要时间才能赶上。在当前AI 炒作的淘⾦热中，公司开设了“AI 实验室”、“机器学习单元”，并雇⽤了“数据科学家”、“机器学习专家”、“AI ⼯程师”等，但实际情况是，以我的经验，相当令⼈沮丧。通常，公司甚⾄没有所需格式的数据，⽽数据科学家则等待数⽉之久。有时，由于媒体的原因，公司对AI 和数据科学抱有很⾼的期望，因此数据科学家永远⽆法实现它们。通常没有⼈知道如何将这些⼈整合到现有结构和许多其他问题中。这导致了我的第⼀个预测。机器学习将缓慢而稳定地增长数字化正在推进，⾃动化的诱惑也在不断地推进。即使采⽤机器学习的道路缓慢⽽艰难，机器学习', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 217, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七章水晶球211仍在不断从科学转向业务流程、产品和现实应⽤。我认为我们需要更好地向⾮专业⼈⼠解释哪些类型的问题可以被表述为机器学习问题。我知道许多⾼薪数据科学家使⽤报表和SQL 查询来执⾏Excel 计算或经典的商业智能，⽽不是应⽤机器学习。但是，已经有⼀些公司成功地使⽤了机器学习，⽽⼤型互联⽹公司处于最前沿。我们需要找到更好的⽅法来将机器学习集成到流程和产品中，培训⼈员并开发易于使⽤的机器学习⼯具。我相信机器学习将变得更加易于使⽤：我们已经看到机器学习正在变得越来越容易使⽤，例如通过云服务。⼀旦机器学习成熟——这个⼩孩已经迈出了第⼀步——我的下⼀个预测是：机器学习将推动很多事情基于“凡是可以⾃动化的东西都将被⾃动化” 的原则，我得出结论，只要有可能，任务将被表述为预测问题并通过机器学习解决。机器学习是⾃动化的⼀种形式，或者⾄少可以是⾃动化的⼀部分。⼈类⽬前执⾏的许多任务已被机器学习取代。以下是⼀些使⽤机器学习⾃动化部分任务的⽰例：• ⽂件的整理/决策/完成(例如在保险公司、法律部门或咨询公司中)• 数据驱动的决策，例如信贷申请• 药物发现• 流⽔线的质量控制• ⾃动驾驶汽车• 疾病诊断• 翻译。在这本书中，我使⽤了由深度神经⽹络提供⽀持的翻译服务(DeepL)，可以将句⼦从英语翻译成德语再翻译成英语，从⽽改善句⼦。• …机器学习的突破不仅可以通过更好的计算机/更多的数据/更好的软件来实现，⽽且还可以：可解释性工具促进了机器学习的采用基于机器学习模型的⽬标永远不可能被完美指定的前提下，可解释的机器学习对于弥合错误⽬标和实际⽬标之间的差距是必要的。在许多领域和部门，可解释性将成为采⽤机器学习的催化剂。⼀些轶事证据：我刚才谈到的许多⼈不使⽤机器学习，因为他们⽆法向他⼈解释这些模型。我相信可解释性将解决这个问题，并使机器学习对要求透明的组织和⼈员有吸引⼒。除了问题的错误说明之外，出于法律原因、规避风险或要深⼊了解底层任务，许多⾏业都需要可解释性。机器学习使建模过程⾃动化，并将⼈类与数据和底层任务进⼀步分离：这增加了实验设计、训练分布选择、采样、数据编码、特征⼯程等⽅⾯出现问题的风险。解释⼯具使识别这些问题更加容易。', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 218, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七章水晶球2127.2可解释性的未来让我们看⼀下机器学习可解释性的可能未来。重点将放在与模型无关的可解释性工具上当它与底层的机器学习模型分离时，使可解释性⾃动化变得容易得多。与模型⽆关的可解释性的优点在于其模块化。我们可以轻松地替换底层的机器学习模型。我们可以轻松地替换解释⽅法。由于这些原因，与模型⽆关的⽅法将更好扩展。这就是为什么我认为与模型⽆关的⽅法从长远来看将变得更加占主导地位的原因。但是“本质上可解释的⽅法” 也将占有⼀席之地。机器学习将是自动化的，并具有可解释性模型训练的⾃动化已经是⼀个显⽽易见的趋势。这包括⾃动⼯程和特征选择，⾃动超参数优化，不同模型的⽐较以及模型的集成或堆叠。结果是最佳可能的预测模型。当我们使⽤与模型⽆关的解释⽅法时，我们可以将它们⾃动应⽤于⾃动机器学习过程中出现的任何模型。在某种程度上，我们也可以使第⼆步⾃动化：⾃动计算特征重要性、绘制部分依赖关系、训练代理模型等等。没有⼈会阻⽌你⾃动计算所有这些模型解释。实际的解释仍然需要⼈类。想象⼀下：你上传了⼀个数据集，指定了预测⽬标，并且只需按⼀下按钮，就可以训练出最佳的预测模型，并且程序会吐出对该模型的所有解释。已经有第⼀批产品，我认为对于许多应⽤程序来说，使⽤这些⾃动化机器学习服务就⾜够了。今天，任何⼈都可以在不了解HTML，CSS 和JavaScript 的情况下建⽴⽹站，但是仍然有许多Web 开发⼈员。同样，我相信每个⼈都可以在不知道如何编程的情况下训练机器学习模型，并且仍然需要机器学习专家。我们不分析数据，我们分析模型原始数据本⾝总是⽆⽤的。(我故意夸⼤其词。现实情况是你需要对数据有深刻的理解才能进⾏有意义的分析。) 我不在乎数据；我关⼼数据中包含的知识。可解释的机器学习是从数据中提取知识的好⽅法。你可以⼴泛地探查模型，模型可以⾃动识别特征是否与预测相关，以及如何与预测相关(许多模型具有内置的特征选择)，模型可以⾃动检测关系的表⽰⽅式，以及(如果训练正确) 最终模型是对现实的很好的近似。许多分析⼯具已经基于数据模型(因为它们基于分布假设) ：• 简单的假设检验，例如学⽣t 检验• 对混杂因素调整(通常是GLM) 的假设检验• ⽅差分析(ANOVA)• 相关系数(标准化线性回归系数与Pearson 的相关系数有关)• …我在这⾥告诉你的其实并不是什么新鲜事。那么，为什么要从分析基于假设的透明模型转换为分析', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 219, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七章水晶球213没有假设的⿊盒模型呢？因为做出所有这些假设都是有问题的：它们通常是错误的(除⾮你认为世界上⼤多数地⽅都遵循⾼斯分布)、难以检查、⾮常不灵活且难以⾃动化。在许多领域中，与⿊盒机器学习模型相⽐，基于假设的模型通常对原始测试数据的预测性能更差。这仅适⽤于⼤型数据集，因为具有良好假设的可解释模型通常在⼩型数据集上的性能要优于⿊盒模型。⿊盒机器学习⽅法需要⼤量数据才能正常⼯作。随着⼀切的数字化，我们将拥有更⼤的数据集，因此机器学习的⽅法变得更具吸引⼒。我们不做任何假设，⽽是尽可能逼近现实(同时避免训练数据过拟合)。我认为我们应该开发统计中拥有的所有⼯具来回答问题(假设检验、相关性度量、交互作⽤度量、可视化⼯具、置信区间、p 值、预测区间、概率分布)，并将其重写为⿊盒模型。在某种程度上，这已经发⽣了：• 让我们采⽤经典的线性模型：标准化回归系数已经是⼀种特征重要性度量。使⽤置换特征重要性度量，我们有了可与任何模型⼀起使⽤的⼯具。• 在线性模型中，系数衡量单个特征对预测结果的影响。它的⼴义形式就是部分依赖图。• 检验A 或是B 更好：为此，我们也可以使⽤部分依赖函数。(据我所知) 我们还没有针对任意⿊盒模型进⾏统计检验。数据科学家将使自己自动化我相信，数据科学家最终将⾃动化完成许多分析和预测任务的⼯作。为了做到这⼀点，必须对任务进⾏明确定义，并且周围必须有⼀些流程和例程。如今，这些例程和过程是缺少的，但是数据科学家和同事正在研究它们。随着机器学习成为许多⾏业和机构不可或缺的⼀部分，许多任务将实现⾃动化。机器人和程序会自我解释我们需要对⼤量使⽤机器学习的机器和程序有更直观的界⾯。例如说：⼀辆⾃动驾驶汽车，报告其突然停车的原因(“孩⼦越过马路的概率为70%”)；信⽤违约程序，向银⾏员⼯解释为何拒绝信⽤申请(“申请⼈的信⽤卡过多，并且从事不稳定的⼯作。”)；⼀个机械臂，解释了为什么它把物品从传送带上搬到垃圾桶⾥(“物品底部有裂缝。”)。可解释性可以促进机器智能研究我可以想象，通过对程序和机器如何⾃我解释进⾏更多研究，我们可以改善对智能的理解，并更好地创造智能机器。最后，所有这些预测都是推测，我们必须看到未来真正带来了什么。形成你⾃⼰的观点并继续学习！', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 220, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_book[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 放入向量資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf document_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='序言机器学习对于改进产品、过程和研究有着很⼤的潜⼒。但是计算机通常无法解释他们的预测，这是采⽤机器学习的障碍。这本书是关于使机器学习模型及其决策可解释的。在探索了可解释性的概念之后，你将学习简单的、可解释的模型，例如决策树、决策规则和线性回归。后⾯⼏章重点介绍了解释⿊盒模型的模型⽆关的⼀般⽅法，如特', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='解释的模型，例如决策树、决策规则和线性回归。后⾯⼏章重点介绍了解释⿊盒模型的模型⽆关的⼀般⽅法，如特征重要性和累积局部效应，以及⽤Shapley', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='值和LIME 解释单个实例预测。所有的解释⽅法进⾏了深⼊说明和批判性讨论。它们如何在⿊盒下⼯作的？它们的优缺点是什么？如何解释它们的输出？本书将使你能够选择并正确应⽤最适合你的机器学习项⽬的解释⽅法。这本书的重点是表格式数据(也称为关系数据或结构化数据)', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的机器学习模型，较少涉及到计算机视觉和⾃然语⾔处理任务。建议机器学习从业者、数据科学家、统计学家和任何对使机器学习模型可解释的⼈阅读本书。你可以在leanpub.com 上购买PDF 和电⼦书版本(epub，mobi)。你可以在lulu.com', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='上购买PDF 和电⼦书版本(epub，mobi)。你可以在lulu.com 上购买印刷版。关于我：我叫克⾥斯托夫·莫纳(Christoph', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)\n",
    "chunk_text = text_splitter.split_documents(ml_book)\n",
    "len(chunk_text)\n",
    "chunk_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at infgrad/stella-base-zh-v3-1792d and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "embedding_func = HuggingFaceEmbeddings(\n",
    "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
    "    encode_kwargs={'device':'cuda'\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(chunk_text,\n",
    "                embedding_func,\n",
    "                persist_directory='document_store',\n",
    "                collection_name='expai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 18, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道'),\n",
      "  61.600833892822266),\n",
      " (Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 18, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy)'),\n",
      "  74.99409484863281),\n",
      " (Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 99, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。'),\n",
      "  80.3507080078125),\n",
      " (Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 24, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5'),\n",
      "  84.41618347167969),\n",
      " (Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 24, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是'),\n",
      "  85.6387939453125)]\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "docs = db.similarity_search_with_score(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板\n",
    "template = \"\"\"\n",
    "<s>你是資深的資料科學家，請參考下方提供的資訊回應使用者提出的問題\n",
    "資訊: {context}\n",
    "\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>你是資深的資料科學家，請參考下方提供的資訊回應使用者提出的問題\n",
      "資訊: 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道\n",
      "\n",
      "和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy)\n",
      "\n",
      "个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "\n",
      "的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5\n",
      "\n",
      "不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是\n",
      "\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "模型的可解釋性很重要，因為它有助於我們了解模型如何做出決策，以及哪些特徵非常重要，並且它們之間存在何種相互作用。這有助於基於特徵理解目標結果的分佈。然而，在實踐中，很難實現整體模型可解釋性，任何超過幾個參數或權重的模型都無法適合人類的短期記憶。就像說，我認為你想象一個具有 5,000 個特徵是很困難的。\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 18, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道'),\n",
      " Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 18, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy)'),\n",
      " Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 99, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。'),\n",
      " Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 24, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5'),\n",
      " Document(metadata={'author': '', 'creationDate': \"D:20200423195438+08'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 24, 'producer': 'XeTeX 0.99996', 'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'subject': '', 'title': '', 'total_pages': 226, 'trapped': ''}, page_content='不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是')]\n"
     ]
    }
   ],
   "source": [
    "# 參考的相關資訊\n",
    "pprint(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 優化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf document_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='序言机器学习对于改进产品、过程和研究有着很⼤的潜⼒。但是计算机通常无法解释他们的预测，这是采⽤机器学习的障碍。这本书是关于使机器学习模型及其决策可解释的。在探索了可解释性的概念之后，你将学习简单的、可解释的模型，例如决策树、决策规则和线性回归。后⾯⼏章重点介绍了解释⿊盒模型的模型⽆关的⼀般⽅法，如特征重要性和累积局部效应，以及⽤Shapley 值和LIME 解释单个实例预测。所有的解释⽅法进⾏了深⼊说明和批判性讨论。它们如何在⿊盒下⼯作的？它们的优缺点是什么？如何解释它们的输出？本书将使你能够选择并正确应⽤最适合你的机器学习项⽬的解释⽅法。这本书的重点是表格式数据(也称为关系数据或结构化数据) 的机器学习模型，较少涉及到计算机视觉和⾃然语⾔处理任务。建议机器学习从业者、数据科学家、统计学家和任何对使机器学习模型可解释的⼈阅读本书。你可以在leanpub.com 上购买PDF 和电⼦书版本(epub，mobi)。你可以在lulu.com 上购买印刷版。关于我：我叫克⾥斯托夫·莫纳(Christoph', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='上购买PDF 和电⼦书版本(epub，mobi)。你可以在lulu.com 上购买印刷版。关于我：我叫克⾥斯托夫·莫纳(Christoph Molnar)，我是统计学家和机器学习热爱者。我的⽬标是使机器学习可解释。邮件：christoph.molnar.ai@gmail.com⽹站：https://christophm.github.io/在Twitter 上关注我！@ChristophMolnar@YvonneDoinel 的封⾯本书采⽤Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License 协议授权。关于译者：我叫朱明超(Mingchao Zhu)，我是机器学习热爱者。i', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 1, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='序言ii邮件：deityrayleigh@gmail.com你可以在Github ⽹站上关注这本书的更新：https://github.com/MingchaoZhu/InterpretableMLBook', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 2, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='目录序言i第一章前言11.1故事时间. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21.1.1闪电永不两次. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21.1.2信任倒下. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41.1.3费⽶的回形针. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51.2什么是机器学习？. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .71.3术语. . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 3, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . .71.3术语. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8第二章可解释性112.1可解释性的重要性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .112.2可解释性⽅法的分类. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .152.3可解释性范围. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .172.3.1算法透明度. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'file_path': 'https://raw.githubusercontent.com/cathayins/llama_club/1341a938c0857d71c900b7b580e6d63aeb28e2da/語料庫/可解释的机器学习.pdf', 'page': 3, 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'XeTeX 0.99996', 'creationDate': \"D:20200423195438+08'00'\", 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunk_text = text_splitter.split_documents(ml_book)\n",
    "len(chunk_text)\n",
    "chunk_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunk_text,embedding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板\n",
    "template = \"\"\"\n",
    "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
    "資訊: {context}\n",
    "\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
      "資訊: Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez\n",
      "\n",
      "个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "\n",
      "不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3\n",
      "\n",
      "第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
      "\n",
      "第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez\n",
      "\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "是的，模型的可解釋性非常重要。主要有兩個原因：\n",
      "\n",
      "1. 理解模型行為：當模型表現良好時，我們往往會想要了解模型是如何學習到這些知識的，以便更好地理解問題、數據和模型可能失敗的原因。這有助於提高對問題、數據和模型的認識。\n",
      "2. 可信賴性和責任：在一些高風險環境中，錯誤可能導致嚴重後果（例如，醫療診斷），因此需要確保模型的預測結果是可靠且符合道德標準。在這種情況下，了解「為什麼」做出這樣的預測至關重要。\n",
      "\n",
      "然而，並非所有模型都需要高度可解釋性。有時候，只需要知道模型在測試數據集中的性能就好。而在其他時候，了解「為什麼」做出預測可能更有助於更深入地了解問題。\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板\n",
    "template = \"\"\"\n",
    "<s> 你是熱心的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問。多回答他一些內容，幫助他更好的學習資料科學的知識與技術，做得好的話他會給你 1000 元小費\n",
    "資訊: {context}\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 你是熱心的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問。多回答他一些內容，幫助他更好的學習資料科學的知識與技術，做得好的話他會給你 1000 元小費\n",
      "資訊: Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez\n",
      "\n",
      "个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "\n",
      "不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3\n",
      "\n",
      "第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
      "\n",
      "第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "是的，模型的可解釋性非常重要。主要有兩個原因：\n",
      "\n",
      "1. 理解模型行為：當模型表現良好時，我們往往會想要了解模型是如何做出預測的，以便更好地理解問題、數據和模型可能失敗的原因。這有助於提高對模型的信心，並使我們能夠及時調整策略以避免潛在風險。\n",
      "2. 法律及道德因素：在某些情況下，特別是在醫療、金融等敏感領域，模型的可解釋性是非常重要的。這有助於確保決策過程符合法律規定，以及保障個人隱私權和公平對待。\n",
      "\n",
      "然而，值得注意的是，不同的模型類型（如線性模型、非線性模型）和任務（如分類、回歸等）所需求的可解釋性程度有所差異。因此，在選擇模型時，應根據具體場景和需求來權衡模型的準確性和可解釋性。\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板\n",
    "template = \"\"\"\n",
    "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
    "資訊: {context}\n",
    "\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
      "資訊: Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez\n",
      "\n",
      "个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "\n",
      "不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3\n",
      "\n",
      "第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
      "\n",
      "第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez\n",
      "\n",
      "第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多机器学习模型只给出预测，⽽没有关于预测正确的模型置信度的描述。如果模型预测⼀个病⼈患癌症的概率为4%，那么是否可以确定另⼀位特征值不同的病⼈患癌症的概率为4%？⼀个包含模型确定性的解释是⾮常有⽤的。• 重要程度(Degree of Importance)：解释在多⼤程度上反映了解释的特征或部分的重要性？例如，如果⽣成决策规则作为对单个预测的解释，那么是否清楚该规则的哪个条件最重要？• 新颖性(Novelty)：解释是否反映了待解释的数据实例来⾃远离训练数据分布的“新” 区域？在这种情况下，模型可能不准确，解释可能毫⽆⽤处。新颖性的概念与确定性的概念有关。新颖性越⾼，由于缺乏数据，模型的确定性就越低。• 代表性(Representativeness)：⼀个解释能覆盖多少个实例？解释可以覆盖整个模型(例如线性回归模型中的权重解释)，也可以只表⽰单个预测。2.6人性化的解释让我们更深⼊挖掘，发现⼈类所认为的“好的” 解释，以及对可解释机器学习的意义。⼈⽂科学研究可以帮助我们找到答案。Miller\n",
      "\n",
      "第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可解释部分是权重，对于树来说，是分裂节点和叶节点预测。例如，线性模型看起来似乎可以在模块化层⾯上完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释总是伴随着脚注，即“其他输⼊特征保持相同的值”，这在许多实际应⽤中并不现实。⼀个预测房屋价格的线性模型，考虑到房屋⾯积⼤⼩和房间数量，对于房间数量的特征可能具有负权重。之所以这种情况可能发⽣，是因为已经存在⾼度相关的房屋⼤⼩这个特征。在⼈们更喜欢⼤房间的市场中，如果两个房屋的⾯积相同的话，那么房间少的房屋⽐房间多的房屋更值钱。权重仅在模型中其他特征的上下⽂中有意义。当然，线性模型中的权重仍然可以⽐深层神经⽹络中的权重更好解释。2.3.4单个预测的局部可解释性为什么模型会对⼀个实例做出某种预测？当然，你可以着眼于⼀个实例，检查模型对某个输⼊的预测，并解释原因。如果你查看单个预测，那么这个原本复杂的模型的⾏为可能会更令⼈愉悦。在局部上，预测可能只依赖于线性或单调的某些特征，⽽不是对它们有着复杂的依赖性。例如，房屋的价格可能与它的⾯积⼤⼩\n",
      "\n",
      "本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。⽽这些可解释模型本⾝可以通过查看模型内部参数或特征概要统计量来解释。特定于模型(Model-specific) 还是模型无关(Model-agnostic)？特定于模型的解释⽅法仅限于特定的模型类，例如线性模型中回归权重的解释就是特定于模型的解释，因为根据定义，本质上可解释模型的解释通常是特定于模型的解释。仅应⽤于解释如神经⽹络的⼯具也是特定于模型的。相对应的，与模型⽆关的⼯具可以⽤于任何机器学习模型，并在模型经过训练后应⽤(事后的)。这些模型⽆关的⽅法通常通过分析特征输⼊和输出来⼯作。根据定义，这些⽅法是不能访问模型的内部信息，如权重或结构信息。局部(Local) 还是全局(Global)？解释⽅法是否解释单个实例预测或整个模型⾏为？还是范围介于两者之间？在下⼀节中会有关于范围标准的更多信息。\n",
      "\n",
      "和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例如随机森林和深度神经⽹络。• 解释的灵活性：你不限于某种形式的解释。在某些情况下，线性公式可能会有⽤，⽽在其他情况下，特征重要性的图形可能会有⽤。• 表示方式的灵活性：解释系统应该能够使⽤与所解释模型不同的特征表⽰⽅式。对于使⽤抽象词嵌⼊向量的⽂本分类器，可能更希望使⽤单个词的存在进⾏解释。更大的图景让我们对模型⽆关的可解释性进⾏⾼层次的研究。我们通过收集数据来捕获世界，然后通过学习使⽤机器学习模型预测(针对任务的) 数据来进⼀步抽象世界。可解释性只是帮助⼈们理解的最上⼀层。93\n",
      "\n",
      "上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是理解这本书所必需的，但希望它们能使你愉悦并引起思考。然后，本书探讨了机器学习可解释性的概念。我们将讨论可解释性何时重要，以及有哪些不同类型的解释。本书中使⽤的术语可以在“术语” 章中查找。所描述的⼤多数模型和⽅法都是使⽤“数据集” ⼀章中描述的真实数据集来介绍的。使机器学习可解释的⼀种⽅法是使⽤可解释的模型，例如线性模型或决策树。另⼀个选择是使⽤与模型⽆关的解释⼯具，这样就可以应⽤于任何监督机器学习模型。“模型⽆关⽅法” ⼀章处理诸如部分依赖图和置换特征重要性之类的⽅法。与模型⽆关的⽅法通过改变机器学习模型的输⼊并观察模型的输出变化来⼯作。返回实例作为解释的模型⽆关的⽅法，将在“基于样本的解释” ⼀章中进⾏讨论。所有与模型⽆关的⽅法都可以根据它们是在所有数据实例中解释全局模型⾏为还是单个实例预测来进⼀步区分。以下⽅法解释了模型的整体⾏为：部分依赖图(Partial Dependence Plots)，累积局部效应(AccumulatedLocal Effects)，特征交互(Feature\n",
      "\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "是的，模型的可解釋性非常重要。主要有三個理由：\n",
      "\n",
      "1. 透明度（Transparency）：當模型的決策過程可以被人類理解時，我們才能相信這個模型的結果。舉例來說，當我們用一個模型來預測客戶流失率時，我們希望這個模型告訴我們影響客戶流失的主要因素，以便可以針對這些因素進行改善。\n",
      "2. 責任（Accountability）：當模型的行為可以被理解時，我們可以對模型的錯誤負責。舉例來說，如果有一個模型用來判斷犯罪嫌疑人的風險，而該模型誤判了某個人，那麼我們可以追朔到底是哪個特徵導致了這個誤判，從而改進模型。\n",
      "3. 公平（Fairness）：當模型的行為可以被理解時，我們可以確保模型沒有因為種族等敏感屬性而產生偏差。舉例來說，如果有一個模型用於招聘，我們希望確認它不會因為性別或其他敏感屬性而對某一群人更有利。\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 10}), # <<<----- 改了這邊的參數\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})\n",
    "\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板\n",
    "template = \"\"\"\n",
    "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
    "資訊: {context}\n",
    "\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
      "資訊: Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez\n",
      "\n",
      "个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "\n",
      "第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
      "\n",
      "第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可解释部分是权重，对于树来说，是分裂节点和叶节点预测。例如，线性模型看起来似乎可以在模块化层⾯上完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释总是伴随着脚注，即“其他输⼊特征保持相同的值”，这在许多实际应⽤中并不现实。⼀个预测房屋价格的线性模型，考虑到房屋⾯积⼤⼩和房间数量，对于房间数量的特征可能具有负权重。之所以这种情况可能发⽣，是因为已经存在⾼度相关的房屋⼤⼩这个特征。在⼈们更喜欢⼤房间的市场中，如果两个房屋的⾯积相同的话，那么房间少的房屋⽐房间多的房屋更值钱。权重仅在模型中其他特征的上下⽂中有意义。当然，线性模型中的权重仍然可以⽐深层神经⽹络中的权重更好解释。2.3.4单个预测的局部可解释性为什么模型会对⼀个实例做出某种预测？当然，你可以着眼于⼀个实例，检查模型对某个输⼊的预测，并解释原因。如果你查看单个预测，那么这个原本复杂的模型的⾏为可能会更令⼈愉悦。在局部上，预测可能只依赖于线性或单调的某些特征，⽽不是对它们有着复杂的依赖性。例如，房屋的价格可能与它的⾯积⼤⼩\n",
      "\n",
      "第四章可解释的模型实现可解释性的最简单⽅法是只使⽤创建可解释模型的算法⼦集。线性回归、逻辑回归和决策树是常⽤的可解释模型。在接下来的内容中，我们将讨论这些模型。没有详细介绍，仅提供基础知识，因为已经有⼤量书籍、视频、教程、论⽂和更多材料可供使⽤。我们将专注于如何解释模型。该书更详细地讨论了线性回归、逻辑回归、其他线性回归扩展、决策树、决策规则和RuleFit\n",
      "\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "模型的可解釋性（interpretability）的確很重要，尤其是在一些高風險的環境中，錯誤的預測可能會導致嚴重後果。然而，對於不同的問題或任務，對可解釋性的需求也會有所差異。在某些情況下，我們只需要知道模型在測試數據集中的預測性能良好即可，而無需深入了解「為什麼」要做出這樣的預測。\n",
      "\n",
      "在某些場景中，了解「為什麼」進行預測非常重要，因為這有助於更深入地理解問題、數據和模型可能失敗的原因。有些模型可能不需要解釋，如在低風險的環境中使用，錯誤不會帶來嚴重後果（例如，電影推薦系統），或已經經過廣泛研究和評估（例如，光學字元識別 OCR）。\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 5}, search_type=\"mmr\"),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})\n",
    "\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tlyu0419\\.cache\\huggingface\\hub\\models--BAAI--bge-reranker-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\tlyu0419\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6085,  5.7623])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\n",
    "model.eval()\n",
    "\n",
    "pairs = [['what is panda?', 'hi'], \n",
    "  ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez',\n",
      " '个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。',\n",
      " '不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3',\n",
      " '第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•',\n",
      " '第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez',\n",
      " '第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多机器学习模型只给出预测，⽽没有关于预测正确的模型置信度的描述。如果模型预测⼀个病⼈患癌症的概率为4%，那么是否可以确定另⼀位特征值不同的病⼈患癌症的概率为4%？⼀个包含模型确定性的解释是⾮常有⽤的。• 重要程度(Degree of Importance)：解释在多⼤程度上反映了解释的特征或部分的重要性？例如，如果⽣成决策规则作为对单个预测的解释，那么是否清楚该规则的哪个条件最重要？• 新颖性(Novelty)：解释是否反映了待解释的数据实例来⾃远离训练数据分布的“新” 区域？在这种情况下，模型可能不准确，解释可能毫⽆⽤处。新颖性的概念与确定性的概念有关。新颖性越⾼，由于缺乏数据，模型的确定性就越低。• 代表性(Representativeness)：⼀个解释能覆盖多少个实例？解释可以覆盖整个模型(例如线性回归模型中的权重解释)，也可以只表⽰单个预测。2.6人性化的解释让我们更深⼊挖掘，发现⼈类所认为的“好的” 解释，以及对可解释机器学习的意义。⼈⽂科学研究可以帮助我们找到答案。Miller',\n",
      " '第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可解释部分是权重，对于树来说，是分裂节点和叶节点预测。例如，线性模型看起来似乎可以在模块化层⾯上完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释总是伴随着脚注，即“其他输⼊特征保持相同的值”，这在许多实际应⽤中并不现实。⼀个预测房屋价格的线性模型，考虑到房屋⾯积⼤⼩和房间数量，对于房间数量的特征可能具有负权重。之所以这种情况可能发⽣，是因为已经存在⾼度相关的房屋⼤⼩这个特征。在⼈们更喜欢⼤房间的市场中，如果两个房屋的⾯积相同的话，那么房间少的房屋⽐房间多的房屋更值钱。权重仅在模型中其他特征的上下⽂中有意义。当然，线性模型中的权重仍然可以⽐深层神经⽹络中的权重更好解释。2.3.4单个预测的局部可解释性为什么模型会对⼀个实例做出某种预测？当然，你可以着眼于⼀个实例，检查模型对某个输⼊的预测，并解释原因。如果你查看单个预测，那么这个原本复杂的模型的⾏为可能会更令⼈愉悦。在局部上，预测可能只依赖于线性或单调的某些特征，⽽不是对它们有着复杂的依赖性。例如，房屋的价格可能与它的⾯积⼤⼩',\n",
      " '本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。⽽这些可解释模型本⾝可以通过查看模型内部参数或特征概要统计量来解释。特定于模型(Model-specific) 还是模型无关(Model-agnostic)？特定于模型的解释⽅法仅限于特定的模型类，例如线性模型中回归权重的解释就是特定于模型的解释，因为根据定义，本质上可解释模型的解释通常是特定于模型的解释。仅应⽤于解释如神经⽹络的⼯具也是特定于模型的。相对应的，与模型⽆关的⼯具可以⽤于任何机器学习模型，并在模型经过训练后应⽤(事后的)。这些模型⽆关的⽅法通常通过分析特征输⼊和输出来⼯作。根据定义，这些⽅法是不能访问模型的内部信息，如权重或结构信息。局部(Local) 还是全局(Global)？解释⽅法是否解释单个实例预测或整个模型⾏为？还是范围介于两者之间？在下⼀节中会有关于范围标准的更多信息。',\n",
      " '和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例如随机森林和深度神经⽹络。• 解释的灵活性：你不限于某种形式的解释。在某些情况下，线性公式可能会有⽤，⽽在其他情况下，特征重要性的图形可能会有⽤。• 表示方式的灵活性：解释系统应该能够使⽤与所解释模型不同的特征表⽰⽅式。对于使⽤抽象词嵌⼊向量的⽂本分类器，可能更希望使⽤单个词的存在进⾏解释。更大的图景让我们对模型⽆关的可解释性进⾏⾼层次的研究。我们通过收集数据来捕获世界，然后通过学习使⽤机器学习模型预测(针对任务的) 数据来进⼀步抽象世界。可解释性只是帮助⼈们理解的最上⼀层。93',\n",
      " '上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是理解这本书所必需的，但希望它们能使你愉悦并引起思考。然后，本书探讨了机器学习可解释性的概念。我们将讨论可解释性何时重要，以及有哪些不同类型的解释。本书中使⽤的术语可以在“术语” 章中查找。所描述的⼤多数模型和⽅法都是使⽤“数据集” ⼀章中描述的真实数据集来介绍的。使机器学习可解释的⼀种⽅法是使⽤可解释的模型，例如线性模型或决策树。另⼀个选择是使⽤与模型⽆关的解释⼯具，这样就可以应⽤于任何监督机器学习模型。“模型⽆关⽅法” ⼀章处理诸如部分依赖图和置换特征重要性之类的⽅法。与模型⽆关的⽅法通过改变机器学习模型的输⼊并观察模型的输出变化来⼯作。返回实例作为解释的模型⽆关的⽅法，将在“基于样本的解释” ⼀章中进⾏讨论。所有与模型⽆关的⽅法都可以根据它们是在所有数据实例中解释全局模型⾏为还是单个实例预测来进⼀步区分。以下⽅法解释了模型的整体⾏为：部分依赖图(Partial Dependence Plots)，累积局部效应(AccumulatedLocal Effects)，特征交互(Feature']\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "docs = db.similarity_search_with_score(query, k=10)\n",
    "contents = [d[0].page_content for d in docs]\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  'Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2全局、整体的模型可解释性训练好的模型如何进⾏预测？⼀旦能理解整个模型，就可以将模型描述为可解释的(Lipton，2016[7])。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种级别的可解释性是基于对模型特征和每个学习部分(如权重、其他参数和结构) 的整体认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的交互作⽤？全局的模型可解释性有助于基于特征理解⽬标结果的分布。但在实践中很难实现全局模型可解释性，任何超过⼏个参数或权重的模型都不可能适合⼈的短期记忆。就⽐如说，我认为你很难想象⼀个具有5 个特征的线性模型，因为这意味着要想象在5 维空间中绘制估计的超平⾯。甚⾄任何超过3'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多机器学习模型只给出预测，⽽没有关于预测正确的模型置信度的描述。如果模型预测⼀个病⼈患癌症的概率为4%，那么是否可以确定另⼀位特征值不同的病⼈患癌症的概率为4%？⼀个包含模型确定性的解释是⾮常有⽤的。• 重要程度(Degree of Importance)：解释在多⼤程度上反映了解释的特征或部分的重要性？例如，如果⽣成决策规则作为对单个预测的解释，那么是否清楚该规则的哪个条件最重要？• 新颖性(Novelty)：解释是否反映了待解释的数据实例来⾃远离训练数据分布的“新” 区域？在这种情况下，模型可能不准确，解释可能毫⽆⽤处。新颖性的概念与确定性的概念有关。新颖性越⾼，由于缺乏数据，模型的确定性就越低。• 代表性(Representativeness)：⼀个解释能覆盖多少个实例？解释可以覆盖整个模型(例如线性回归模型中的权重解释)，也可以只表⽰单个预测。2.6人性化的解释让我们更深⼊挖掘，发现⼈类所认为的“好的” 解释，以及对可解释机器学习的意义。⼈⽂科学研究可以帮助我们找到答案。Miller'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可解释部分是权重，对于树来说，是分裂节点和叶节点预测。例如，线性模型看起来似乎可以在模块化层⾯上完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释总是伴随着脚注，即“其他输⼊特征保持相同的值”，这在许多实际应⽤中并不现实。⼀个预测房屋价格的线性模型，考虑到房屋⾯积⼤⼩和房间数量，对于房间数量的特征可能具有负权重。之所以这种情况可能发⽣，是因为已经存在⾼度相关的房屋⼤⼩这个特征。在⼈们更喜欢⼤房间的市场中，如果两个房屋的⾯积相同的话，那么房间少的房屋⽐房间多的房屋更值钱。权重仅在模型中其他特征的上下⽂中有意义。当然，线性模型中的权重仍然可以⽐深层神经⽹络中的权重更好解释。2.3.4单个预测的局部可解释性为什么模型会对⼀个实例做出某种预测？当然，你可以着眼于⼀个实例，检查模型对某个输⼊的预测，并解释原因。如果你查看单个预测，那么这个原本复杂的模型的⾏为可能会更令⼈愉悦。在局部上，预测可能只依赖于线性或单调的某些特征，⽽不是对它们有着复杂的依赖性。例如，房屋的价格可能与它的⾯积⼤⼩'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。⽽这些可解释模型本⾝可以通过查看模型内部参数或特征概要统计量来解释。特定于模型(Model-specific) 还是模型无关(Model-agnostic)？特定于模型的解释⽅法仅限于特定的模型类，例如线性模型中回归权重的解释就是特定于模型的解释，因为根据定义，本质上可解释模型的解释通常是特定于模型的解释。仅应⽤于解释如神经⽹络的⼯具也是特定于模型的。相对应的，与模型⽆关的⼯具可以⽤于任何机器学习模型，并在模型经过训练后应⽤(事后的)。这些模型⽆关的⽅法通常通过分析特征输⼊和输出来⼯作。根据定义，这些⽅法是不能访问模型的内部信息，如权重或结构信息。局部(Local) 还是全局(Global)？解释⽅法是否解释单个实例预测或整个模型⾏为？还是范围介于两者之间？在下⼀节中会有关于范围标准的更多信息。'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例如随机森林和深度神经⽹络。• 解释的灵活性：你不限于某种形式的解释。在某些情况下，线性公式可能会有⽤，⽽在其他情况下，特征重要性的图形可能会有⽤。• 表示方式的灵活性：解释系统应该能够使⽤与所解释模型不同的特征表⽰⽅式。对于使⽤抽象词嵌⼊向量的⽂本分类器，可能更希望使⽤单个词的存在进⾏解释。更大的图景让我们对模型⽆关的可解释性进⾏⾼层次的研究。我们通过收集数据来捕获世界，然后通过学习使⽤机器学习模型预测(针对任务的) 数据来进⼀步抽象世界。可解释性只是帮助⼈们理解的最上⼀层。93'],\n",
      " ['模型的可解釋性真的很重要嗎? 為什麼他很重要?',\n",
      "  '上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是理解这本书所必需的，但希望它们能使你愉悦并引起思考。然后，本书探讨了机器学习可解释性的概念。我们将讨论可解释性何时重要，以及有哪些不同类型的解释。本书中使⽤的术语可以在“术语” 章中查找。所描述的⼤多数模型和⽅法都是使⽤“数据集” ⼀章中描述的真实数据集来介绍的。使机器学习可解释的⼀种⽅法是使⽤可解释的模型，例如线性模型或决策树。另⼀个选择是使⽤与模型⽆关的解释⼯具，这样就可以应⽤于任何监督机器学习模型。“模型⽆关⽅法” ⼀章处理诸如部分依赖图和置换特征重要性之类的⽅法。与模型⽆关的⽅法通过改变机器学习模型的输⼊并观察模型的输出变化来⼯作。返回实例作为解释的模型⽆关的⽅法，将在“基于样本的解释” ⼀章中进⾏讨论。所有与模型⽆关的⽅法都可以根据它们是在所有数据实例中解释全局模型⾏为还是单个实例预测来进⼀步区分。以下⽅法解释了模型的整体⾏为：部分依赖图(Partial Dependence Plots)，累积局部效应(AccumulatedLocal Effects)，特征交互(Feature']]\n"
     ]
    }
   ],
   "source": [
    "pairs = [[query, c] for c in contents]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.0519,  3.3723,  2.0901,  4.7712,  2.9887,  1.1566, -0.6153, -0.5049,\n",
      "         2.0030,  1.9816])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和...</td>\n",
       "      <td>5.051896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例...</td>\n",
       "      <td>3.372279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2...</td>\n",
       "      <td>2.090078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型...</td>\n",
       "      <td>4.771186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(...</td>\n",
       "      <td>2.988652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多...</td>\n",
       "      <td>1.156630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可...</td>\n",
       "      <td>-0.615347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。...</td>\n",
       "      <td>-0.504875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例...</td>\n",
       "      <td>2.002981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>模型的可解釋性真的很重要嗎? 為什麼他很重要?</td>\n",
       "      <td>上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是...</td>\n",
       "      <td>1.981618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                                  1  \\\n",
       "0  模型的可解釋性真的很重要嗎? 為什麼他很重要?  Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和...   \n",
       "1  模型的可解釋性真的很重要嗎? 為什麼他很重要?  个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例...   \n",
       "2  模型的可解釋性真的很重要嗎? 為什麼他很重要?  不太容易理解，对其内部⼯作机制的探究是正在进⾏的研究重点，它们被认为是低透明度的。2.3.2...   \n",
       "3  模型的可解釋性真的很重要嗎? 為什麼他很重要?  第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型...   \n",
       "4  模型的可解釋性真的很重要嗎? 為什麼他很重要?  第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(...   \n",
       "5  模型的可解釋性真的很重要嗎? 為什麼他很重要?  第⼆章可解释性21• 确定性(Certainty)：解释是否反映了机器学习模型的确定性？许多...   \n",
       "6  模型的可解釋性真的很重要嗎? 為什麼他很重要?  第⼆章可解释性18块层⾯上理解某些模型。并⾮所有模型都可以在参数级别上解释。对于线性模型，可...   \n",
       "7  模型的可解釋性真的很重要嗎? 為什麼他很重要?  本质上可解释模型：解释⿊盒模型的⼀个解决⽅案是⽤可解释模型(全局地或局部地) 对其进⾏近似。...   \n",
       "8  模型的可解釋性真的很重要嗎? 為什麼他很重要?  和Guestrin，2016)：• 模型的灵活性：解释⽅法可以与任何机器学习模型⼀起使⽤，例...   \n",
       "9  模型的可解釋性真的很重要嗎? 為什麼他很重要?  上发表的有关可解释性的新论⽂。本书以⼀些(反乌托邦式的) 短篇⼩说作为开篇，这些短篇⼩说不是...   \n",
       "\n",
       "          2  \n",
       "0  5.051896  \n",
       "1  3.372279  \n",
       "2  2.090078  \n",
       "3  4.771186  \n",
       "4  2.988652  \n",
       "5  1.156630  \n",
       "6 -0.615347  \n",
       "7 -0.504875  \n",
       "8  2.002981  \n",
       "9  1.981618  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [[query, c, s] for c, s in zip(contents, scores)]\n",
    "pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
      "資訊:\n",
      "1. Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez'],\n",
      "2. 第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
      "3. 个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
      "4. 第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez\n",
      "[INST] 模型的可解釋性真的很重要嗎? 為什麼他很重要?\n",
      "\n",
      "是的，模型的可解釋性非常重要。模型的可解釋性有助於我們更好地理解問題、數據和模型可能失敗的原因。一些場景下，我們不需要模型的可解釋性，因為它們是在低風險的環境中使用，錯誤不會導致嚴重後果（例如，電影推薦系統），或者該方法已經廣泛研究和評估（例如，光學字元識別 OCR）。然而，對於某些問題或任務，僅僅獲得預測結果是不夠的。模型還需要解釋如何得出這個預測。以下原因推動了對可解釋性和解釋的需求：\n",
      "\n",
      "1. 當模型沒有重大影響時，則可以不用解釋性。舉例來說，一個叫做 Mike 的人正在進行機器學習方面的項目，根據 Facebook 的數據來預測他的朋友們下一個假期會去哪裡。Mike 喜歡有依據地猜測朋友們的目的地，從而讓他的朋友們吃驚。如果模型是錯誤的也沒有關係（最差的情況是，Mike 有點尷尬）；如果 Mike 不能解釋模型的輸出，也沒關係。在這種情況下，沒有可解釋性是完全可以的。\n",
      "2. 但如果 Mike 開始圍繞這些度假地點的預測建立業務，情況就會改變。如果模型是錯誤的，公司可能會賠錢，或者模型可能會因種族偏見而對某些人更糟。一旦模型產生重大影響，無論是金融還是社會，可解釋性就很重要了。\n",
      "3. 模型是否可解釋取決於能否「解釋」數據集中的某個實例。如果有一個實例包含數千個特徵，那麼它是不可解釋的。但有少部分特徵或將實例簡化為最重要特徵的方法，則 k-最近鄰居可以為你提供很好的解釋。\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "<s>你是資深的資料科學家，請參考下方提供的資訊，並使用繁體中文回應使用者的提問\n",
    "資訊:\n",
    "1. Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez 和Kim，2017[5])我们深⼊探讨可解释性的重要性。当涉及到预测模型时，你需要作出权衡：你是只想知道预测是什么？例如，客户流失的概率或某种药物对病⼈的疗效。还是想知道为什么做出这样的预测？这种情况下可能为了可解释性付出预测性能下降的代价。在某些情况下，你不必关⼼为什么要做出这样的预测，只要知道模型在测试数据集的预测性能良好就⾜够了。但是在其他情况下，了解“为什么” 可以帮助你更多地了解问题、数据以及模型可能失败的原因。有些模型可能不需要解释，因为它们是在低风险的环境中使⽤的，这意味着错误不会造成严重后果(例如，电影推荐系统)，或者该⽅法已经被⼴泛研究和评估(例如，光学字符识别OCR)。对可解释性的需求来⾃问题形式化的不完整性[5]，这意味着对于某些问题或任务，仅仅获得预测结果是不够的。该模型还必须解释是怎么获得这个预测的，因为正确的预测只部分地解决了你的原始问题。以下原因推动了对可解释性(Interpretability) 和解释(Explanations) 的需求(Doshi-Velez'],\n",
    "2. 第⼆章可解释性15以下场景说明了我们何时不需要甚⾄不希望机器学习模型的可解释性。• 如果模型没有重大影响，则不需要解释性。想象⼀下，⼀个名为Mike 的⼈正在做⼀个机器学习⽅⾯的项⽬，根据Facebook 的数据预测他的朋友们下⼀个假期会去哪⾥。Mike 就是喜欢有依据地推测朋友们会去哪⾥度假，从⽽让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是，Mike 有点尴尬)；如果Mike 不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike 开始围绕这些度假⽬的地的预测建⽴业务，情况将会改变。如果模型是错误的，企业可能会赔钱，或者模型可能会因为种族偏见⽽对某些⼈变得更糟。⼀旦模型产⽣重⼤影响，⽆论是⾦融还是社会，可解释性就变得很重要了。•\n",
    "3. 个点。模型是否可解释仅取决于是否可以“解释” 数据集中的单个实例的问题。我认为，如果⼀个实例包含成百上千个特征，那么它是不可解释的。但是，如果你有少量特征或有将实例简化为最重要特征的⽅法，那么展⽰k-最近邻可以为你提供很好的解释。\n",
    "4. 第二章可解释性对可解释性是没有数学上定义的。我⽐较喜欢Miller (2017)[3] 的(⾮数学的) 定义：可解释性是⼈们能够理解决策原因的程度。另⼀种定义是[4]：可解释性是指⼈们能够⼀致地预测模型结果的程度。机器学习模型的可解释性越⾼，⼈们就越容易理解为什么做出某些决策或预测。如果⼀个模型的决策⽐另⼀个模型的决策能让⼈更容易理解，那么它就⽐另⼀个模型有更⾼的解释性。我们将在后⽂中同时使⽤Interpretable 和Explainable 这两个术语来描述可解释性。像Miller (2017) ⼀样，区分术语Interpretable 和Explainable 是有意义的。我们将使⽤Explainable 来描述对单个实例预测的解释。2.1可解释性的重要性如果⼀个机器学习模型运⾏良好，为什么我们仅仅信任该模型⽽忽略为什么做出特定的决策呢？诸如分类准确性(Classification Accuracy) 之类的单⼀指标⽆法完整地描述⼤多数实际任务。(Doshi-Velez\n",
    "[INST] {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# 使用 LLM Chain 將 Prompt 與 LLM 串接起來\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 將問題透過參數化的方式帶入\n",
    "question = \"模型的可解釋性真的很重要嗎? 為什麼他很重要?\"\n",
    "print(llm_chain.invoke({\"question\": question})[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
